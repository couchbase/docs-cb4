<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_rwn_1vf_ps">
 <title>High Availability and Disaster Recovery</title>
 <shortdesc>Couchbase Server supports a range of high availability and disaster recovery (HA/DR)
  strategies depending on an organizationâ€™s needs, standard operating procedures, and budget. </shortdesc>
 <conbody>
  <p>Although Couchbase Server supports true zero downtime deployments, often less stringent
   downtime objectives can be supported more economically and without serious impact to business
   continuity for mission critical applications. The exact tools and configurations to use depend on
   an organization's recovery point objectives (RPO) which define how much data can be lost in case
   of failure, and recovery time objectives (RTO) which define how much time does the technical team
   have to restore the system and the data. Most HA/DR strategies rely on a multi-pronged approach
   of maximizing availability, increasing redundancy within and across data centers, and performing
   regular backups.</p>
  <section id="rza"><title>Availability</title>
   <p>Couchbase Server delivers key high availability features such as zero downtime administration and maintenance, built-in data redundancy, and automatic failover.</p>
   <p>Factors that increase system uptime and availability include: <ul id="ul_njt_hzf_vs">
     <li>Number of replicas </li>
     <li>Number of racks or availability zones</li>
     <li>Number of clusters</li>
    </ul>
    <xref href="../architecture/server-group-awareness.dita#concept_exs_rrg_rn">Server Group
     Awareness</xref> helps protect against multi-node failure events by separating active data and
    its replicas across <xref href="../clustersetup/manage-groups.dita#mangage-server-groups">server
     groups</xref> which can then be mapped such that they occupy different racks, zones, or VM
    hosts. This is still within the context of a single Couchbase Server cluster, and the entire
    active data set is still evenly distributed across all nodes or groups.</p>
  </section>
  <section id="geo-dist-and-xdcr"><title>Geographic Distribution and XDCR</title>
   <p>Cross Data Center Replication (XDCR) replicates data between two or more autonomous Couchbase Server clusters. It is typically used to replicate data between clusters in different data centers and geographic regions, and can also be used to sync a second Couchbase Server cluster within the same data center.</p>
   <p>XDCR serves an important role in high availability / disaster recovery, performance, and load
    distribution. <ul>
     <li>For disaster recovery, one or more clusters can act as hot standbys, enabling cluster-level
      failover by taking over load as soon as a cluster stops responding.</li>
     <li>In case of serious failures, XDCR can also be used to recover data from a remote cluster.
      The result is similar to recovery using a backup but often faster. </li>
     <li>In geographically distributed data centers, XDCR can improve performance by placing data
      close to end users. Users can be routed to the Couchbase Server cluster that serves their
      request with the lowest latency, typically the one that is physically closest to them. XDCR
      with bi-directional replication can create a truly global Couchbase Server deployment by
      allowing full read and write access at every endpoint. <note>XDCR with bi-directional
       replication does not create a global "cluster" of Couchbase Servers. Rather, each cluster is
       autonomous, is managed completely independently, and data is replicated to and/or from each
       cluster. </note></li>
    </ul></p>
   <dl>
    <dlentry>
     <dt>XDCR is simple.</dt>
     <dd>It takes just two clicks to associate two clusters and two more clicks to create a replication stream from one bucket to another.  Additional replication streams can be created with the same simple steps to support bi-directional replication and more complex topologies. There is no restriction on the size of any cluster involved in XDCR replication and the streams are automatically maintained through cluster topology changes and failures.</dd>
    </dlentry>
    <dlentry>
     <dt>XDCR is efficient.</dt>
     <dd>XDCR replicates data directly from RAM-RAM between each node of the source and destination clusters of a stream.  There is no central gateway which greatly reduces complexity and allows for very high parallelization of throughput. Mutations are automatically de-duplicated to cut down on unnecessary transmissions and XDCR can be paused or resumed administratively. In order to recover from situations such as temporary disconnection between clusters, XDCR uses internal checkpoints to keep track of exactly which mutations it was able to stream. XDCR then begins streaming the mutations from the point where it left off.  With regular expression filtering on key names, users can limit what is transferred between clusters, for example to partially replicate regional data from a central, global database.</dd>
    </dlentry>
    <dlentry>
     <dt>XDCR is eventually consistent and features automatic conflict resolution.</dt>
     <dd>In order to determine what the final state of a document should be between two clusters, XDCR will pick the version with the most revisions if there is one. Otherwise, it falls down through the document sequence number, the CAS values, document flags, and TTL expiration value. Because each cluster applies the same logic, document consistency is maintained across the clusters.</dd>
    </dlentry>
    <dlentry>
     <dt>XDCR supports any topology.</dt>
     <dd>XDCR can perform unidirectional or bidirectional replication and supports spoke, circular,
      or star topologies of one-to-one, one-to-many or many-to-one clusters. For example,
      administrators can push from a central data center out to multiple data centers, aggregate data from multiple clusters to a central cluster, or make multiple data centers each function as masters. XDCR can also be used to synchronize a development or analytics cluster. Finally, XDCR is currently used to keep an Elasticsearch or SolR cluster updated for full-text indexing requirements.</dd>
    </dlentry>
   </dl>
   <p>For more information about managing XDCR and tuning XDCR performance, see <xref
     href="../xdcr/xdcr-intro.dita"/> in the<cite> Administration</cite> guide.</p>
  </section>
  <section><title>Backup and restore</title>
   <p>Backups protect against worst-case disaster situations and logical data corruption, such as an
    administrator accidentally deleting a bucket or a bug in the application. Backups can also be
    used to copy buckets for development, test or staging purposes. Couchbase Server supports
    zero-downtime backups and restores. Backups can be full, differential or cumulative, and
    Couchbase Server can restore any point to any bucket or topology. </p><p>For information on how to perform backup and restore, see <xref
     href="../backup-restore/backup-restore.dita"/> in the<cite> Administration</cite> guide.</p>
  </section>
  <section><title>Resilient applications</title>
   <p>Application design can also help improve resilience in the face of temporary or more permanent interruptions in node availability. For example, applications can be programmed to read from replicas, with timeouts set as low as 5 milliseconds. Similarly writes can be queued within the application to be retried once the node has been failed over. When using XDCR, an application can write to any cluster in a master-master replication scheme and the changes will be replicated to the other clusters.</p>
  </section>
 </conbody>
 <related-links>
  <linklist>
   <link href="../xdcr/xdcr-intro.dita">
    <linktext>Managing XDCR</linktext></link>
   <link href="../backup-restore/backup-restore.dita"></link>
  </linklist>
 </related-links>
</concept>
