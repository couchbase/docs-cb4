<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference xml:lang="en-us" id="cdbrestore-tool">
  <title><cmdname>cbrestore</cmdname></title>
  <shortdesc>The <cmdname>cbrestore</cmdname> tool restores data from a file to an entire cluster or a
    single bucket in the cluster. </shortdesc>
  <refbody>
    <section><title>Syntax</title>
      <p>The basic syntax is:</p>
      <codeblock>cbrestore [options] [backup-dir] [destination]</codeblock>
      
      <p>Where:</p>
     <ul>
       <li><codeph>[options]</codeph>: Command options for <cmdname>cbrestore</cmdname>.</li>
       <li><codeph>[backup-dir]</codeph>: The backup directory for the source data  created by
            <codeph>cbbackup</codeph> when performing the backup.</li>
       <li><codeph>[destination]</codeph>: This is a bucket in an existing cluster used as the
          destination bucket for the restored information. If you are restoring data to a single
          node in a cluster, provide the hostname and port of the node, If you are restoring an
          entire bucket, provide the URL of one of the nodes within the cluster. Be sure to
          create the destination bucket before restoring the data. </li>
     </ul> 
     
      
    </section>
    <section><title>Description</title><p>Items that had been written to file on disk are restored
        to RAM. </p><p>The <codeph>cbbackup</codeph>, <codeph>cbrestore</codeph>, and
          <codeph>cbtransfer</codeph> tools do not communicate with external IP addresses for server
        nodes outside of a cluster. Backup, restore, or transfer operations are performed on data
        from a node within a Couchbase cluster. They only communicate with nodes from a node list
        obtained within a cluster. If you install Couchbase Server with a default IP address, you
        cannot use an external host name to access it.</p>
      <b>Restoring Global Secondary Indexes</b>
      <p>If the backup being restored contains global secondary index definitions (stored in
          <codeph>index.json</codeph>), then these are also restored as part of the cbrestore
        process. Unlike both data and map-reduce views, global secondary indexes are not
        automatically distributed across a cluster. As a result, many users manually distribute
        their indexes across separate index nodes to ensure high availability (see <xref
          href="../n1ql/n1ql-language-reference/createindex.dita"> CREATE INDEX</xref>).</p><p>Due
        to the topology-agnostic nature of <codeph>cbbackup</codeph>,these index definitions are
        stored without the explicit node names, which are passed in as part of the <codeph>CREATE
          INDEX</codeph> command. <codeph>Cbbackup</codeph> does however store the indexer-id for
        each index, a value used to internally identify a specific index node. In order to restore
        indexes to a cluster, <codeph>cbrestore</codeph> sends each index definition, along with
        this identifier, to the index service which will appropriately distribute these indexes
        across all index nodes. When you restore indexes to the same cluster (and same topology)
        that the backup was taken from, the indexing service is able to locate the appropriate index
        nodes so that the indexes are on the same nodes as before. Often the topology of a cluster
        at the time of backup is not the same as the topology when restoring. In this case the
        indexing service may be unable to find the specific node associated with an index which it
        is restoring. The behaviour when the specific index node cannot be found is for the index
        service to distribute the index definition to another index node, chosen at random. This
        random distribution can cause indexes with the same definitions to be placed on the same
        node rather than separate nodes. You should always check that the indexes have been
        correctly distributed after running a restore, you may need to manually redistribute some
        indexes if necessary. </p><p>Once the index definitions have been restored by cbrestore they
        must be built manually (see <xref href="../n1ql/n1ql-language-reference/build-index.dita">
          BUILD INDEX</xref>) as the index service only creates them (without building them). This
        is to prevent resource contention caused by simultaneous data restoration and building of
        indexes during the cbrestore process. Additionally this gives the Couchbase administrator a
        chance to examine and redistribute the indexes before they are built.</p><p>You can check
        whether or not an index is in a ‘created’, ‘built’ or ‘building’ state by viewing the
        ‘Index’ tab in the web console.</p>
      <b>Conflict Resolution</b>
      <p>By default, when restoring from a backup using cbrestore, Couchbase Server will perform
        conflict resolution on all documents to be restored. The conflict resolution behaviour is
        the same as cross datacenter replication (XDCR), which is detailed in <xref
          href="../architecture/high-availability-replication-architecture.dita">XDCR
          architecture</xref>. This is so that documents which have been updated after the backup
        are not incorrectly overwritten by the restore process.</p><p>As a result, after running the
        restore process, some documents may not match the content of the backup file. In certain
        cases where a document has been recently deleted on the cluster, the document may not be
        restored at all due to this conflict resolution.</p><p>To restore the contents of a backup
        while overwriting current documents with the same key, pass the extra parameter
          <codeph>conflict_resolve=1</codeph> as part of the cbrestore command. To ensure that only
        the documents contained in the backup exist in the bucket after performing the restore,
        flush the bucket prior to performing the restore</p>
      <b>Tool Location</b><p>The tool is in the following locations:</p><table>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="7.33*"/>
          <thead>
            <row>
              <entry>Operating system</entry>
              <entry>Location</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Linux</entry>
              <entry><filepath>/opt/couchbase/bin/cbrestore</filepath></entry>
            </row>
            <row>
              <entry>Windows</entry>
              <entry><filepath>C:\Program Files\Couchbase\Server\bin\cbrestore</filepath></entry>
            </row>
            <row>
              <entry>Mac OS X</entry>
              <entry><filepath>/Applications/Couchbase Server.app/Contents/Resources/couchbase-core/bin/cbrestore</filepath></entry>
            </row>
          </tbody>
        </tgroup>
      </table><note type="important">Be sure to create the destination bucket before restoring the
        data.</note></section>
    
    <section><title>Options</title>
      
    
      <p>The following are the command options:</p>
      <table>
        <title>cbrestore options</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="3*"/>
          <thead>
            <row>
              <entry>Option</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><codeph>-h, --help</codeph></entry>
              <entry>Command line help.</entry>
            </row>
            <row>
              <entry><codeph>-a, --add</codeph></entry>
              <entry>Used to not overwrite existing items in the destination. Use
                  <cmdname>add</cmdname> instead of <cmdname>set</cmdname>.</entry>
            </row>
            <row>
              <entry><codeph>-b BUCKET_SOURCE, --bucket-source=BUCKET_SOURCE</codeph></entry>
              <entry>Single named bucket from the backup directory to restore. If the backup
                directory only contains a single bucket, then that bucket is automatically
                used.</entry>
            </row>
            <row>
              <entry><codeph>-B BUCKET_DESTINATION,
                --bucket-destination=BUCKET_DESTINATION</codeph></entry>
              <entry>When <codeph>--bucket-source</codeph> is specified, overrides the destination
                bucket name. This allows you to restore to a different bucket and defaults to the
                same as the bucket-source.</entry>
            </row>
            <row>
              <entry><codeph>from-date=FROM_DATE</codeph></entry>
              <entry>Restore data from the date specified as <codeph>yyyy-mm-dd</codeph>
                <b>*</b>. By default, all data from the very beginning is restored. <p>The updated
                  tool adds new options to support partial restore operations. The tool still
                  supports the existing options for full restores. </p></entry>
            </row>
            <row>
              <entry><codeph>to-date=TO_DATE</codeph></entry>
              <entry>Restore data until the date specified as <codeph>yyyy-mm-dd</codeph><b>*</b>.
                By default, all data collected is restored. The updated tool adds new options to
                support partial restore operations. The tool still supports the existing options for
                full restores. </entry>
            </row>
            <row>
              <entry><codeph>-i ID, --id=ID</codeph></entry>
              <entry>Transfer only items that match a vbucket ID.</entry>
            </row>
            <row>
              <entry><codeph>-k KEY, --key=KEY</codeph></entry>
              <entry>Transfer only items with keys that match a regexp.</entry>
            </row>
            <row>
              <entry><codeph>-m MODE, --mode=MODE</codeph></entry>
              <entry/>
            </row>
            <row>
              <entry><codeph>-n, --dry-run</codeph></entry>
              <entry>No actual transfer. Just validate parameters, files, connectivity, and
                configurations.</entry>
            </row>
            <row>
              <entry><codeph>-u USERNAME, --username=USERNAME</codeph></entry>
              <entry>REST username for source cluster or server node.</entry>
            </row>
            <row>
              <entry><codeph>-p PASSWORD, --password=PASSWORD</codeph></entry>
              <entry>REST password for cluster or server node.</entry>
            </row>
            <row>
              <entry><codeph>--single-node</codeph></entry>
              <entry>Use a single server node from the source only, not all server nodes from the
                entire cluster. This single server node is defined by the source URL.</entry>
            </row>
            <row>
              <entry><codeph>-t THREADS, --threads=THREADS</codeph></entry>
              <entry>Number of concurrent workers threads performing the transfer. </entry>
            </row>
            <row>
              <entry><codeph>-v, --verbose</codeph></entry>
              <entry>Verbose logging. More v's provide more verbosity. Max: -vvv.</entry>
            </row>
            <row>
              <entry><codeph>-x EXTRA, --extra=EXTRA</codeph></entry>
              <entry>Provide extra, uncommon configuration parameters. Comma-separated
                key=val(key-val)* pairs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      
      
      <p><b>*</b> The format of the <codeph>DATE</codeph> specification is
          <codeph>YYYY-MM-DD</codeph>, where: <dl>
          <dlentry>
            <dt><codeph>YYYY</codeph></dt>
            <dd>4-digit year</dd>
          </dlentry>
          <dlentry>
            <dt><codeph>MM</codeph></dt>
            <dd>2-digit month</dd>
          </dlentry>
          <dlentry>
            <dt><codeph>DD</codeph></dt>
            <dd>2-digit day </dd>
          </dlentry>
        </dl></p> 
      
      
      <p>The following are extra, specialized command options with the <codeph>cbrestore -x</codeph>
        parameter.</p>
      
      <table>
        <title>cbrestore -x options</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="3*"/>
          <thead>
            <row>
              <entry>-x option</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><codeph>backoff_cap=10</codeph></entry>
              <entry>Maximum back-off time during the rebalance period.</entry>
            </row>
            <row>
              <entry><codeph>batch_max_bytes=400000</codeph></entry>
              <entry>Transfer this # of bytes per batch.</entry>
            </row>
            <row>
              <entry><codeph>batch_max_size=1000</codeph></entry>
              <entry>Transfer this # of documents per batch.</entry>
            </row>
            <row>
              <entry><codeph>cbb_max_mb=100000</codeph></entry>
              <entry>Split backup file on destination cluster if it exceeds the MB.</entry>
            </row>
            <row>
              <entry><codeph>conflict_resolve=1</codeph></entry>
              <entry>By default, disable conflict resolution.</entry>
            </row>
            <row>
              <entry><codeph>data_only=0</codeph></entry>
              <entry>For value 1, transfer only data from a backup file or cluster.</entry>
            </row>
            <row>
              <entry><codeph>design_doc_only=0</codeph></entry>
              <entry>The documents are restored from a backup file (created with the
                  <codeph>cbbackup</codeph>) tool. For value 1, transfer only design documents from
                a backup file or cluster. Default: 0.</entry>
            </row>
            <row>
              <entry><codeph>max_retry=10</codeph></entry>
              <entry>Max number of sequential retries if the transfer fails.</entry>
            </row>
            <row>
              <entry><codeph>mcd_compatible=1</codeph></entry>
              <entry>For value 0, display extended fields for stdout output.</entry>
            </row>
            <row>
              <entry><codeph>nmv_retry=1</codeph></entry>
              <entry>0 or 1, where 1 retries transfer after a NOT_MY_VBUCKET message. Default:
                1.</entry>
            </row>
            <row>
              <entry><codeph>recv_min_bytes=4096</codeph></entry>
              <entry>Amount of bytes for every TCP/IP batch transferred.</entry>
            </row>
            <row>
              <entry><codeph>rehash=0</codeph></entry>
              <entry>For value 1, rehash the partition IDs of each item as it is required when
                transferring data between clusters with the different number of partitions; for
                example, when transferring data from an Mac OS X server to a non-Mac OS X
                cluster.</entry>
            </row>
            <row>
              <entry><codeph>report=5</codeph></entry>
              <entry>Number batches transferred before updating the progress bar in the
                console.</entry>
            </row>
            <row>
              <entry><codeph>report_full=2000</codeph></entry>
              <entry>Number batches transferred before emitting the progress information in the
                console.</entry>
            </row>
            <row>
              <entry><codeph>seqno=0</codeph></entry>
              <entry>By default, start <codeph>seqno</codeph> from beginning.</entry>
            </row>
            <row>
              <entry><codeph>try_xwm=1</codeph></entry>
              <entry>Transfer documents with metadata. Default: 1. The value of 0 is used only when
                transferring from 1.8.x to 1.8.x.</entry>
            </row>
            <row>
              <entry><codeph>uncompress=0</codeph></entry>
              <entry>For value 1, restore data in uncompressed mode.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
   <section><title>Examples</title>
     <p>The following are basic syntax examples:</p>
     
     <codeblock>cbrestore ~/backup http://192.0.2.0:8091 -b travel-sample
cbrestore ~/backup couchbase://192.0.2.0:8091 -b travel-sample
cbrestore ~/backup memcached://192.0.2.0:11211 -b travel-sample		</codeblock>
     
  <p><b>Example for restoring only design documents</b></p> 
     <p>The following example restores design documents from the backup file,
        ~/backup/travel-sample, to the destination bucket, travel-sample, in a cluster. </p>
     <codeblock>cbrestore ~/backup http://192.0.2.0:8091 -x design_doc_only=1 \ 
    -b travel-sample -B travel-sample</codeblock>
     <p>If multiple source buckets were backed up, this command must be performed multiple times.
       In the following example, a cluster with two data buckets is backed up and has the following backup
       files: <ul>
         <li><codeph>~/backup/travel-sample/design.json</codeph></li>
         <li><codeph>~/backup/beer-sample/design.jsonT</codeph></li>
       </ul></p>
     
     <p>The following command restores the design documents in both backup files to a bucket
       in a cluster named <codeph>my_bucket</codeph>.</p>
     
     <codeblock>cbrestore -b travel-sample -B travel-sample -x design_doc_only=1 \
    ~/backup http://192.0.2.0:8091    

cbrestore -b beer-sample -B beer-sample -x design_doc_only=1 \
    ~/backup http://192.0.2.0:8091</codeblock>
     <p>The following example response shows a successful restore.</p>
     <codeblock>transfer design doc only. bucket msgs will be skipped.
done     </codeblock>
     
  <p><b>Example for restoring data incrementally</b></p>   
     
     <p>The following example requests a restoration of data backed up between August 1, 2014, and
        August 3, 2014. The <codeph>-b</codeph> option specifies the name of the bucket to restore from the
        backup file, and the <codeph>-B</codeph> option specifies the name of the destination bucket in the
        cluster.</p>
     
     <codeblock>cbrestore -b travel-sample -B travel-sample \
    --from-date=2014-08-01 --to-date=2014-08-03 ~/backup \
    http://192.0.2.0:8091</codeblock> 
     
     <p><b>Example for restoring data while ignoring conflict resolution</b></p>   
     
     <p>The following example requests a restoration of data while ignoring the built-in conflict
        resolution. This causes all documents in the backup file to be restored regardless of
        whether the documents in the target cluster would win conflict resolution.</p>
     
     <codeblock>cbrestore -b travel-sample -B travel-sample -x conflict_resolve=0 \
    ~/backup http://192.0.2.0:8091</codeblock> 
     
   </section> 
     
    
  </refbody>
 
</reference>
