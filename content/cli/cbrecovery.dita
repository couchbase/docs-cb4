<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference xml:lang="en-us" id="cbrecovery">
  
  <title>
    <cmdname>
      cbrecovery
    </cmdname>
  </title>
  
  <shortdesc>
    The <cmdname>cbrecovery</cmdname> tool restores data to a local cluster, from a bucket
    on a remote cluster that was previously established as an XDCR remote replica.
  </shortdesc>
  
  
  <refbody>
    
    <section>
      <title>
        Syntax
      </title>
      
      <p>
        The basic syntax is:
      </p>
      <codeblock>cbrecovery [options] [source-cluster] [destination-cluster]</codeblock>
      
      <p>
        Where:
      </p>
      
     <ul>
       
       <li>
         <codeph>[options]</codeph>: Command options for <cmdname>cbrecovery</cmdname>.
       </li>
       
       <li>
         <codeph>[source-cluster]</codeph>: A cluster containing an XDCR remote-replica bucket, which is
         used as the source for data-recovery.
       </li>
       
       <li>
         <codeph>[destination-cluster]</codeph>: A cluster that has suffered data loss from a bucket
         previously replicated to a remote cluster, by means of XDCR.
       </li>
       
     </ul> 
     
    </section>
    
    <section>
      <title>
        Description
      </title>
      
      <p>
        Couchbase Server allows one or more replicas to be created for each vBucket on the cluster. This helps
        to ensure continued data-availability in the event of node-failure.
      </p>
      
      <p>
        However, if multiple nodes within a single cluster fail simultaneously,          
        one or more active vBuckets and all their replicas may be affected; meaning that lost data cannot be
        recovered locally.
      </p>
      
      <p>
        In such cases,
        provided that a bucket affected by such failure has already been established as a source bucket for XDCR, the
        lost data may be retrieved from the bucket defined
        on the remote server as the corresponding replication-target. This retrieval is achieved from the command-line, by means of
        <b>cbrecovery</b>.     
      </p>
      
      <p>
        Before attempting to recover the lost data, restore capacity to the local cluster,
        as appropriate. However, do not rebalance added nodes into the cluster until <i>after</i>
        <b>cbrecovery</b> has been used to recover the lost data.
      </p>
      
      <b>Tool Location</b><p>The tool is in the following locations:</p><table>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="7.33*"/>
          <thead>
            <row>
              <entry>Operating system</entry>
              <entry>Location</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Linux</entry>
              <entry><filepath>/opt/couchbase/bin/cbrecovery</filepath></entry>
            </row>
            <row>
              <entry>Windows</entry>
              <entry><filepath>C:\Program Files\Couchbase\Server\bin\cbrecovery</filepath></entry>
            </row>
            <row>
              <entry>Mac OS X</entry>
              <entry><filepath>/Applications/Couchbase Server.app/Contents/Resources/couchbase-core/bin/cbrecovery</filepath></entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    
    <section><title>Options</title>
      
    
      <p>The following are the command options:</p>
      <table>
        <title>cbrestore options</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="2*"/>
          <colspec colname="col2" colwidth="3*"/>
          <thead>
            <row>
              <entry>Option</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><codeph>-h, --help</codeph></entry>
              <entry>Command line help.</entry>
            </row>
            
            <row>
              <entry><codeph>-b BUCKET_SOURCE, --bucket-source=BUCKET_SOURCE</codeph></entry>
              <entry>Name of a bucket on the source cluster. This must previously have been
              established and used as a replication target for XDCR, using the source-bucket
              specified here as BUCKET_DESTINATION.</entry>
            </row>


            <row>
              <entry><codeph>-B BUCKET_DESTINATION,
                --bucket-destination=BUCKET_DESTINATION</codeph></entry>
              <entry>Name of a bucket on the destination cluster. This is a bucket that has suffered
              data-loss, and was previously established and used as a source for XDCR replication, using
              the target-bucket specified here as BUCKET_SOURCE.</entry>
            </row>
            
            <row>
              <entry><codeph>-u USERNAME, --username=USERNAME</codeph></entry>
              <entry>The username for the Full Administrator, Cluster Administrator, or XDCR Administrator
                for the cluster from which the data is recovered.</entry>
            </row>
            
            <row>
              <entry><codeph>-p PASSWORD, --password=PASSWORD</codeph></entry>
              <entry>The password for the Full Administrator, Cluster Administrator, or XDCR Administrator
                for the cluster from which the data is recovered.</entry>
            </row>
            
            <row>
              <entry><codeph>-U USERNAME_DESTINATION, --username-destination=USERNAME_DESTINATION</codeph></entry>
              <entry>The username for the Full Administrator, Cluster Administrator, or XDCR Administrator
                for the cluster onto which the recovered data is copied.</entry>
            </row>
            
            <row>
              <entry><codeph>-P PASSWORD_DESTINATION, --password-destination=PASSWORD_DESTINATION</codeph></entry>
              <entry>The password for the Full Administrator, Cluster Administrator, or XDCR Administrator
                for the cluster onto which the recovered data is copied.</entry>
            </row>
            
            <row>
              <entry><codeph>-i ID, --id=ID</codeph></entry>
              <entry>Transfer only items that match a vBucket ID.</entry>
            </row>
            
            <row>
              <entry><codeph>-s, --ssl</codeph></entry>
              <entry>Transfer data with SSL enabled.</entry>
            </row>
            
            <row>
              <entry><codeph>-t THREADS, --threads=THREADS</codeph></entry>
              <entry>Number of concurrent worker-threads, performing the transfer. </entry>
            </row>


            <row>
              <entry><codeph>-n, --dry-run</codeph></entry>
              <entry>No actual transfer. Just validate parameters, files, connectivity, and
                configurations.</entry>
            </row>


            <row>
              <entry><codeph>-v, --verbose</codeph></entry>
              <entry>Verbose logging. More v's provide more verbosity. Max: -vvv.</entry>
            </row>
            
            <row>
              <entry><codeph>-x EXTRA, --extra=EXTRA</codeph></entry>
              <entry>Provide extra, uncommon configuration-parameters. Comma-separated
                key=val(key-val)* pairs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      
      
      
      <p>
        The following are extra, specialized command options, used by means of the <codeph>cbrecovery -x</codeph>
        parameter.
      </p>
      
      <table>
        <title>cbrestore -x options</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="3*"/>
          <thead>
            <row>
              <entry>-x option</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><codeph>backoff_cap=10</codeph></entry>
              <entry>Maximum back-off time during the rebalance period.</entry>
            </row>
            <row>
              <entry><codeph>batch_max_bytes=400000</codeph></entry>
              <entry>Transfer this # of bytes per batch.</entry>
            </row>
            <row>
              <entry><codeph>batch_max_size=1000</codeph></entry>
              <entry>Transfer this # of documents per batch.</entry>
            </row>
            <row>
              <entry><codeph>cbb_max_mb=100000</codeph></entry>
              <entry>Split backup file on destination cluster if it exceeds the MB.</entry>
            </row>
            <row>
              <entry><codeph>conflict_resolve=1</codeph></entry>
              <entry>By default, enable conflict resolution.</entry>
            </row>
            <row>
              <entry><codeph>data_only=0</codeph></entry>
              <entry>For value 1, transfer only data from a backup file or cluster.</entry>
            </row>
            
            <row>
              <entry><codeph>dcp_consumer_queue_length=1000</codeph></entry>
              <entry>A DCP client
                needs a queue for incoming documents/messages. A large length is more
                efficient, but memory proportional to length*avg. doc size. Below
                length 150, performance degrades significantly.</entry>
            </row>
            
            <row>
              <entry><codeph>design_doc_only=0</codeph></entry>
              <entry>For value 1, transfer only design documents from
                a backup file or cluster. Default: 0.
                The design documents are restored from a backup file created with 
                the <codeph>cbbackup</codeph> tool.</entry>
            </row>
            
            <row>
              <entry><codeph>max_retry=10</codeph></entry>
              <entry>Max number of sequential retries if the transfer fails.</entry>
            </row>
            
            <row>
              <entry><codeph>mcd_compatible=1</codeph></entry>
              <entry>For value 0, display extended fields for stdout output.</entry>
            </row>
            
            <row>
              <entry><codeph>nmv_retry=1</codeph></entry>
              <entry>0 or 1, where 1 retries transfer after a NOT_MY_VBUCKET message. Default:
                1.</entry>
            </row>
            
            <row>
              <entry><codeph>recv_min_bytes=4096</codeph></entry>
              <entry>Amount of bytes for every TCP/IP batch transferred.</entry>
            </row>
            
            <row>
              <entry><codeph>rehash=0</codeph></entry>
              <entry>For value 1, rehash the partition IDs of each item as it is required when
                transferring data between clusters with the different number of partitions; for
                example, when transferring data from an Mac OS X server to a non-Mac OS X
                cluster.</entry>
            </row>
            <row>
              <entry><codeph>report=5</codeph></entry>
              <entry>Number batches transferred before updating the progress bar in the
                console.</entry>
            </row>
            
            <row>
              <entry><codeph>report_full=2000</codeph></entry>
              <entry>Number batches transferred before emitting the progress information in the
                console.</entry>
            </row>
            
            <row>
              <entry><codeph>seqno=0</codeph></entry>
              <entry>By default, start <codeph>seqno</codeph> from beginning.</entry>
            </row>
            
            <row>
              <entry><codeph>try_xwm=1</codeph></entry>
              <entry>Transfer documents with metadata. Default: 1. The value of 0 is used only when
                transferring from 1.8.x to 1.8.x.</entry>
            </row>
            
          </tbody>
        </tgroup>
      </table>
    </section>
    
   <section>
     <title>
       Example
     </title>
     
     <codeblock>$ cbrecovery http://10.142.180.104:8091 http://10.142.180.101:8091 \
 > -b travelSampleBackup \
 > -B travel-sample \
 > -u Administrator \
 > -p password \
 > -U Administrator \
 > -P password \
 > -v</codeblock>
     
   </section> 
     
    
  </refbody>
 
</reference>
