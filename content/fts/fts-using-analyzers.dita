<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_o23_j34_1v">
  
  <title>
    Understanding Analyzers
  </title>
  
  <shortdesc>
    Analyzers increase search-awareness by transforming input text into token-streams, which
    permit the management of richer and more finely controlled forms of text-matching. An 
    analyzer consists of modules, each of
    which performs a particular, sequenced role in the transformation. 
  </shortdesc>
  
  <body>
  
    <section id="principles-of-text-analysis">
    <title>
      Principles of Text-Analysis
    </title>
  
    <p>
      <i>Analyzers</i> pre-process input-text submitted for Full Text Search; typically, by removing
      characters that might prohibit certain match-options. Analysis is performed on document-contents
      when indexes are created; and is also performed on the input-text submitted for a search. The
      benefit of analysis is often referred to as <i>language awareness</i>.
    </p>
    
    <p>
      For example, if the input-text for a search is <codeph>enjoyed staying here</codeph>, and the document-content
      contains the phrase <codeph>many enjoyable activities</codeph>, the dictionary-based words do not permit
      a match. However, by using an analyzer that (by means of its inner <i>Token Filter</i> component) 
      <i>stems</i> words, the input-text yields the tokens
      <codeph>enjoy</codeph>, <codeph>stay</codeph>, and <codeph>here</codeph>; while the document-content yields the tokens
      <codeph>enjoy</codeph> and <codeph>activ</codeph>. By means of the common token <codeph>enjoy</codeph>, this 
      permits a match
      between <codeph>enjoyed</codeph> and <codeph>enjoyable</codeph>.
    </p>
    
    <p>
      Since different analyzers pre-process text in different ways, effective Full Text Search depends on the right choice
      of analyzer, for the type of matches that are desired.
    </p>
    
    <p>
      Couchbase Full Text Search provides a number of pre-constructed analyzers that can be used with
      Full Text Indexes. Additionally, analyzers can be custom-created, by means of the Couchbase Web Console. The 
      remainder of this page explains the architecture of analyzers, and describes the modular components that
      Couchbase Full Text Search makes available for custom-creation. It also lists the pre-constructed analyzers
      that are available, and describes the modules that they contain.
    </p>
    
    <p>
      For examples of both selecting and custom-creating analyzers by means of the Couchbase Web Console, see
      <xref href="./fts-creating-indexes.dita" scope="local" format="dita">Creating Indexes</xref>.
    </p>
      
  </section>
    
    <section id="analyzer-architecture">
      
      <title>
        Analyzer Architecture
      </title>

      <p>
        Analyzers are built from modular components:
      </p>
        
        <ul>
          
          <li>
            <b>Character Filters</b>
            remove undesirable characters from input: for example, the <codeph>html</codeph> 
            character filter removes HTML tags, and indexes HTML text-content alone.
          </li>
          
          <li>
            <b>Tokenizers</b>
            split input-strings into individual 
            <i>tokens</i>, which together are made into a <i>token stream</i>. The nature of the decision-making whereby splits are
            made differs across tokenizers.
          </li>
          
          <li>
            <b>Token Filters</b> are chained together, with each performing additional post-processing on each token
            in the stream provided by the tokenizer. This may include reducing tokens to the <i>stems</i> of the
            dictionary-based words from which they were derived, removing any remaining punctuation from tokens, and removing
            certain tokens deemed unnecessary.
          </li>
          
        </ul> 
      
      <p>
        Each component-type is described in more detail below. Note that these components can be used to custom-create an analyzer
        by means of the Couchbase Web Console. This is explained and exemplified in
        <xref href="./fts-creating-indexes.dita" scope="local" format="dita">Creating Indexes</xref>.
      </p>
    
    </section>
    
    <section id="character-filters">
      
      <title>
        Character Filters
      </title>
      
      <p>
        <i>Character Filters</i> remove undesirable characters. The following filters are available:
      </p>
      
      <ul>
        
        <li>
          <b>html</b>: Removes html elements such as <codeph>&lt;p&gt;</codeph>, and decodes expressions such as
          <codeph>&amp;amp;</codeph> to their appropriate text equivalent.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>zero_width_spaces</b>: Substitutes a regular space-character for each <i>zero-width non-joiner</i> space.
          
          <p>
            
          </p>
          
        </li>
        
      </ul>
      
    </section>
    
    <section id="tokenizers">
      
      <title>
        Tokenizers
      </title>
      
      <p>
        Tokenizers split input-strings into individual tokens: characters likely to prohibit certain kinds
        of matching (for example, spaces or commas) are omitted. The tokens so created are then made into
        a <i>token stream</i> for the query.
      </p>

      <p>
        The following tokenizers are available from the Couchbase Web Console:
      </p>
      
      <ul>
        <li>
          <b>Letter</b>: Creates tokens by breaking input-text into subsets that consist of letters only:
          characters such as punctuation-marks and numbers are omitted. Creation of a token ends whenever
          a non-letter character is encountered. For example, the text <codeph>Reqmnt: 7-element phrase</codeph>
          would return the following tokens: <codeph>Reqmnt</codeph>, <codeph>element</codeph>, and 
          <codeph>phrase</codeph>.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>Single</b>: Creates a single token from the entirety of the input-text. For example, the text
          <codeph>in each place</codeph> would return the following token: <codeph>in each place</codeph>.
          Note that this may be useful for handling URLs or email-addresses, which can thus be prevented
          from being broken at punctuation or special-character boundaries. It may also be used to prevent
          multi-word phrases (for example, placenames such as <codeph>Milton Keynes</codeph> or
          <codeph>San Francisco</codeph>) from being broken up due to whitespace; so that they become indexed
          as a single term.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>Unicode</b>: Creates tokens by performing <i>Unicode Text Segmentation</i> on word-boundaries, using the
          <xref href="https://github.com/blevesearch/segment" format="html" scope="external">segment</xref> library.
          For examples, see
          <xref href="http:http://www.unicode.org/reports/tr29/#Word_Boundaries" scope="external" format="html">Unicode Word Boundaries</xref>.      
 
          <p>
            
          </p>     
          
        </li>
        
        <li>
          <b>Web</b>: Creates tokens by identifying and removing html tags. For example, the text
          <codeph>&lt;h1&gt;Introduction&lt;\h1&gt;</codeph> would return the token <codeph>Introduction</codeph>.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>Whitespace</b>: Creates tokens by breaking input-text into subsets according to where
          whitespace occurs. For example, the text <codeph>in each place </codeph> would return the following
          tokens: <codeph>in</codeph>, <codeph>each</codeph>, and <codeph>place</codeph>.
          
          <p>
            
          </p>
          
        </li>
        
      </ul>
    
    </section>
    
    <section id="token-filters">
      
      <title>
        Token Filters
      </title>
      
      <p>
        <i>Token Filters</i> accept a token-stream provided by a tokenizer, and make modifications to the tokens in the stream.
      </p>
      
      <p>
        A frequently used form of token filtering is <i>stemming</i>; this reduces words
        to a base form that typically consists of the initial <i>stem</i> of the word (for example,
        <codeph>play</codeph>, which is the stem of <codeph>player</codeph>, <codeph>playing</codeph>,
        <codeph>playable</codeph>, and more). With the stem used as the token, a wider variety of matches
        can be made (for example, the input-text <codeph>player</codeph> can be matched with the document-content
        <codeph>playable</codeph>).
      </p>
      
      <p>
        The following kinds of token-filtering are supported by Couchbase Full Text Search:
      </p>
      
      <ul>
        
        <li>
          <b>dict_compound</b>: Allows user-specification of a dictionary whose words can be combined
          into compound forms, and individually indexed.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>apostrophe</b>: Removes all characters after an apostrophe, and the apostrophe itself. For example,
          <codeph>they've</codeph> becomes <codeph>they</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>elision</b>: Identifies and removes characters that prefix a term and are separated 
          from it by an apostrophe. For example,
          in French, <codeph>l'avion</codeph> becomes <codeph>avion</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>edge_ngram</b>: From each token, computes 
          <xref href="https://en.wikipedia.org/wiki/N-gram" scope="external" format="html">n-grams</xref> that
          are rooted either at the front or the back.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>keyword_marker</b>: Identifies keywords and marks them as such. These are then ignored by any
          downstream stemmer. 
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>normalize_unicode</b>: Converts tokens into 
          <xref href="http://unicode.org/reports/tr15/" scope="external" format="html">Unicode Normalization Form</xref>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>ngram</b>: From each token, computes 
          <xref href="https://en.wikipedia.org/wiki/N-gram" scope="external" format="html">n-grams</xref>.
          There are two parameters, which are the minimum and maximum n-gram length.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>shingle</b>: Computes multi-token shingles from the token stream. For example, the token stream
          <codeph>the quick brown fox</codeph>, when configured with a shingle minimum and a shingle
          maximum length of 2,
          produces the tokens <codeph>the quick</codeph>, <codeph>quick brown</codeph>, and <codeph>brown fox</codeph>.
          
          <p>
            
          </p>
        </li>
        
        
        <li>
          <b>stemmer</b>: Uses 
          <xref href="http://snowball.tartarus.org/" scope="external" format="html">libstemmer</xref> to reduce 
          tokens to word-stems.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>stop_tokens</b>: Removes from the stream tokens considered unnecessary for a
          Full Text Search: for example, <codeph>and</codeph>, <codeph>is</codeph>, and <codeph>the</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>to_lower</b>: Converts all characters to lower case. For example, <codeph>HTML</codeph> becomes <codeph>html</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>truncate</b>: Truncates each token to a maximum-permissible token-length.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>possessive</b>: Removes English possessives. For example, <codeph>software's</codeph> 
          becomes <codeph>software</codeph>.
          
          <p>
            
          </p>
        </li>
        
      </ul>
      
      <p>
        Note that token filters are frequently configured according to the special characteristics of individual languages.
        Couchbase Full Text Search provides multiple language-specific versions of the <b>elision</b>, <b>normalize</b>, 
        <b>stemmer</b>, <b>possessive</b>, and <b>stop</b> token filters. Specially supported languages are shown in
        the table immediately below.
      </p>
      
      <table frame="all" rowsep="1" colsep="1" id="token_filter_languages_5.5">
        
        <title>Token-Filter Lanaguages Supported by Couchbase Server 5.5</title>
        
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="0.2*"/>
          <colspec colname="c2" colnum="2" colwidth="0.8*"/>
          
          <thead>
            <row>
              <entry>Name</entry>
              <entry>Language</entry>
            </row>
          </thead>
          
          <tbody>
            
            <row>
              <entry>ar</entry>
              <entry>Arabic</entry>
            </row>
            
            <row>
              <entry>bg</entry>
              <entry>Bulgarian</entry>
            </row>
            
            <row>
              <entry>ca</entry>
              <entry>Catalan</entry>
            </row>
            
            <row>
              <entry>cjk</entry>
              <entry>Chinese | Japanese | Korean</entry>
            </row>
            
            <row>
              <entry>ckb</entry>
              <entry>Kurdish</entry>
            </row>
            
            <row>
              <entry>da</entry>
              <entry>Danish</entry>
            </row>
            
            <row>
              <entry>de</entry>
              <entry>German</entry>
            </row>
            
            <row>
              <entry>el</entry>
              <entry>Greek</entry>
            </row>
            
            <row>
              <entry>en</entry>
              <entry>English</entry>
            </row>
            
            <row>
              <entry>es</entry>
              <entry>Spanish (Castilian)</entry>
            </row>
            
            <row>
              <entry>eu</entry>
              <entry>Basque</entry>
            </row>
            
            <row>
              <entry>fa</entry>
              <entry>Persian</entry>
            </row>
            
            <row>
              <entry>fi</entry>
              <entry>Finnish</entry>
            </row>
            
            <row>
              <entry>fr</entry>
              <entry>French</entry>
            </row>
            
            <row>
              <entry>ga</entry>
              <entry>Gaelic</entry>
            </row>
            
            <row>
              <entry>gl</entry>
              <entry>Spanish (Galician)</entry>
            </row>
            
            <row>
              <entry>hi</entry>
              <entry>Hindi</entry>
            </row>
            
            <row>
              <entry>hu</entry>
              <entry>Hungarian</entry>
            </row>
            
            <row>
              <entry>hy</entry>
              <entry>Armenian</entry>
            </row>
            
            <row>
              <entry>id, in</entry>
              <entry>Indonesian</entry>
            </row>
            
            <row>
              <entry>it</entry>
              <entry>Italian</entry>
            </row>
            
            <row>
              <entry>nl</entry>
              <entry>Dutch</entry>
            </row>
            
            <row>
              <entry>no</entry>
              <entry>Norwegian</entry>
            </row>
            
            <row>
              <entry>pt</entry>
              <entry>Portuguese</entry>
            </row>
            
            <row>
              <entry>ro</entry>
              <entry>Romanian</entry>
            </row>
            
            <row>
              <entry>ru</entry>
              <entry>Russian</entry>
            </row>
            
            <row>
              <entry>sv</entry>
              <entry>Swedish</entry>
            </row>
            
            <row>
              <entry>tr</entry>
              <entry>Turkish</entry>
            </row>
            
          </tbody>
        </tgroup>
      </table>
      
    </section>
    
    <section id="pre-constructed-analyzers">
      
      <title>
        Pre-Constructed Analyzers
      </title>
    
      <p>
        A number of pre-constructed analyzers are available, and can be selected from the Couchbase Web Console. For examples
        of selection, see
        <xref href="./fts-creating-indexes.dita" scope="local" format="dita">Creating Indexes</xref>. The basic analyzers are 
        as follows. See the sections above for details on the referenced analyzer-components.
      </p>
      
      <ul>
        <li>
          <b>keyword</b>: Creates a single token representing the entire input, which is otherwise
          unchanged. This forces exact matches, and preserves characters such as spaces.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>simple</b>: Analysis by means of the <b>Unicode</b> tokenizer and the <b>to_lower</b> token filter.
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>standard</b>: Analysis by means of the <b>Unicode</b> tokenizer, the <b>to_lower</b> token filter, and the <b>stop</b> token filter.
          
          <p>
            
          </p>
          
        </li>
          
        
        <li>
          <b>web</b>: Analysis by means of the <b>Web</b> tokenizer and the  <b>to_lower</b> token filter. 
          
          <p>
            
          </p>
          
        </li>
        
      </ul>
      
      <p>
        Additionally, a range of analyzers is provided for the specific support of certain languages; as shown by
        the table immediately below.
      </p>
      
      <table frame="all" rowsep="1" colsep="1" id="analyzer_languages_5.5">
        
        <title>Analyzer-Lanaguages Supported by Couchbase Server 5.5</title>
        
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="0.2*"/>
          <colspec colname="c2" colnum="2" colwidth="0.8*"/>
          
          <thead>
            <row>
              <entry>Name</entry>
              <entry>Language</entry>
            </row>
          </thead>
          
          <tbody>
            
            <row>
              <entry>ar</entry>
              <entry>Arabic</entry>
            </row>
            
            <row>
              <entry>cjk</entry>
              <entry>Chinese | Japanese | Korean</entry>
            </row>
            
            <row>
              <entry>ckb</entry>
              <entry>Kurdish</entry>
            </row>
            
            <row>
              <entry>da</entry>
              <entry>Danish</entry>
            </row>
            
            <row>
              <entry>de</entry>
              <entry>German</entry>
            </row>
            
            <row>
              <entry>en</entry>
              <entry>English</entry>
            </row>
            
            <row>
              <entry>es</entry>
              <entry>Spanish (Castilian)</entry>
            </row>
            
            <row>
              <entry>fa</entry>
              <entry>Persian</entry>
            </row>
            
            <row>
              <entry>fi</entry>
              <entry>Finnish</entry>
            </row>
            
            <row>
              <entry>fr</entry>
              <entry>French</entry>
            </row>
            
            <row>
              <entry>hi</entry>
              <entry>Hindi</entry>
            </row>
            
            <row>
              <entry>hu</entry>
              <entry>Hungarian</entry>
            </row>
            
            <row>
              <entry>it</entry>
              <entry>Italian</entry>
            </row>
            
            <row>
              <entry>nl</entry>
              <entry>Dutch</entry>
            </row>
            
            <row>
              <entry>no</entry>
              <entry>Norwegian</entry>
            </row>
            
            <row>
              <entry>pt</entry>
              <entry>Portuguese</entry>
            </row>
            
            <row>
              <entry>ro</entry>
              <entry>Romanian</entry>
            </row>
            
            <row>
              <entry>ru</entry>
              <entry>Russian</entry>
            </row>
            
            <row>
              <entry>sv</entry>
              <entry>Swedish</entry>
            </row>
            
            <row>
              <entry>tr</entry>
              <entry>Turkish</entry>
            </row>
            
          </tbody>
        </tgroup>
      </table>
      
    </section>
    
  </body>
  
</topic>
