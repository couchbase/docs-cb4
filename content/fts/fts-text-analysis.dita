<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_o23_j34_1v">
  <title>Text Analysis</title>
  <body>
    <p><term>Text analysis</term> is the process of transforming input text into a series of terms,
      or canonical forms of words that are typically stripped of inflections and junk characters.
      Analysis is done on the text documents when you create the index, and again on the text of
      your query when you perform a search. </p>
    <p>When people talk about full text search being "language aware", they are usually referring to the results of text analysis.</p>
    <p><b>Example</b></p>
    <p>Consider you have a document containing the text, "watery light beer" and you search for
      "watered down".</p>
    <p>There is no exact match in this case, but if you want this query to match this document you
      can accomplish this by choosing the right analyzer. In this case, you want to use an analyzer
      that can stem English words. </p>
    <p>By doing this, the index entry for the document will contain the terms: "water", "light", "beer". And the query will search for terms: "water", "down".</p>
    <p>Now you see that the terms will match and you get the expected search results.</p>
    <p>Stemming is just one way in which the text can be transformed, the key here is that choosing the right analyzer is essential to getting high quality search results.</p>
    <section> <title>Analyzers</title>
      <p>Analyzers are used to transform input text into a stream of tokens for indexing. Full Text
        Search includes a number of analyzers or you can build your own. Analyzers are built up from
        modular components.<ul>
          <li>Character Filters strip out undesirable characters from the input. For example, the
            "html" character filter ignores HTML tags and just indexes the content from HTML
            documents.</li>
          <li>Tokenizers split input strings into a token stream. Typically you’re trying to create
            a token for each word.</li>
          <li>Token Filters are chained together to perform additional post processing on the token
            stream.</li>
        </ul></p> </section>
    <section><title>Tokenizers</title>
      <p>Tokenizers cut sequences of characters into meaningful units (tokens) while ignoring useless characters, like spaces or commas.</p>
      <p>Stemming is the process of reducing words to an underlying base form. The goal of stemming is to get rid of grammatical inflections while preserving the words’ meanings. Those underlying base forms are then written into the index.</p>
      <p>In the Couchbase Server Web Console, you can find a list of available stemmers on the
          <uicontrol>Index</uicontrol> > <uicontrol>Full Text</uicontrol> >
          <uicontrol>Edit</uicontrol> or <uicontrol>Create New Index</uicontrol> >
          <uicontrol>Analyzers</uicontrol>. If you click "<uicontrol>New Analyzer</uicontrol>" you
        see a list of available stemmers in the "<uicontrol>Token Filters</uicontrol>" drop-down
        list.</p>
      <p><b>Single Token</b> </p> 
      <p>The Single Token Tokenizer returns the entire input bytes as a single token. This can be
        useful for things like URLs
          (<filepath>http://www.example.com/blog/stuff+and+nonsense/</filepath>) or email address
          (<codeph>will.spammington@couchbase.com</codeph>) that might otherwise be broken up at
        punctuation or special character boundaries. It could also be used when multi-word phrases
        should be indexed as a single term, for example, in the case of proper place names that
        would otherwise be broken into multiple terms by the whitespace tokenizer. </p>
      <p>Consider the excerpt below from the brewery documents in the beer-sample database. You can
        use a single token analyzer for the <parmname>city</parmname>, <parmname>name</parmname>,
          <parmname>country</parmname>, <parmname>phone</parmname>, or <parmname>website</parmname>
        fields because they are all continuous strings that you may not want to divide at dots,
        dashes, or spaces. The single token tokenizer is probably less useful for the name field,
        which users will probably enter many different
        ways.<codeblock>{
        "type": "brewery",
        "name": "21st Amendment Brewery Cafe",
        "city": "San Francisco",
        "country": "United States",
        "phone": "1-415-369-0900",
        "website": "http://www.21st-amendment.com/",
        ...
        }</codeblock>
      </p>
      <p><b>Regular Expression</b> </p> 
      <p>The Regular Expression Tokenizer will tokenize input using a configurable regular expression. The regular expression should match token text. See the Whitespace Tokenizer for an example of its usage.
      </p>
      <p><b>Whitespace</b> </p> 
      <p>The Whitespace Tokenizer is an instance of the Regular Expression Tokenizer. It uses the
        regular expression <codeph>\w+</codeph>. For many western languages this may work well as a
        simple tokenizer.</p>
      <p><b>Unicode</b> </p> 
      <p>The Unicode Tokenizer uses the <xref href="https://github.com/blevesearch/segment"
          format="html" scope="external">segment</xref> library to perform <xref
          href="http://www.unicode.org/reports/tr29/" format="html" scope="external">Unicode Text
          Segmentation</xref> on word boundaries. It is recommended over the ICU tokenizer for all
        languages not requiring dictionary-based tokenization that is supported by ICU.</p>
      <p>Full Text Search includes a number of analyzers or you can build your own. </p> </section>
    <section><title>Customizing the Field Mappings</title>
      <p><b>Dynamic Index Mapping</b> </p> 
      <p>When the indexer encounters a field whose type you haven’t explicitly specified, it guesses
        the type by looking at the JSON value. <table frame="all" rowsep="1" colsep="1"
          id="table_dcs_gl4_1v">
          <tgroup cols="2" align="left">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Type of JSON value</entry>
                <entry>Indexed as...</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Boolean</entry>
                <entry>Boolean</entry>
              </row>
              <row>
                <entry>Number</entry>
                <entry>Number</entry>
              </row>
              <row>
                <entry>String containing a date</entry>
                <entry>Date</entry>
              </row>
              <row>
                <entry>String (not containing a date)</entry>
                <entry>String</entry>
              </row>
            </tbody>
          </tgroup>
        </table> The indexer attempts to parse String values as dates and indexes them as such if
        the operation succeeds. Full text search and Bleve expect dates to be in the format
        specified by <xref href="https://www.ietf.org/rfc/rfc3339.txt" format="html"
          scope="external">RFC-3339</xref>, which is a specific profile of ISO-8601 that is more
        restrictive. </p>
      <p>Note that other String values like "7" or "true" will never be indexed as a number or
        Boolean. The number type is modeled as 64-bit floating point value internally.</p>
      <p/>    </section>
  </body>
</topic>
