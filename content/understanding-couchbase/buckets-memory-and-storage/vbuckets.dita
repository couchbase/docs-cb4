<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_lzt_qr2_xs2">
 
 <title>vBuckets</title>
 
  <shortdesc>
   <i>vBuckets</i> are virtual buckets that help distribute data effectively across a cluster, and 
   support replication across multiple nodes. 
  </shortdesc>
 
 <body>
  
  <section>
  
  <title>
   Understanding vBuckets
  </title>
   
   <p>
    Couchbase Server allows users and applications to save data, in binary or JSON format, in named <i>buckets</i>. Each 
    bucket therefore contains <i>keys</i> and associated <i>values</i>. See
    <xref href="../buckets-memory-and-storage/buckets.dita" scope="local" format="dita">Buckets</xref>, for detailed
    information.
   </p>
   
   <p>
    Within the memory and storage management system of Couchbase Server, both Couchbase and Ephermal buckets are implemented
    as <i>vBuckets</i>, 1024 of which are created for every bucket (except on MacOS, where the number is 64). 
    vBuckets are distributed evenly across the memory and storage facilities of the cluster; and the bucket's items are
    distributed evenly across its vBuckets. This evenness of distribution ensures that all instances
    of the 
    <xref href="../services-and-indexes/services/data-service.dita" scope="local" format="dita">Data Service</xref> 
    take an approximately equal share of the workload, in terms of numbers of documents to maintain, and
    operations to handle.
   </p>
   
   <p>
    The 1024 vBuckets that implement a defined bucket are referred to as <i>active</i> vBuckets.
    If a bucket is replicated, each replica is implemented as a further 1024 (or 64) vBuckets, referred to
    as <i>replica</i> vBuckets. Thus, a bucket configured on Linux
    to be replicated twice results in a total of 3072 vBuckets, distributed across the cluster. 
    <i>Write</i> operations are performed only on <i>active</i> vBuckets. Most <i>read</i> operations
    are performed on <i>active</i> vBuckets, though items can also be read from <i>replica</i> vBuckets
    when necessary.
   </p>
   
   <p>
    Note that vBuckets are sometimes referred to as <i>shards</i>.
   </p>
   
   <p>
    Items are written to and retrieved from vBuckets by means of a <i>CRC32</i> hashing algorithm, which is
    applied to the item's key, and so produces the number of the vBucket in which the item resides. vBuckets are
    mapped to individual nodes by the Cluster Manager: the mapping is constantly updated and made generally available
    to SDK and other clients.
   </p>
   
   <p>
    The relationships between a bucket, its keys (and their associated values), the hashing algorithm, vBuckets, server-mappings,
    and servers, is illustrated below:
   </p>
   
   <p>
    <image href="./images/vbucketToNodeMapping.png" id="vbucket_to_node_mapping" align="left" width="820"/>
   </p>
   
   <p>
    Thus, an authorized client attempting to access data performs a hash operation on the appropriate key, and thereby
    calculates the number of the vBucket that owns the key. The client then examines the vBucket map to determine the
    server-node on which the vBucket resides; and finally performs its operation directly on that server-node. 
   </p>
   
   <p>
    When a cluster-configuration changes &#8212; due to rebalance, failover, or node-addition &#8212; replica buckets
    are promoted to primaries if appropriate, and both primaries and replicas are redistributed across the available
    nodes of the modified cluster. The vBucket map is duly updated by the Cluster Manager.
    The updated map is then sent to all cluster-participants. For additional information on the distribution
    of vBuckets across the cluster, see
    <xref href="../clusters-and-availability/replication-architecture.dita" scope="local" format="dita">Availability</xref>.
   </p>
   
   <p>
    Note that this use of client-side hashing for access to Couchbase and Ephemeral bucket-data contrasts with the
    Memcached data-access method; which requires active management of the server-list, and a specific 
    hashing algorithm, such as Ketama, to handle topology-changes.
   </p>
  
 </section>
 
 </body>
</topic>
