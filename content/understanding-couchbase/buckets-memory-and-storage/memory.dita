<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_x54_dmj_vs2">
 
 <title>
  Memory
 </title>
 
 <shortdesc>
  Couchbase Server memory-management ensures high performance and scalability.
 </shortdesc>
 
 <body>
  
  <section id="service-memory-quotas">
   
   <title>Service Memory Quotas</title>
   
   <p>
    Memory quota allocation on Couchbase Server occurs <i>per service</i>: this applies to all services
    except <i>Query Service</i>, which does not require a specific allocation. Quotas for buckets
    are specified on bucket-creation, and are subtracted 
    from the quotat allocated to the <i>Data Service</i>.
   </p>
   
   <p>
    The memory quota allocation specified for a given service applies to every instance of that
    service across the cluster. For example, if 2048 MB is specified for the Analytics Service,
    and the Analytics Service is run on three of a cluster's five nodes, each of the three
    instances of the service is duly allocated 2048 MB. Note that it is not possible to have
    a different memory allocation for each service-instance.
   </p>
   
   <p>
    By default, Couchbase Server allows 80% of a node's total available memory to be allocated to services.
    Thus, if a node's total available memory is 100 GB, any attempt to allocate memory beyond 80 GB produces
    an error. Therefore, when a cluster is being assembled, available memory resources and projected
    service-requirements must be carefully calculated. Note, for example, that if an initial cluster-node 
    with 100 GB of memory is assigned 
    the Data Service only, and with
    an allocation of 80 GB, an error results from any attempt to add to the cluster a new
    node with only 90 GB of total memory: since the Data Service must run on every node, with
    a required memory allocation of 80 GB, and the new node is unable to fit this
    allocation within 80% of its own available memory.
   </p>
   
   <p>
    When a node is added to a cluster, the <i>Default Configuration</i>, as established by set-up of
    the first node in the cluster, can be specified for all configurable elements, including memory
    quotas. If insufficient memory is available on the new node, an error occurs: in which case, settings
    for the new node can be custom-configured. This allows a subset of services to be specified, such
    as will fit within the memory-limits of the new node. 
   </p>
   
   <p>
    If, when the initial node of a cluster is set up, only a subset of services is assigned, additional
    nodes may subsequently be added in order to host additional services: in which case, each new service can be given
    its cluster-wide memory allocation as the new node is added.
   </p>
   
   <p>
    The <b>Service Memory Quotas</b> panel on <b>Settings</b> screen of Couchbase Web Console lists all 
    services running on the cluster, and specifies
    the memory allocation for each.
    The panel is interactive, and allows the memory allocations to be changed and saved. If a modification
    attemptedly goes beyond the maximum allocation tolerated by any node in the cluster, a notification 
    of the error is displayed, and the modification is disallowed.
   </p>
   
   
  </section>
  
  <section>
   <title>
    Listeners
   </title>
   
   <p>
    When client connection requests arrive at the database engine, the listener service receives the requests 
    and authenticates the client. Upon successful authentication, the listener service assigns a worker 
    thread to the connection to service its request. A single worker thread can handle multiple client 
    connections using a non-blocking event loop.
   </p>
   
   <p>
    The number of worker threads that can be created is automatically determined based on the number of CPU 
    threads present on the node. By default the number of worker threads is 0.75 x number of CPU threads.
   </p>
  </section>
  
  <section>
   <title>
    vBucket manager and managed cache
   </title>
   
   <p>
    After executing mutation and read requests, the server uses the managed cache to hold updated and newly 
    created values. However, with a high flow of incoming operations, the system can run out of memory 
    quickly. In order to reuse the memory, mutations are also queued for disk persistence.  Once the 
    mutated items are persisted, the server frees up the memory consumed by these items, making space 
    for newer operations. This operation is called <i>cache eviction</i>. With a highly concurrent set 
    of operations consuming memory and a high throughput disk subsystem persisting data to disk, 
    there can be many pages eligible for reuse. The server uses the Least Recently Used (LRU) 
    algorithm to identify the memory pages that can be reused. 
   </p>
   
   <p>
    It is important to size the RAM capacity appropriately for your working set: the portion of data that 
    your application is working with at any given point in time and needs very low latency and high 
    throughput access. In some applications, the working set is the entire data set, while in others 
    it is a smaller subset.
   </p>
   
  </section>
  
  <section>
   <title>
    Initialization and Warmup
   </title>
   
   <p>
    Whenever you restart the Couchbase Server or restore the data, the node goes through a warmup process 
    before it starts handling data requests again. During warmup, the Couchbase Server loads 
    data  persisted on disk into RAM. 
   </p>
   
   <p>
    Couchbase Server provides an optimized warmup process that loads data sequentially from disk
    into RAM. It divides the data to be loaded and handles it in multiple phases. After the warmup
    process completes, the data is available for clients to read and write. The time needed for a
    node warmup depends on the system size, system configuration, the amount of data persisted in
    the node, and the ejection policy configured for the buckets. 
   </p>
   
   <p>
    Couchbase Server identifies items that are frequently used, prioritizes them, and loads them
    before sequentially loading the remaining data. The frequently-used items are prioritized in an
    access log. The server performs a prefetch to get a list of the most frequently accessed keys
    and then fetches these keys before fetching any other items from disk. 
   </p>
   
   <p>
    The server runs a configurable scanner process that determines the keys that are most frequently 
    used. The scanner process is preset and is configurable. You can use the command-line tool,
    <cmdname>cbepctl flush_param</cmdname>, to change the initial time and interval for the scanner 
    process. For example, you can configure the scanner process to run during a specific time period 
    when a given list of keys need to be identified and made available sooner.
   </p>
   
   <p>The server can also switch into a <i>ready mode</i> before it has actually retrieved all documents 
    for keys into RAM, thereby enabling data to be served before all the stored items are loaded. 
    Switching into ready mode is a configurable setting that enables you to adjust the server 
    warmup time.
   </p>
  </section>
  
  <section id="full-ejection">
   
   <title>
    Tunable Memory with Ejection Policy
   </title>
   
   <p>
    Tunable memory enables you to configure the ejection policy for a bucket as one of the following: 
    
    <ul>
    <li id="value-only-ejection"><term>Value-only ejection </term>(default) removes data from the cache but keeps all keys
     and metadata fields for non-resident items. When a value bucket ejection occurs, the value of
     the item is reset. Value-only ejection, also referred to as value ejection, is well suited for
     cases where low latency access is critical to the application and the total item keys for the
     bucket can easily fit in the allocated Data RAM quota. 
    </li>
     
    <li>
     <term>Full metadata ejection </term>removes all data <b>including keys</b>, metadata, and
     key-value pairs from the cache for non-resident items. Full ejection is well suited for cases
     where the application has cold data that is not accessed frequently or the total data size is
     too large to fit in memory plus higher latency access to the data is accepted. The performance
     of full eviction cache management is significantly improved by <xref href="./bloom-filters.dita"
      >Bloom filters</xref>. Bloom filters are enabled by default and cannot be disabled. <note
       spectitle="Important">Full ejection may involve additional disk I/O per operation. For
       example, when the request <i>get_miss</i> which requests a key that does not exist is
       received, Couchbase Server will check for the key on the disk even if the bucket is 100%
       resident. </note>
    </li>
     
   </ul>
   </p>
  </section>
  
  <section>
   
   <title>
    Working Set Management and Ejection
   </title>
   
   <p>
    Couchbase Server actively manages the data stored in a caching layer; this includes the 
    information which is frequently accessed by clients and which needs to be available for rapid 
    reads and writes. When there are too many items in RAM, Couchbase Server removes certain data 
    to create free space and to maintain system performance. This process is called "working set 
    management" and the set of data in RAM is referred to as the "working set". In general, the 
    working set consists of all the keys, metadata, and associated documents which are frequently 
    used require fast access. The process the server performs to remove data from RAM is known as 
    ejection.
   </p>
   
   <p>
    Couchbase Server performs ejections automatically. When ejecting information, it works in 
    conjunction with the disk persistence system to ensure that data in RAM is persisted to 
    disk and can be safely retrieved back into RAM whenever the item is requested. 
   </p>
   
   <p>
    In addition to the Data RAM quota for the caching layer, the engine uses two watermarks,
    <parmname>mem_low_wat</parmname> and <parmname>mem_high_wat</parmname>, to determine when it
    needs to start persisting more data to disk.
   </p>
   
   <p>
    As more and more data is held in the caching layer, at some point in time it passes the
    <parmname>mem_low_wat</parmname> value. At this point, no action is taken. As data continues to
    load, it eventually reaches the <parmname>mem_high_wat</parmname> value. At this point, the
    Couchbase Server schedules a background job called item pager which ensures that items are
    migrated to disk and memory is freed up for other Couchbase Server items. This job runs until
    measured memory reaches <parmname>mem_low_wat</parmname>. If the rate of incoming items is
    faster than the migration of items to disk, the system returns errors indicating there is not
    enough space until there is sufficient memory available. The process of migrating data from the
    cache to make way for actively used information is called ejection and is controlled
    automatically through thresholds set on each configured bucket in the Couchbase Server cluster.
   </p> 
    
    <p>
     <image href="./images/tunableMemory.png" width="300" id="tunable_memory"/>
    </p>

   <p>
    Depending on the ejection policy set for the bucket, the vBucket Manager removes just the
    document or both the document, key and the metadata for the item being ejected. Keeping an
    active working set with keys and metadata in RAM serves three important purposes in a system:
    
    <ul>
     <li>
      Couchbase Server uses the remaining key and metadata in RAM if a client requests for that
      key. Otherwise, the node tries to fetch the item from disk and return it into RAM.
     </li>
     
     <li>
      The node can also use the keys and metadata in RAM for <i>miss access</i>. This means that
      it can quickly determine whether an item is missing and if so, perform some action, such as
      add it.
     </li>
     
     <li>
      The expiration process in Couchbase Server uses the metadata in RAM to quickly scan for
      items that have expired and later removes them from disk. This process is known as <i>expiry pager</i> 
      and runs every 60 minutes by default.
     </li>
     
    </ul>
   </p>
   
  </section>
  
  <section>
   
   <title>
    Not Recently Used (NRU) Items
   </title>
   
   <p>
    All items in the server contain metadata indicating whether the item has been recently accessed or 
    not. This metadata is known as not-recently-used (NRU). If an item has not been recently used, 
    then the item is a candidate for ejection. When data in the cache exceeds the high water mark 
    (mem_high_wat), the server evicts items from RAM.
   </p>
   
   <p>
    Couchbase Server provides two NRU bits per item and also provides a replication protocol that can 
    propagate items that are frequently read, but not mutated often.
   </p>
   
   <p>
    NRUs are decremented or incremented by server processes to indicate an item that is more
    frequently or less frequently used. The following table lists the bit values with the
    corresponding scores and statuses: 
    
    <table frame="all" rowsep="1" colsep="1"
     id="table_ekt_2yz_xs">
     <title>Scoring for NRU bit values</title>
     <tgroup cols="4" align="left">
      <colspec colname="c1" colnum="1"/>
      <colspec colname="c2" colnum="2"/>
      <colspec colname="c3" colnum="3"/>
      <colspec colname="c4" colnum="4"/>
      <thead>
       <row>
        <entry>Binary NRU</entry>
        <entry>Score</entry>
        <entry>Access pattern</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>00</entry>
        <entry>0</entry>
        <entry>Set by write access to 00. Decremented by read access or no access.</entry>
        <entry>Most heavily used item.</entry>
       </row>
       <row>
        <entry>01</entry>
        <entry>1</entry>
        <entry>Decremented by read access.</entry>
        <entry>Frequently accessed item.</entry>
       </row>
       <row>
        <entry>10</entry>
        <entry>2</entry>
        <entry>Initial value or decremented by read access.</entry>
        <entry>Default value for new items.</entry>
       </row>
       <row>
        <entry>11</entry>
        <entry>3</entry>
        <entry>Incremented by item pager for eviction.</entry>
        <entry>Less frequently used item.</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </p>
   
   <p>
    There are two processes that change the NRU for an item:
    
    <ul>
     <li>
      When a client reads or writes an item, the server decrements NRU and lowers the item's score.
     </li>
     
     <li>
      A daily process which creates a list of frequently-used items in RAM. After the completion of this process, 
      the server increments one of the NRU bits. 
     </li>
     
    </ul>
    
    Because these two processes change NRUs, they play an important role in identifying the candidate items for ejection. </p>
   <p>You can configure the Couchbase Server settings to change the behavior during ejection. For example, you can specify the percentage of RAM to be consumed before items are ejected, or specify whether ejection should occur more frequently on replicated data than on original data. Couchbase recommends that the default settings be used.</p>
  </section>
  <section><title>Understanding the Item Pager</title>
   <p>The item pager process runs periodically to remove documents from RAM. When the amount of RAM
    used by items reaches the high water mark (upper threshold), both active and replica data are
    ejected until the amount of RAM consumed (memory usage) reaches the low water mark (lower
    threshold). Using the default settings, active documents have a 40% chance of eviction while
    replica documents have a 60% chance of eviction. Both the high water mark and low water mark are
    expressed as a percentage amount of RAM, such as 80%.</p>
   <p>You can change the high water mark and low water mark settings for a node by specifying a
    percentage amount of RAM, for example, 80%. Couchbase recommends that you use the following
    default settings: <table frame="all" rowsep="1" colsep="1" id="table_pnj_x21_ys">
     <title>Default setting for RAM water marks</title>
     <tgroup cols="3" align="left">
      <colspec colname="c1" colnum="1"/>
      <colspec colname="c2" colnum="2"/>
      <colspec colname="c3" colnum="3"/>
      <thead>
       <row>
        <entry>Version</entry>
        <entry>High water mark</entry>
        <entry>Low water mark</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>2.0</entry>
        <entry>75%</entry>
        <entry>60%</entry>
       </row>
       <row>
        <entry>2.0.1 and higher</entry>
        <entry>85%</entry>
        <entry>75%</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </p>
   
   <p>
    The item pager ejects items from RAM in two phases: 
    
    <ol>
     
     <li>
      Eject items based on NRU: The item pager scans NRU for items, creates a list of 
      items with a NRU score 3, and ejects all the identified items. It then 
      checks the RAM usage and repeats the process if the usage is still above 
      the low water mark. 
     </li>
     
     <li>
      Eject items based on algorithm: The item pager increments the NRU of all items by 1. For
      every item whose NRU is equal to 3, it generates a random number. If the random number for an
      item is greater than a specified probability, it ejects the item from RAM. The probability is
      based on the current memory usage, low water mark, and whether a vBucket is in an active or
      replica state. If a vBucket is in an active state, the probability of ejection is lower than
      if the vBucket is in a replica state. 
      
      <table frame="all" rowsep="1" colsep="1"
       id="table_qpb_hg1_ys">
       <title>Probability of ejection based on active vBuckets versus replica vBuckets (using
        default settings)</title>
       <tgroup cols="2" align="left">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <thead>
         <row>
          <entry>Active vBucket</entry>
          <entry>Replica vBucket</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>40%</entry>
          <entry>60%</entry>
         </row>
        </tbody>
       </tgroup>
      </table></li>
    </ol>
   </p>
  </section>
  
  <section>
   <title>
    Active Memory Defragmenter
   </title>
   
   <p>
    Over time, the memory used by the managed cache of a running Couchbase Server can become 
    fragmented. The storage engine now includes an <i>Active Defragmenter</i> task to 
    defragment cache memory.
   </p>
   
   <p>
    Cache fragmentation is a side-effect of how Couchbase Server organizes cache memory to maximize 
    performance. Each page in the cache is typically responsible for holding documents of a specific 
    size range. Over time, if memory pages assigned to a specific size range become sparsely populated 
    (due to documents of that size being ejected or items changing in size), then the unused space in 
    those pages cannot be used for documents of other sizes until a complete page is free and that page 
    is re-assigned to a new size. Such effects are highly workload dependent and can result in memory 
    that cannot be used efficiently by the managed cache. 
   </p> 
   
   <p>
    The Active Memory Defragmenter attempts to address any fragmentation by periodically scanning the 
    cache to identify pages which are sparsely used, and repacking the items stored on those pages to 
    free up <i>whole</i> pages.
   </p>
   
  </section>
  
  
  <section>
   
   <title>
    Bloom Filters
   </title>
   
   <p>
    A Bloom filter is a probabilistic data structure used to test whether an element is a
   member of a set. False positive matches are possible, but false negatives are not. This means a
   query returns either "possibly in set" or "definitely not in set". It is a bit array with a
   predefined size that is calculated based on the expected number of items and the probability of
   false positives or the probability of finding a key that doesn't exist. Bloom filter
   significantly improves the performance of full ejection scenarios and XDCR.  
   </p>

   <p>
    In the full ejection mode, the key and metadata are evicted along with the value. Therefore,
    if a key is non resident, there is no way of knowing if a key exists or not, without accessing
    the disk. In such a scenario, if a client issues a lot of GETs on keys that may not even exist
    in server, Bloom filters help eliminate many unnecessary disk accesses. Similarly for XDCR,
    when we set up remote replication to a brand new cluster, we would be able to avoid many
    unnecessary GetMeta-disk-fetches with the help of the bloom filter.
   </p>
   
   <p>
    With Bloom filters, the probability of false positives decreases as the size of the array
    increases and increases as the number of inserted elements increases. Based on the algorithm
    that takes into account the number of keys and the probability of false positives, you can
    estimate the size of the Bloom filter and the number of bits to store each key. 
   </p>
   
   <p>
    For value eviction only the deleted keys will be stored, while in case of full eviction both
    the deleted keys and non-resident items will be stored. 
   </p>
   
   <p>
    The algorithm calculates the almost exact probability of false positives, including the
    number of hash functions (<codeph>k</codeph>), size of the bit array (<codeph>m</codeph>), and
    the number of inserted elements (<codeph>n</codeph>):
   </p>
   
   <codeblock outputclass="language-c">k = m/n (ln 2)</codeblock>
   
   <p>
    You can expect an increase in memory usage or memory overhead while using the Bloom filter:
    
    <table frame="all" rowsep="1" colsep="1" id="table_dkx_rvg_vq">
     <title>Memory overhead for Bloom filter use</title>
     <tgroup cols="3" align="left">
      <colspec colname="c1" colnum="1" colwidth="1.0*"/>
      <colspec colname="c2" colnum="2" colwidth="1.0*"/>
      <colspec colname="c3" colnum="3" colwidth="1.0*"/>
      <thead>
       <row>
        <entry>False positive probability</entry>
        <entry>0.01</entry>
        <entry>0.05</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Estimated number of keys</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
       </row>
       <row>
        <entry>Number of bits per key in the filter</entry>
        <entry>7 bits</entry>
        <entry>4 bits</entry>
       </row>
       <row>
        <entry>Size of the Bloom filter to fit the estimated keys with desired false positive
         probability</entry>
        <entry>95851 bits (=> =12 KB per vBucket) (=> =12 MB for 1024 vBuckets)</entry>
        <entry>62353 bits (=> =8 KB per vBucket) (=> =8 MB for 1024 vBuckets)</entry>
       </row>
      </tbody>
     </tgroup>
    </table></p> 
   
   <p>
    In a case of full eviction, you will not know whether an item exists in the memory until you
    perform a background fetch. Therefore, use of the Bloom filter helps to avoid unnecessary
    background fetches and improves latency. 
   </p>
   
   <p>
    For more information about working set management and eviction, see <xref href="./db-engine-architecture.dita"></xref></p>
  </section>
 </body>
</topic>
