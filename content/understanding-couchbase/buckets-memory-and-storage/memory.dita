<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_x54_dmj_vs2">
 <title>Memory</title>
 <shortdesc>Since Couchbase built Couchbase Server on a memory-first architecture, achieving high performance and scalability requires effective memory management.</shortdesc>
 <body>
  <section> <title>Caching Layer</title> 
   <p> Each service in Couchbase Server tunes its caching based on its needs. <ul>
    <li>Data service tunes its managed cache to enable fast key based read and write operations with low latency under high concurrency.</li>
    <li>Index and Search services manage the cache to ensure index maintenance and fast scans for
      the most popular indexes in the system. </li>
    <li>Query service manages memory to calculate query responses by processing streams effectively for the execution plans generated by the optimizer. </li>
   </ul></p><p>The Couchbase SDK never access the persistence layer directly, but communicate through the
    caching layer. Couchbase Server moves the data to and from the disk internally as needed, thereby acting as both a read-through and a write-through cache. This facilitates extremely high
    read-write rates and eliminates the need for an external caching tier. Unlike many other
    database systems, Couchbase Server does not depend on external caching systems. This simplifies
    development since developers do not need to deal with complex cache coherency issues or varied
    performance capabilities across technologies.</p>
   <p>In order to provide the best possible performance for the most-frequently accessed items, Couchbase Server 
    manages a subset of the users' entire dataset in the caching layer. The Server ensures performance remains acceptable 
    by coordinating between the caching layer and persistent storage. For example, when an item enters the caching layer 
    Couchbase Server also inserts the item into a disk-writing queue. As a result, if the software's algorithm later determines
    the item is accessed too infrequently to justify its continued use of cache resources, the server can safely 
    remove the item from RAM to free up space for other incoming operations.  Similarly, if a user requests an 
    infrequently accessed item, the server retrieves the item from disk and stores it in the caching layer in case the user 
    needs ongoing access to the item.</p>
   <p>By default, the Couchbase Server automatically keeps frequently used data in memory and less-frequently used data 
    on disk. Couchbase Server moves data from the managed cache to disk asynchronously, in the background, to ensure 
    there is enough memory that can be freed up for incoming operations. The server constantly monitors the information 
    accessed by clients and decides how to keep the active data within the caching layer. Items may be ejected from memory
    when additional memory is needed to perform incoming operations. These items have already been persisted to disk and 
    require no additional I/O. The managed cache ensures that reads and writes are handled at a very fast rate, while 
    removing the typical load and performance spikes that would otherwise cause a traditional RDBMS to produce erratic 
    performance.</p>
   <!--<p>For information about cluster and bucket RAM quotas and how allocation of memory to services is governed, see  <xref href="cluster-ram-quotas.dita"></xref>.</p>-->
  </section>
  <section id="ram-quota"><title>RAM quotas</title>
   <p>RAM quota allocation is governed through individual services. Each service in Couchbase Server tunes its caching based on its needs. <ul>
    <li>The Data service uses a managed cache based on memcached that is tuned to enable fast key based read and write operations with low latency under high concurrency. </li>
    <li>The Index and Search services manage cache to ensure index maintenance and scans can be
     serviced fast for the most popular indexes in the system.</li>
    <li>Query service manages its memory to calculate query responses by processing streams effectively for the execution plans generated by the optimizer and caches certain parts of those query plans.</li>
   </ul></p><p>Allocation of memory to services is governed through RAM quota allocations. Data, Index and
    Search services both configure RAM quotas per node in the cluster. Query service automatically
    manages its memory without a defined quota.</p>
   <p>Each node in the cluster running the relevant services inherits the value and may allocate up to the specified amount. 
    <ul><li><b>Index RAM Quota</b> governs the index service RAM quota allocation per node. Each node running the index service inherits the value of Index RAM Quota for caching Global Secondary Indexes (GSI). </li>
     <li><b>Search RAM Quota</b> governs the search service RAM quota allocation per node. Each
      node running the search service inherits the value of search RAM Quota for caching Full
      Text Indexes.</li>
     <li><b>Data RAM Quota</b> governs the data service RAM quota allocation per node. Each node running the data service inherits the value set for Data RAM Quota for caching bucket data. </li>
    </ul></p>
   <p><b>Bucket RAM Quotas</b> are allocated out of the Cluster Data RAM quota. As an administrator, you can control the total RAM quota allocated to each bucket through Bucket RAM Quota under bucket settings. The total RAM configured across all buckets cannot exceed the total <b>Data RAM Quota</b> allocated for the data service at the cluster level.</p>
  </section>
  
  <section><title>Listeners</title>
   <p>When client connection requests arrive at the database engine, the listener service receives the requests and authenticates the client. Upon successful authentication, the listener service assigns a worker thread to the connection to service its request. A single worker thread can handle multiple client connections using a non-blocking event loop.
   </p>
   <p>The number of worker threads that can be created is automatically determined based on the number of CPU threads present on the node. By default the number of worker threads is 0.75 x number of CPU threads.</p></section>
  <section><title>vBucket manager and managed cache</title>
   <p>After executing mutation and read requests, the server uses the managed cache to hold updated and newly created values. However, with a high flow of incoming operations, the system can run out of memory quickly. In order to reuse the memory, mutations are also queued for disk persistence.  Once the mutated items are persisted, the server frees up the memory consumed by these items, making space for newer operations. This operation is called <i>cache eviction</i>. With a highly concurrent set of operations consuming memory and a high throughput disk subsystem persisting data to disk, there can be many pages eligible for reuse. The server uses the Least Recently Used (LRU) algorithm to identify the memory pages that can be reused. </p>
   <p>It is important to size the RAM capacity appropriately for your working set: the portion of data that your application is working with at any given point in time and needs very low latency and high throughput access. In some applications, the working set is the entire data set, while in others it is a smaller subset.</p>
  </section>
  <section><title>Initialization and Warmup</title>
   <p>Whenever you restart the Couchbase Server or restore the data, the node goes through a warmup process before it starts handling data requests again. During warmup, the Couchbase Server loads data  persisted on disk into RAM. </p>
   <p>Couchbase Server provides an optimized warmup process that loads data sequentially from disk
    into RAM. It divides the data to be loaded and handles it in multiple phases. After the warmup
    process completes, the data is available for clients to read and write. The time needed for a
    node warmup depends on the system size, system configuration, the amount of data persisted in
    the node, and the ejection policy configured for the buckets. </p>
   <p>Couchbase Server identifies items that are frequently used, prioritizes them, and loads them
    before sequentially loading the remaining data. The frequently-used items are prioritized in an
    access log. The server performs a prefetch to get a list of the most frequently accessed keys
    and then fetches these keys before fetching any other items from disk. </p>
   <p>The server runs a configurable scanner process that determines the keys that are most frequently used. The scanner process is preset and is configurable. You can use the command-line tool,<cmdname>cbepctl flush_param</cmdname>, to change the initial time and interval for the scanner process. For example, you can configure the scanner process to run during a specific time period when a given list of keys need to be identified and made available sooner.</p>
   <p>The server can also switch into a <i>ready mode</i> before it has actually retrieved all documents for keys into RAM, thereby enabling data to be served before all the stored items are loaded. Switching into ready mode is a configurable setting that enables you to adjust the server warmup time.</p>
  </section>
  <section id="full-ejection"><title>Tunable Memory with Ejection Policy</title>
   <p>Tunable memory enables you to configure the ejection policy for a bucket as one of the following: <ul>
    <li id="value-only-ejection"><term>Value-only ejection </term>(default) removes data from the cache but keeps all keys
     and metadata fields for non-resident items. When a value bucket ejection occurs, the value of
     the item is reset. Value-only ejection, also referred to as value ejection, is well suited for
     cases where low latency access is critical to the application and the total item keys for the
     bucket can easily fit in the allocated Data RAM quota. </li>
    <li><term>Full metadata ejection </term>removes all data <b>including keys</b>, metadata, and
     key-value pairs from the cache for non-resident items. Full ejection is well suited for cases
     where the application has cold data that is not accessed frequently or the total data size is
     too large to fit in memory plus higher latency access to the data is accepted. The performance
     of full eviction cache management is significantly improved by <xref href="./bloom-filters.dita"
      >Bloom filters</xref>. Bloom filters are enabled by default and cannot be disabled. <note
       spectitle="Important">Full ejection may involve additional disk I/O per operation. For
       example, when the request <i>get_miss</i> which requests a key that does not exist is
       received, Couchbase Server will check for the key on the disk even if the bucket is 100%
       resident. </note></li>
   </ul></p>
  </section>
  <section><title>Working Set Management and Ejection</title>
   <p>Couchbase Server actively manages the data stored in a caching layer; this includes the information which is frequently accessed by clients and which needs to be available for rapid reads and writes. When there are too many items in RAM, Couchbase Server removes certain data to create free space and to maintain system performance. This process is called "working set management" and the set of data in RAM is referred to as the "working set". In general, the working set consists of all the keys, metadata, and associated documents which are frequently used require fast access. The process the server performs to remove data from RAM is known as ejection.</p>
   <p>Couchbase Server performs ejections automatically. When ejecting information, it works in conjunction with the disk persistence system to ensure that data in RAM is persisted to disk and can be safely retrieved back into RAM whenever the item is requested. </p>
   <p>In addition to the Data RAM quota for the caching layer, the engine uses two watermarks,
    <parmname>mem_low_wat</parmname> and <parmname>mem_high_wat</parmname>, to determine when it
    needs to start persisting more data to disk.</p>
   <p>As more and more data is held in the caching layer, at some point in time it passes the
    <parmname>mem_low_wat</parmname> value. At this point, no action is taken. As data continues to
    load, it eventually reaches the <parmname>mem_high_wat</parmname> value. At this point, the
    Couchbase Server schedules a background job called item pager which ensures that items are
    migrated to disk and memory is freed up for other Couchbase Server items. This job runs until
    measured memory reaches <parmname>mem_low_wat</parmname>. If the rate of incoming items is
    faster than the migration of items to disk, the system returns errors indicating there is not
    enough space until there is sufficient memory available. The process of migrating data from the
    cache to make way for actively used information is called ejection and is controlled
    automatically through thresholds set on each configured bucket in the Couchbase Server cluster.
    <fig id="fig_cr5_jk5_xs">
     <title>Working set management and ejection</title>
     <image placement="break" href="./images/tunable-memory-bucket-config.png" width="300"
      id="image_dr5_jk5_xs"/>
    </fig></p> 
   <p>Depending on the ejection policy set for the bucket, the vBucket Manager removes just the
    document or both the document, key and the metadata for the item being ejected. Keeping an
    active working set with keys and metadata in RAM serves three important purposes in a system:<ul>
     <li>Couchbase Server uses the remaining key and metadata in RAM if a client requests for that
      key. Otherwise, the node tries to fetch the item from disk and return it into RAM.</li>
     <li>The node can also use the keys and metadata in RAM for <i>miss access</i>. This means that
      it can quickly determine whether an item is missing and if so, perform some action, such as
      add it.</li>
     <li>The expiration process in Couchbase Server uses the metadata in RAM to quickly scan for
      items that have expired and later removes them from disk. This process is known as <i>expiry
       pager</i> and runs every 60 minutes by default.</li>
    </ul></p>
  </section>
  <section><title>Not Recently Used (NRU) Items</title>
   <p>All items in the server contain metadata indicating whether the item has been recently accessed or not. This metadata is known as not-recently-used (NRU). If an item has not been recently used, then the item is a candidate for ejection. When data in the cache exceeds the high water mark (mem_high_wat), the server evicts items from RAM.</p>
   <p>Couchbase Server provides two NRU bits per item and also provides a replication protocol that can propagate items that are frequently read, but not mutated often.</p>
   <p>NRUs are decremented or incremented by server processes to indicate an item that is more
    frequently or less frequently used. The following table lists the bit values with the
    corresponding scores and statuses: <table frame="all" rowsep="1" colsep="1"
     id="table_ekt_2yz_xs">
     <title>Scoring for NRU bit values</title>
     <tgroup cols="4" align="left">
      <colspec colname="c1" colnum="1"/>
      <colspec colname="c2" colnum="2"/>
      <colspec colname="c3" colnum="3"/>
      <colspec colname="c4" colnum="4"/>
      <thead>
       <row>
        <entry>Binary NRU</entry>
        <entry>Score</entry>
        <entry>Access pattern</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>00</entry>
        <entry>0</entry>
        <entry>Set by write access to 00. Decremented by read access or no access.</entry>
        <entry>Most heavily used item.</entry>
       </row>
       <row>
        <entry>01</entry>
        <entry>1</entry>
        <entry>Decremented by read access.</entry>
        <entry>Frequently accessed item.</entry>
       </row>
       <row>
        <entry>10</entry>
        <entry>2</entry>
        <entry>Initial value or decremented by read access.</entry>
        <entry>Default value for new items.</entry>
       </row>
       <row>
        <entry>11</entry>
        <entry>3</entry>
        <entry>Incremented by item pager for eviction.</entry>
        <entry>Less frequently used item.</entry>
       </row>
      </tbody>
     </tgroup>
    </table></p>
   <p>There are two processes that change the NRU for an item:
    <ul>
     <li>When a client reads or writes an item, the server decrements NRU and lowers the item's score.</li>
     <li>A daily process which creates a list of frequently-used items in RAM. After the completion of this process, the server increments one of the NRU bits. </li>
    </ul>
    Because these two processes change NRUs, they play an important role in identifying the candidate items for ejection. </p>
   <p>You can configure the Couchbase Server settings to change the behavior during ejection. For example, you can specify the percentage of RAM to be consumed before items are ejected, or specify whether ejection should occur more frequently on replicated data than on original data. Couchbase recommends that the default settings be used.</p>
  </section>
  <section><title>Understanding the Item Pager</title>
   <p>The item pager process runs periodically to remove documents from RAM. When the amount of RAM
    used by items reaches the high water mark (upper threshold), both active and replica data are
    ejected until the amount of RAM consumed (memory usage) reaches the low water mark (lower
    threshold). Using the default settings, active documents have a 40% chance of eviction while
    replica documents have a 60% chance of eviction. Both the high water mark and low water mark are
    expressed as a percentage amount of RAM, such as 80%.</p>
   <p>You can change the high water mark and low water mark settings for a node by specifying a
    percentage amount of RAM, for example, 80%. Couchbase recommends that you use the following
    default settings: <table frame="all" rowsep="1" colsep="1" id="table_pnj_x21_ys">
     <title>Default setting for RAM water marks</title>
     <tgroup cols="3" align="left">
      <colspec colname="c1" colnum="1"/>
      <colspec colname="c2" colnum="2"/>
      <colspec colname="c3" colnum="3"/>
      <thead>
       <row>
        <entry>Version</entry>
        <entry>High water mark</entry>
        <entry>Low water mark</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>2.0</entry>
        <entry>75%</entry>
        <entry>60%</entry>
       </row>
       <row>
        <entry>2.0.1 and higher</entry>
        <entry>85%</entry>
        <entry>75%</entry>
       </row>
      </tbody>
     </tgroup>
    </table></p>
   <p>The item pager ejects items from RAM in two phases: 
    <ol>
     <li>Eject items based on NRU: The item pager scans NRU for items, creates a list of items with a NRU score 3, and ejects all the identified items. It then checks the RAM usage and repeats the process if the usage is still above the low water mark. </li>
     <li>Eject items based on algorithm: The item pager increments the NRU of all items by 1. For
      every item whose NRU is equal to 3, it generates a random number. If the random number for an
      item is greater than a specified probability, it ejects the item from RAM. The probability is
      based on the current memory usage, low water mark, and whether a vBucket is in an active or
      replica state. If a vBucket is in an active state, the probability of ejection is lower than
      if the vBucket is in a replica state. <table frame="all" rowsep="1" colsep="1"
       id="table_qpb_hg1_ys">
       <title>Probability of ejection based on active vBuckets versus replica vBuckets (using
        default settings)</title>
       <tgroup cols="2" align="left">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <thead>
         <row>
          <entry>Active vBucket</entry>
          <entry>Replica vBucket</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>40%</entry>
          <entry>60%</entry>
         </row>
        </tbody>
       </tgroup>
      </table></li>
    </ol></p>
  </section>
  <section><title>Active Memory Defragmenter</title>
   <p>Over time, the memory used by the managed cache of a running Couchbase Server can become fragmented. The storage engine now includes an <i>Active Defragmenter</i> task to defragment cache memory.</p>
   <p>Cache fragmentation is a side-effect of how Couchbase Server organizes cache memory to maximize performance. Each page in the cache is typically responsible for holding documents of a specific size range. Over time, if memory pages assigned to a specific size range become sparsely populated (due to documents of that size being ejected or items changing in size), then the unused space in those pages cannot be used for documents of other sizes until a complete page is free and that page is re-assigned to a new size. Such effects are highly workload dependent and can result in memory that cannot be used efficiently by the managed cache. </p> 
   <p>The Active Memory Defragmenter attempts to address any fragmentation by periodically scanning the cache to identify pages which are sparsely used, and repacking the items stored on those pages to free up <i>whole</i> pages.</p>
  </section>
  <section><title>High Performance Storage</title>
   <p>The scheduler and the shared thread pool provide high performance storage to the Couchbase Server.</p>
   <dl>
    <dlentry>
     <dt>Scheduler</dt>
     <dd>The scheduler is responsible for managing a shared thread-pool and providing a fair
      allocation of resources to the jobs waiting to execute in the vBucket engine. Shared thread
      pool services requests across all buckets. <p>As an administrator, you can govern the
       allocation of resources by configuring a bucketâ€™s disk I/O prioritization setting to be
       either high or low.</p></dd></dlentry>
    <dlentry>
     <dt>Shared thread pool</dt>
     <dd>A shared thread pool is a collection of threads which are shared across multiple buckets for long running operations such as disk I/O. Each node in the cluster has a thread pool that is shared across multiple vBuckets on the node. Based on the number of CPU cores on a node, the database engine spawns and allocates threads when a node instance starts up.
      <p>Using a shared thread pool provides the following benefits: <ul>
       <li>Better parallelism for worker threads with more efficient I/O resource management. </li>
       <li>Better system scalability with more buckets being serviced with fewer worker threads.</li>
       <li>Availability of task priority if the disk bucket I/O priority setting is used.</li>
      </ul></p></dd> </dlentry>
   </dl>
  </section>
  
  <section>
   
   <title>
    Bloom Filters
   </title>
   
   <p>A Bloom filter is a probabilistic data structure used to test whether an element is a
   member of a set. False positive matches are possible, but false negatives are not. This means a
   query returns either "possibly in set" or "definitely not in set". It is a bit array with a
   predefined size that is calculated based on the expected number of items and the probability of
   false positives or the probability of finding a key that doesn't exist. Bloom filter
   significantly improves the performance of full ejection scenarios and XDCR.  </p>

   <p>In the full ejection mode, the key and metadata are evicted along with the value. Therefore,
    if a key is non resident, there is no way of knowing if a key exists or not, without accessing
    the disk. In such a scenario, if a client issues a lot of GETs on keys that may not even exist
    in server, Bloom filters help eliminate many unnecessary disk accesses. Similarly for XDCR,
    when we set up remote replication to a brand new cluster, we would be able to avoid many
    unnecessary GetMeta-disk-fetches with the help of the bloom filter.</p>
   <p>With Bloom filters, the probability of false positives decreases as the size of the array
    increases and increases as the number of inserted elements increases. Based on the algorithm
    that takes into account the number of keys and the probability of false positives, you can
    estimate the size of the Bloom filter and the number of bits to store each key. </p>
   
   <p>For value eviction only the deleted keys will be stored, while in case of full eviction both
    the deleted keys and non-resident items will be stored. </p>
   <p>The algorithm calculates the almost exact probability of false positives, including the
    number of hash functions (<codeph>k</codeph>), size of the bit array (<codeph>m</codeph>), and
    the number of inserted elements (<codeph>n</codeph>):</p>
   <codeblock>    
    k = m/n (ln 2)</codeblock>
   <p>You can expect an increase in memory usage or memory overhead while using the Bloom filter:
    <table frame="all" rowsep="1" colsep="1" id="table_dkx_rvg_vq">
     <title>Memory overhead for Bloom filter use</title>
     <tgroup cols="3" align="left">
      <colspec colname="c1" colnum="1" colwidth="1.0*"/>
      <colspec colname="c2" colnum="2" colwidth="1.0*"/>
      <colspec colname="c3" colnum="3" colwidth="1.0*"/>
      <thead>
       <row>
        <entry>False positive probability</entry>
        <entry>0.01</entry>
        <entry>0.05</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Estimated number of keys</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
       </row>
       <row>
        <entry>Number of bits per key in the filter</entry>
        <entry>7 bits</entry>
        <entry>4 bits</entry>
       </row>
       <row>
        <entry>Size of the Bloom filter to fit the estimated keys with desired false positive
         probability</entry>
        <entry>95851 bits (=> =12 KB per vBucket) (=> =12 MB for 1024 vBuckets)</entry>
        <entry>62353 bits (=> =8 KB per vBucket) (=> =8 MB for 1024 vBuckets)</entry>
       </row>
      </tbody>
     </tgroup>
    </table></p> 
   
   <p>In a case of full eviction, you will not know whether an item exists in the memory until you
    perform a background fetch. Therefore, use of the Bloom filter helps to avoid unnecessary
    background fetches and improves latency. </p>
   <p>For more information about working set management and eviction, see <xref href="./db-engine-architecture.dita"></xref></p>
  </section>
 </body>
</topic>
