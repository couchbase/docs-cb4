<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_cy4_tr2_xs2">
 <title>Storage</title>
 <shortdesc>When storing data in a Couchbase bucket, the server first writes data to the caching
  layer and eventually stores all data to disk to provide a higher level of reliability. </shortdesc>
 <body>
  <p>The Couchbase Server first writes data to the caching layer and puts the data into a disk write queue to be persisted to disk. Disk persistence enables you to perform backup and restore operations and to grow your datasets larger than the built-in caching layer. This disk storage process is called eventual persistence because the server does not block a client while it writes to disk.</p>
  <p>If a node fails and all data in the caching layer is lost, the items can be recovered from disk. When the server identifies an item that needs to be loaded from disk because it is not in active memory, it places it in a load queue. A background process processes the load queue and reads the information back from disk and into memory. The client waits until the data is loaded back into memory before returning the information.</p>
  <dl>
   <dlentry>
    <dt>Multiple readers and writers</dt>
    <dd>Multithreaded readers and writers provide simultaneous read and write operations for data on
     disk. Simultaneous reads and writes increase I/O throughput. The multithreaded engine includes
     additional synchronization among threads that are accessing the same data cache to avoid
     conflicts. To maintain performance while avoiding conflicts over data, Couchbase Server uses a
     form of locking between threads and thread allocation among vBuckets with static partitioning.
      <p>When Couchbase Server creates multiple reader and writer threads, the server assesses a
      range of vBuckets for each thread and assigns each thread exclusively to certain vBuckets.
      With this static thread coordination, the server schedules threads so that only a single
      reader and single writer thread can access the same vBucket at any given time. The following
      diagram shows six pre-allocated threads and two data buckets. Each thread has the range of
      vBuckets that is statically partitioned for read and write access. <fig id="fig_thd_q1n_xs">
       <title>Bucket disk storage</title>
       <image placement="break" href="./images/bucket-disk-storage.png" width="600"
        id="image_uhd_q1n_xs"/>
      </fig></p></dd>
   </dlentry>
   <dlentry>
    <dt>Item deletion</dt>
    <dd>Items can be deleted explicitly by the client applications or deleted using an expiration flag. Couchbase Server never deletes items from disk unless one of these operations are performed. However, after deletion or expiration, a tombstone is maintained as the record of deletion. Tombstones help communicate the deletion or the expiration to downstream components. Once all downstream components have been notified, the tombstone gets purged as well. </dd>
   </dlentry>
   <dlentry id="tombstone">
    <dt>Tombstone purging</dt>
    <dd>Tombstones are records of expired or deleted items that include item keys and metadata.
      <p>Couchbase Server and other distributed databases maintain tombstones in order to provide
      eventual consistency between nodes and between clusters. Tombstones are records of expired or
      deleted items and they include the key for the item and metadata. Couchbase Server stores the
      key plus several bytes of metadata per deleted item in two structures per node. With millions
      of mutations, the space taken up by tombstones can grow quickly. This is especially the case
      if there are a large number of deletions or expired documents.</p><p>The Metadata Purge
      Interval sets frequency for a node to permanently purge metadata of deleted and expired items.
      The Metadata Purge Interval setting runs as part of auto-compaction. This helps reduce the
      storage requirement by roughly 3x times than before and also frees up space much faster.</p></dd> </dlentry>
  </dl>
  
  <section id="disk-priority"><title>Disk I/O priority</title>
   <p>Disk I/O priority enables workload priorities to be set at the bucket level.</p>
   <p>You can configure the bucket priority settings at the bucket level and set the value to be either high or low. Bucket priority settings determine whether I/O tasks for a bucket must be queued in the low or high priority task queues. Threads in the global pool poll the high priority task queues more often than the low priority task queues. When a bucket has a high priority, its I/O tasks are picked up at a higher frequency and thus, processed faster than the I/O tasks belonging to a low priority bucket.</p>
   <p>You can configure the bucket I/O priority settings during initial setup and change the
    settings later, if needed. However, changing a bucket I/O priority after the initial setup
    results in a restart of the bucket and the client connections are reset. <fig
     id="fig_hvl_jq2_2t">
     <title>Create bucket settings</title>
     <image placement="break" href="./images/create-bucket-settings.png" width="600"
      id="image_ivl_jq2_2t"/>
    </fig></p>
   <p>The previous versions of Couchbase Server, version 3.0 or earlier, required the I/O thread
    allocation per bucket to be configured manually. However, when you upgrade from a 2.x version to
    a 3.x or higher version, Couchbase Server converts an existing thread value to either a high or
    low priority based on the following criteria: <ul>
     <li>Buckets allocated six to eight (6-8) threads in Couchbase Server 2.x are marked high
      priority in bucket setting after the upgrade to 3.x or later.</li>
     <li>Buckets allocated three to five (3-5) threads in Couchbase Server 2.x are marked low
      priority in bucket settings after the upgrade to 3.x or later.</li>
    </ul></p>
   
  </section>
  <section><title>Monitoring Scheduler</title>
   <p>You can use the <cmdname>cbstats</cmdname> command with the <parmname>raw workload</parmname> option to view the status of the threads as shown in the following example.
    <codeblock># cbstats 10.5.2.54:11210 -b default raw workload
     
     ep_workload:LowPrioQ_AuxIO:InQsize:   3
     ep_workload:LowPrioQ_AuxIO:OutQsize:  0
     ep_workload:LowPrioQ_NonIO:InQsize:   33
     ep_workload:LowPrioQ_NonIO:OutQsize:  0
     ep_workload:LowPrioQ_Reader:InQsize:  12
     ep_workload:LowPrioQ_Reader:OutQsize: 0
     ep_workload:LowPrioQ_Writer:InQsize:  15
     ep_workload:LowPrioQ_Writer:OutQsize: 0
     ep_workload:num_auxio:                1
     ep_workload:num_nonio:                1
     ep_workload:num_readers:              1
     ep_workload:num_shards:               4
     ep_workload:num_sleepers:             4
     ep_workload:num_writers:              1
     ep_workload:ready_tasks:              0
     ep_workload:shard0_locked:            false
     ep_workload:shard0_pendingTasks:      0
     ep_workload:shard1_locked:            false
     ep_workload:shard1_pendingTasks:      0
     ep_workload:shard2_locked:            false
     ep_workload:shard2_pendingTasks:      0
     ep_workload:shard3_locked:            false
     ep_workload:shard3_pendingTasks:      0 </codeblock> </p>
  </section>
  
  <section><title>High Performance Storage</title>
   <p>The scheduler and the shared thread pool provide high performance storage to the Couchbase Server.</p>
   <dl>
    <dlentry>
     <dt>Scheduler</dt>
     <dd>The scheduler is responsible for managing a shared thread-pool and providing a fair
      allocation of resources to the jobs waiting to execute in the vBucket engine. Shared thread
      pool services requests across all buckets. <p>As an administrator, you can govern the
       allocation of resources by configuring a bucketâ€™s disk I/O prioritization setting to be
       either high or low.</p></dd></dlentry>
    <dlentry>
     <dt>Shared thread pool</dt>
     <dd>A shared thread pool is a collection of threads which are shared across multiple buckets for long running operations such as disk I/O. Each node in the cluster has a thread pool that is shared across multiple vBuckets on the node. Based on the number of CPU cores on a node, the database engine spawns and allocates threads when a node instance starts up.
      <p>Using a shared thread pool provides the following benefits: <ul>
       <li>Better parallelism for worker threads with more efficient I/O resource management. </li>
       <li>Better system scalability with more buckets being serviced with fewer worker threads.</li>
       <li>Availability of task priority if the disk bucket I/O priority setting is used.</li>
      </ul></p></dd> </dlentry>
   </dl>
  </section>
  
 </body>
</topic>
