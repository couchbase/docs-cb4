<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_ukb_wgg_ps">
 <title>Data management</title>
	<shortdesc>
		Placeholder text to have something here for testing.</shortdesc>
 <conbody>
  <section><title> Atomicity properties</title>
  <p>
   Couchbase server provides atomicity at the document level. A document write will either clearly succeed or clearly fail. It is impossible to end up with a document write that partially succeeds, for example, recording only some changed fields but not others. Couchbase deliberately does not support cross document transactions in order to maintain strong consistency and deliver the best performance while maximizing horizontal scalability.</p>
   
   <p>The need for transactions can often be avoided by consolidating often-accessed information into a single document, or addressed using other techniques. 
  </p></section>
  <section><title> Consistency and durability</title>
   <p>
    When talking about the CAP Theorem, distributed data management systems are generally considered to be either CP or AP, that is, evaluators focus on a tradeoff in either consistency (C) or availability (A). Couchbase Server behaves like a CP system in its default configuration and running as a single cluster. This is because any access to a given key (read, write, update, delete) is always directed to the node that hosts that active data at that point in time.  The application client library transparently distributes requests to the appropriate nodes and therefore every application server/thread will immediately read the writes of any other application server/thread.  Any write is also replicated within the cluster, but these replicas are primarily for the purpose of high availability and by default do not service any traffic until made active.</p>
    
    <p>According to the CAP Theorem, a network partition cannot be distinguished from a failure of a part of the system. With Couchbase, in the event of a failure of one node In the event of a network partition, in which one part of the Couchbase cluster can no longer communicate with another part of the cluster, Couchbase Server will not accept atomic writes. Instead it will timeout or return an error message rather than accepting the write operation at the risk of inconsistent data. </p>
    
    <p>If a single node dies, the data on a node that dies will not accept writes until the node is failed over (but reads can be serviced from replicas). </p>
    
    <p>Couchbase Server enables users to increase availability by replicating data between multiple Couchbase Server clusters running in the same or separate data centers using a capability called cross datacenter replication (XDCR, described later). With XDCR the state of information on both clusters will eventually be made consistent, and in the meantime Couchbase remains available to take read and write traffic. </p>
    
    <p>Although the CAP Theorem is a useful high-level formulation of principles, a full discussion of how Couchbase Server behaves and recovers from various distributed failure conditions is beyond the scope of this paper, especially given the various configuration and programming options available. For a more thorough discussion of the CAP Theorem as it applies to Couchbase Server, see here.
   </p></section>
  <section> <title>Eventual consistency for indexes and replicas</title>
   <p> 
    Consistency for indexes and replicas follows an eventual consistency model: if there are no new updates to the system, eventually all readers will see the most recent value. Indexes are built incrementally, so after the initial build Couchbase Server can immediately respond to a request, or it can be made to catch up to the point in time when the query was issued. A developer can choose how fresh the index data is at the time of query by setting the “stale” property. This offers a high degree of configurability, from requiring that Couchbase Server process all updates before issuing a response to allowing Couchbase Server to immediately respond to a request using the current state of the index at the time of the query, or somewhere in between. The range of options allows applications to be as fast as possible unless there is a hard requirement to have the freshest information in the index.
   </p> </section>
  <section> <title> Isolation level</title>
   <p> 
    TO DO
   </p> </section>
  <section> <title> Tunable durability requirements </title>
   <p>By default, writes are asynchronous and the data manager sends an acknowledge message (ACK) to a client as soon as an update is in RAM. Once a write is in memory, the data manager immediately adds it to replication, disk, and indexing queues. Replication is RAM-based, so it is extremely fast. To increase tolerance to failures at the cost of increased latency, Couchbase can acknowledge the change to the application only after the update is also  replicated, persisted to disk or both. Data is replicated to 1, 2, or 3 nodes for a total of up to four copies and saved to disk, regardless of whether the ACK message is held up for those operations.  
   </p> </section>
  <section> <title> Document expiration</title>
   <p> 
    Programmers can use Time to Live (TTL) to set an expiration time for a document. This is most commonly used with ephemeral data such as stored user sessions. When this optional value is set, the Couchbase Server will delete values during regular maintenance if the TTL for an item has expired. Documents can also be ‘touched’, updating their expiration time without modifying their contents. By default documents do not have TTLs and do not expire. TTL is one mechanism that enables Couchbase Server to manage its own capacity.
   </p> </section>
 </conbody>
</concept>
