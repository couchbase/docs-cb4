<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="spark-dev-workflow">
	<title>Development Workflow</title>
	<shortdesc>Developing and Deploying Spark applications can be a challenge upfront. This section helps
  you through the development and deployment workflow.</shortdesc>
	<conbody>
		<p>When developing a Spark application that uses external dependencies, typically there are two
    challenges a developer is confronted with:</p>

    <ul>
      <li>How to efficiently and quickly develop locally.</li>
      <li>How to reliably deploy to a production cluster.</li>
    </ul>

    <p>Apart from the actual development, the biggest hurdle most of the time is
      the dependency management aspect. This documentation is intended to guide
      you through and to help you get set up as needed.</p>

      <p>There are two ways the problem is typically approached: you can either
        add the dependencies to the classpath when you submit the application or
        you can create a big jar which contains all dependencies.</p>

		<section>
			<title>Adding the Connector to the Executor's classpath</title>

      <p>If you want to manage the dependency directly on the worker, the
      actual project setup is quite simple. You don't need to set up shadowing
      and can use a <codeph>build.sbt</codeph> like this:</p>

<codeblock outputclass="language-scala"><![CDATA[name := "your-awesome-app"

organization := "com.acme"

version := "1.0.0-SNAPSHOT"

scalaVersion := "2.11.8"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "2.1.0",
  "org.apache.spark" %% "spark-streaming" % "2.1.0",
  "org.apache.spark" %% "spark-sql" % "2.1.0",
  "com.couchbase.client" %% "spark-connector" % "2.1.0"
)]]></codeblock>

    <p>when developing the application you need to make sure that the <codeph>master</codeph>
    is not set, since you need to deploy it eventually to a non-local master.</p>

		<p>Note that since Spark 2.0 it is recommended to use the <codeph>SparkSession</codeph>
		instead of initializing the <codeph>SparkContext</codeph> directly, but it is still supported
		and behaves the same like noted below.</p>

<codeblock outputclass="language-scala"><![CDATA[object Main {
  def main(args: Array[String]): Unit = {
    // Configure Spark
    val cfg = new SparkConf()
      .setAppName("MyApp")
      // .setMaster("local[*]") <--- do not set the master!
      .set("com.couchbase.bucket.travel-sample", "")

    // Generate The Context
    val sc = new SparkContext(cfg)

    // ...
  }
}
]]></codeblock>

    <p>If you run this, Spark will complain:</p>

<codeblock><![CDATA[Exception in thread "main" org.apache.spark.SparkException: A master URL must be set in your configuration]]></codeblock>

    <p>There are different approaches to handle this, but all boil down to conditionally setting the system property for the master. One
      approach that is useful is to have a runnable class in the <codeph>test</codeph> (or somewhere else) directory which sets the property
      and then runs the application. This also has additional benefits that are discussed in the next section.</p>

<codeblock outputclass="language-scala"><![CDATA[object RunLocal {
  def main(args: Array[String]): Unit = {
    System.setProperty("spark.master", "local[*]")
    Main.main(args)
  }
}]]></codeblock>

    <p>You can then build your application via <codeph>sbt package</codeph> on the command line and
        submit it to Spark via <codeph>spark-submit</codeph>. Because Couchbase distributes the
        connector via maven, you can just include it with the <codeph>--packages</codeph> flag:</p>

<codeblock><![CDATA[/spark/bin/spark-submit --master spark://your-spark-master:7077 --packages com.couchbase.client:spark-connector_2.10:1.2.1 /path/to/app/target/scala-2.10/your_app_2.10-1.0.jar]]></codeblock>

  <p>If your environment does not have access to the internet you can use the <codeph>--jars</codeph> argument instead and grab the assembly with
  all the dependencies from here: <xref href="download-links.dita"/>.</p>

    </section>

    <section>
			<title>Deploying a jar with dependencies included</title>

      <p>The previous example shows how to add the connector during the submit phase as a
        dependency. If more than one dependency needs to be managed this can become cumbersome
        quickly since you also need to make sure that your <codeph>build.sbt</codeph> is always in
        sync with your command line parameters.</p>

    <p>The alternative is to create a "fat jar" with batteries included that ships everything in the first place. This requires more setup
      on the project side but saves you some work later on.</p>

    <p>The <codeph>build.sbt</codeph> looks a bit different here:</p>


<codeblock outputclass="language-scala"><![CDATA[lazy val root = (project in file(".")).
  settings(
    name := "my-app",
    version := "1.0",
    scalaVersion := "2.11.8",
    mainClass in Compile := Some("Main")
  )

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "2.1.0" % "provided,test",
  "org.apache.spark" %% "spark-sql" % "2.1.0" % "provided,test",
  "com.couchbase.client" %% "spark-connector" % "2.1.0"
)]]></codeblock>

  <p>The important piece here is that the Spark dependencies are scoped to <codeph>provided</codeph>
        and <codeph>test</codeph>. We need this to not include Spark in the shadowed jar but still
        allow it to run in the IDE for development.</p>

  <p>Also, create <codeph>project/assembly.sbt</codeph> if it doesn't exist and add:</p>

<codeblock><![CDATA[addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.3")]]></codeblock>

  <p>The code now is similar to the previous section, but you need to make sure that the class you
        run in your IDE is in the test namespace so that Spark is actually included during
        development:</p>

  <p>In <codeph>src/main/scala</codeph>:</p>

<codeblock outputclass="language-scala"><![CDATA[object Main {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
      .setAppName("TravelCruncher")
      .set("com.couchbase.bucket.travel-sample", "")

    val sc = new SparkContext(conf)

    // ...
  }
}]]></codeblock>

  <p>Under <codeph>src/test/scala</codeph>:</p>

<codeblock outputclass="language-scala"><![CDATA[object RunLocal {
  def main(args: Array[String]): Unit = {
    System.setProperty("spark.master", "local[*]")
    Main.main(args)
  }
}]]></codeblock>

    <p>Now instead of <codeph>sbt package</codeph> you need to use <codeph>sbt assembly</codeph>
    which will create the shadowed jar under <codeph>target/scala_2.1X/your-app-assembly-1.0.jar</codeph>.</p>

    <p>The jar can be submitted without further dependency shenanigans:</p>

<codeblock><![CDATA[/spark/bin/spark-submit --master spark://your-spark-master:7077 /path/to/app/target/scala-2.10/your_app-assembly-1.0.jar]]></codeblock>

    </section>

	</conbody>
</concept>
