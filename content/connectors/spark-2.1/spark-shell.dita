<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE concept
  PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept xml:lang="en-us" id="concept227">
	<title>Using the Spark Shell</title>
	<shortdesc>The interactive shell can be used together with the couchbase connector for quick and easy data exploration.</shortdesc>

	<conbody>

		<section>
			<title>Getting Started</title>

			<p>The Spark shell provides an easy and convenient way to prototype certain operations
				quickly,without having to develop a full program, packaging it and then deploying
				it.</p>

			<p>You need to <xref href="https://spark.apache.org/downloads.html" format="html" scope="external">download</xref> Apache Spark from the website, then navigate into the <codeph>bin</codeph> directory and run the <codeph>spark-shell</codeph> command:</p>

<codeblock outputclass="language-scala"><![CDATA[michael@daschlbook ~/spark/spark-2.1.0-bin-hadoop2.7/bin $ ./spark-shell -h
Usage: ./bin/spark-shell [options]

Options:
  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.
  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally ("client") or
                              on one of the worker machines inside the cluster ("cluster")
                              (Default: client).
  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
  --name NAME                 A name of your application.
  --jars JARS                 Comma-separated list of local jars
  ....]]></codeblock>


  			<p>If you run the Spark shell as it is, you will only have the built-in Spark commands
				available. If you want to use it with the Couchbase Connector, the easiest way is to
				provide a specific argument that locates the dependency and pulls it in:</p>


<codeblock outputclass="language-scala"><![CDATA[./spark-shell --packages com.couchbase.client:spark-connector_2.11:2.1.0]]></codeblock>

			<p>Now you're all set!</p>
		</section>

		<section>
			<title>Usage</title>

			<p>Once you've loaded the shell, both the <codeph>SparkContext (sc)</codeph> and the surrounding <codeph>SparkSession</codeph> are ready to go:</p>

<codeblock outputclass="language-scala"><![CDATA[scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@708dfe10

scala> spark
res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1716c037]]></codeblock>

			<p>To load the Couchbase-specific implicit imports, run the following commands:</p>

<codeblock outputclass="language-scala"><![CDATA[scala> import com.couchbase.spark._
import com.couchbase.spark._

scala> import com.couchbase.spark.sql._
import com.couchbase.spark.sql._]]></codeblock>

			<p>Now you can run all commands like in a regular program, just in an interactive fashion. The
				following example stores a document and then retrieves it through KeyValue:</p>

<codeblock outputclass="language-scala"><![CDATA[scala> import com.couchbase.client.java.document.JsonDocument
import com.couchbase.client.java.document.JsonDocument

scala> import com.couchbase.client.java.document.json.JsonObject
import com.couchbase.client.java.document.json.JsonObject

scala> sc.parallelize(Seq(JsonDocument.create("mydoc", JsonObject.create().put("hello", "spark")))).saveToCouchbase()
...

scala> sc.parallelize(Seq("mydoc")).couchbaseGet[JsonDocument]().foreach(println)
...
JsonDocument{id='mydoc', cas=39773408329728, expiry=0, content={"hello":"spark"}, mutationToken=null}
...]]></codeblock>

		<p>If you need to pass custom properties (like connect to a different bucket or hostnames) you can do it like this:</p>


<codeblock><![CDATA[./spark-shell --packages com.couchbase.client:spark-connector_2.11:2.1.0 --conf "spark.couchbase.bucket.travel-sample="]]></codeblock>


		<p>You can also make use of the first-class N1QL integration. The following example creates a data frame for airlines <codeph>travel-sample</codeph> bucket.</p>


<codeblock outputclass="language-scala"><![CDATA[scala> val airlines = spark.read.couchbase(schemaFilter = org.apache.spark.sql.sources.EqualTo("type", "airline"))
15/10/20 15:02:51 INFO N1QLRelation: Inferring schema from bucket travel-sample with query 'SELECT META(`travel-sample`).id as `META_ID`, `travel-sample`.* FROM `travel-sample` WHERE  `type` = 'airline' LIMIT 1000'
...
15/10/20 15:02:52 INFO N1QLRelation: Inferred schema is StructType(StructField(META_ID,StringType,true), StructField(callsign,StringType,true), StructField(country,StringType,true), StructField(iata,StringType,true), StructField(icao,StringType,true), StructField(id,LongType,true), StructField(name,StringType,true), StructField(type,StringType,true))
airlines: org.apache.spark.sql.DataFrame = [META_ID: string, callsign: string, country: string, iata: string, icao: string, id: bigint, name: string, type: string]]]></codeblock>


		<p>Now you can print the schema and run ad-hoc data exploration:</p>

<codeblock outputclass="language-scala"><![CDATA[scala> airlines.printSchema
root
 |-- META_ID: string (nullable = true)
 |-- callsign: string (nullable = true)
 |-- country: string (nullable = true)
 |-- iata: string (nullable = true)
 |-- icao: string (nullable = true)
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
 |-- type: string (nullable = true)]]></codeblock>


<codeblock outputclass="language-scala"><![CDATA[scala> airlines.select("name", "callsign").sort(airlines("callsign").desc).show(5)
...
+----------------+--------+
|            name|callsign|
+----------------+--------+
|     Aws express|     aws|
|          Atifly|  atifly|
|        XAIR USA|    XAIR|
|   World Airways|   WORLD|
|Western Airlines| WESTERN|
+----------------+--------+]]></codeblock>



		</section>

	</conbody>
</concept>
