<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="spark-structured-streaming">
	<title>Structured Streaming Integration</title>
	<shortdesc>Couchbase allows you to integrate with Spark Structured
    Streaming as a Source as well as a Sink, making it possible to query
    incoming data in a structural and efficient manner.</shortdesc>
	<conbody>
    <section>
      <title>Couchbase as a Structured Streaming Source</title>

      <p>Since Spark 2.0 it is possible to combine Spark Streaming and Spark
        SQL to what is called "structured streaming". You can think of it as
        a way to operate on batches of a DataFrame where each row is stored
        in an every growing append-only table. You can use it for all kinds of
        analysis, including aggregations.</p>

        <p>The first thing you need to define is the <codeph>SparkSession</codeph>
        as usual:</p>

<codeblock outputclass="language-scala"><![CDATA[// The SparkSession is the main entry point into spark
val spark = SparkSession
  .builder()
  .appName("N1QLExample")
  .master("local[*]") // use the JVM as the master, great for testing
  .config("spark.couchbase.nodes", "127.0.0.1") // connect to couchbase on localhost
  .config("spark.couchbase.bucket.travel-sample", "") // open the travel-sample bucket with empty password
  .getOrCreate()]]></codeblock>

        <p>Next up you need to define a schema. Note that all the records coming in don't need to
          fit the schema, attributes not in the schema are ignored and those that do not existed are
          represented as null:</p>

<codeblock outputclass="language-scala"><![CDATA[// Very simple schema, feel free to add more properties here. Properties that do not
// exist in a streamed document show as null.
val schema = StructType(
  StructField("META_ID", StringType) ::
  StructField("type", StringType) ::
  StructField("name", StringType) :: Nil
)]]></codeblock>

        <p>Now we define the format and start the stream from Couchbase:</p>

<codeblock outputclass="language-scala"><![CDATA[// Define the Structured Stream from Couchbase with the given Schema
val records = spark.readStream
  .format("com.couchbase.spark.sql")
  .schema(schema)
  .load()]]></codeblock>

      <p>Since it is lazy we also need to consume it. In this simple example
        we use the built-in logger which dumps everything to the screen. This
        example also performs grouping first, so it groups every incoming record
        by the <codeph>type</codeph> field and counts them up:</p>

<codeblock outputclass="language-scala"><![CDATA[// Count per type and print to screen
records
  .groupBy("type")
  .count()
  .writeStream
  .outputMode("complete")
  .format("console")
  .start()
  .awaitTermination()]]></codeblock>

      <p>If all goes well you'll see this on the console:</p>

<codeblock><![CDATA[+--------+-----+
|    type|count|
+--------+-----+
|   hotel|  917|
|    null|    1|
|landmark| 4495|
| airline|  187|
| airport| 1968|
|   route|24024|
+--------+-----+]]></codeblock>

    <p>Since it keeps the counts as a total value, if you then modify a
      document in the UI, for example a airport you'll see the airport type
      count increasing by one.</p>

    <p>Nearly all of the implementation details are hidden, so please consult
      the Spark documentation on Structured Streaming for more information. By
      default if you specify an <codeph>META_ID</codeph> field in your schema
      it will set it as the document ID. You can customize this field via the
      <codeph>idField</codeph> param. Also if you want to start streaming at
      the current point in time set the <codeph>streamFrom</codeph> param to
      <codeph>now</codeph>, by default it will start streaming at the very
      beginning of the bucket.</p>

    </section>

		<section>
			<title>Couchbase as a Structured Streaming Sink</title>

      <p>In addition to streaming from Couchbase you can also use it as a Sink and
        store the results of your structured streaming transformation inside
        Couchbase.</p>

      <p>Note that since it is a safe streaming source you need to provide it
        with a checkpoint location, ideally in a HDFS-compatible file system.</p>

      <p>The following example builds on the Spark sample when streaming from a
        network socket, doing word count and then storing the result in Couchbase.</p>

      <p>Define your <codeph>SparkSession</codeph> and create the socket stream:</p>

<codeblock outputclass="language-scala"><![CDATA[val spark = SparkSession
  .builder
  .master("local[*]")
  .appName("StructuredWordCount")
  .getOrCreate()

import spark.implicits._

val lines = spark.readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 5050)
  .load()]]></codeblock>

     <p>Now perform the word count on the split words:</p>

<codeblock outputclass="language-scala"><![CDATA[val words = lines.as[String].flatMap(_.split(" "))

val wordCounts = words.groupBy("value").count()]]></codeblock>

   <p>Write the result into Couchbase, here we also define that the
     document ID is extracted from the <codeph>value</codeph> out of the
     dataframe:</p>

<codeblock outputclass="language-scala"><![CDATA[val query = wordCounts.writeStream
  .outputMode("complete")
  .option("checkpointLocation", "mycheckpointlocation")
  .option("idField", "value")
  .format("com.couchbase.spark.sql")
  .start()

query.awaitTermination()]]></codeblock>

    </section>

	</conbody>
</concept>
