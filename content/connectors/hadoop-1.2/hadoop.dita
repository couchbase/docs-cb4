<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept xml:lang="en-us" id="hadoop-1.2">
	<title>Hadoop Connector 1.2</title>
	<shortdesc>The Couchbase Hadoop Connector allows you to connect to Couchbase Server 2.5, 3.x or 4.x
		to stream keys into Hadoop Distributed File System (HDFS) or Hive for processing with
		Hadoop. </shortdesc>
	<conbody>
		<p>If you have used <xref href="http://sqoop.apache.org" format="html" scope="external">Apache
				Sqoop</xref> before with other databases, using this connector should be straightforward
			because it uses a similar command line argument structure. Some arguments might seem
			slightly different because Couchbase has a very different structure than a typical
			RDBMS.</p>
	
	<section id="hadoop-get-started">
			<title>Getting Started</title>
			<p>Download the Couchbase Hadoop Connector version 1.2.0 from <xref
					href="http://packages.couchbase.com/clients/connectors/couchbase-hadoop-plugin-1.2.0.zip"
					format="zip" scope="external"
					>http://packages.couchbase.com/clients/connectors/couchbase-hadoop-plugin-1.2.0.zip</xref>.</p>
			<p>The Couchbase Hadoop Connector is supported on Cloudera 5. <xref
					href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html"
					format="html" scope="external">Cloudera</xref> has certified the Couchbase Hadoop
				Connector 1.2 release for Cloudera 5.</p>
			<p>The Couchbase Hadoop Connector is supported on Hortonworks Data Platform (HDP) 2.2.
					<xref href="http://hortonworks.com/hdp/" format="html" scope="external"
					>Hortonworks</xref> has certified the Couchbase Hadoop Connector 1.2 release for HDP
				2.2.</p>
		</section>
		
	<section id="hadoop-install">
			<title>Installation </title><p>You can install the Couchbase Hadoop Connector either by
				running a script or by manually copying the files to specified directories within
				your Sqoop installation. The distribution package contains a set of files that need
				to be copied into your Sqoop installation and a script that copies the files for you
				if you provide the path to the Sqoop installation.</p><p>The following table
				describes the files in the Couchbase Hadoop Connector distribution and lists where
				each file is installed. In the installation location column,
					<codeph>$sqoop_home</codeph> represents the path to your Sqoop installation.</p><p>
				<table frame="all" rowsep="1" colsep="1" id="file-list">
					<title id="files-in-hadoop">Files in the Couchbase Hadoop Connector
						package</title>
					<tgroup cols="3" align="left">
						<colspec colname="c1" colnum="1" colwidth="1.16*"/>
						<colspec colname="c2" colnum="2" colwidth="1.3*"/>
						<colspec colname="c3" colnum="3" colwidth="1*"/>
						<thead>
							<row>
								<entry>File name</entry>
								<entry>Description</entry>
								<entry>Installation location</entry>
							</row>
						</thead>
						<tbody>
							<row>
								<entry><filepath>couchbase-client-1.4.4.bundled.jar</filepath></entry>
								<entry>A library dependency of the connector that handles the basic
									communications with the Couchbase cluster</entry>
								<entry><filepath>$sqoop_home/lib</filepath></entry>
							</row>
							<row>
								<entry><filepath>couchbase-config.xml</filepath></entry>
								<entry>Property file used to register a
										<codeph>ManagerFactory</codeph> for the connector with
									Sqoop</entry>
								<entry><filepath>$sqoop_home/conf</filepath></entry>
							</row>
							<row>
								<entry><filepath>couchbase-hadoop-plugin-1.2.0.jar</filepath></entry>
								<entry>Couchbase Hadoop Connector</entry>
								<entry><filepath>$sqoop_home/lib</filepath></entry>
							</row>
							<row>
								<entry><filepath>couchbase-manager.xml</filepath></entry>
								<entry>Property file that tells Sqoop where the
										<codeph>ManagerFactory</codeph> defined in the
									couchsqoop-config.xml resides</entry>
								<entry><filepath>$sqoop_home/conf/managers.d</filepath></entry>
							</row>
							<row>
								<entry><filepath>install.sh</filepath></entry>
								<entry>Couchbase Hadoop Connector installation script</entry>
								<entry>Not applicable</entry>
							</row>
							<row>
								<entry><filepath>jettison-1.1.jar</filepath></entry>
								<entry>A dependency of the Couchbase client</entry>
								<entry><filepath>$sqoop_home/lib</filepath></entry>
							</row>
							<row>
								<entry><filepath>netty-3.5.5.Final.jar</filepath></entry>
								<entry>A dependency of the Couchbase client</entry>
								<entry><filepath>$sqoop_home/lib</filepath></entry>
							</row>
							<row>
								<entry><filepath>spymemcached-2.11.4.jar</filepath></entry>
								<entry>A library dependency of the Couchbase client that provides
									networking and core protocol handling for data transfer</entry>
								<entry><filepath>$sqoop_home/lib</filepath></entry>
							</row>
						</tbody>
					</tgroup>
				</table>
			</p>
			<b>Script-based Installation</b>
			<p>Script-based installation is done through the use of the
					<filepath>install.sh</filepath> script that comes with the connector download.
				The script takes one argument, the path to your Sqoop installation. The basic
				command format for invoking the script is:
				</p><codeblock>shell&gt; sh install.sh path_to_sqoop_home</codeblock><p>In an HDP
				deployment, Sqoop is located at <filepath>/usr/hdp/current/sqoop-client</filepath>,
				which you use as the path to the Sqoop installation. For HDP, invoke the
				installation script as follows: </p><codeblock>shell&gt; sh install.sh /usr/hdp/current/sqoop-client</codeblock>
			<b>Manual Installation</b>
			<p>To install the Couchbase Hadoop Connector manually, copy each JAR and XML file listed
				in the table   <xref href="#hadoop-1.2/files-in-hadoop" format="dita">Files in the
					Couchbase Hadoop Connector package</xref> into the directory specified in the
				installation location column.</p>
			<b>Uninstallation</b>
			<p>Uninstallation of the connector requires removal of all of the files that were added
				to Sqoop during installation. To uninstall the files, <cmdname>cd</cmdname> into
				your Sqoop home directory and execute the following
			command:</p><codeblock>shell&gt; rm lib/couchbase-hadoop-plugin-1.2.0.jar \
    lib/spymemcached-2.11.4.jar \
    lib/jettison-1.1.jar \
    lib/netty-3.5.5.Final.jar \
    lib/couchbase-client-1.4.4.bundled.jar \
    conf/couchbase-config.xml \
    conf/managers.d/couchbase-manager.xml
 </codeblock></section>
	<section id="using">
			<title>Using Sqoop</title><p>The Couchbase Hadoop Connector can be used with a variety
				of command line tools provided by Sqoop. In this section we discuss the usage of
				each tool.</p>
			<b>Tables</b>
			<p>Since Sqoop is built for a relational model it requires that the user specifies a
				table to import and export into Couchbase. The Couchbase Hadoop Connector uses the
					<codeph>&#x2011;&#x2011;table</codeph> option to specify the type of data stream
				for importing and exporting into Couchbase. </p><p>For exports the user must enter a
				value for the <codeph>--table</codeph> option though what is entered will not be
				used by the connector.</p><p>For imports the table command accepts two values and
				will exit reporting errors with invalid input. </p><ul>
				<li>
					<p><codeph>DUMP</codeph>—Causes all keys currently in Couchbase to be read into
						HDFS. Any data items which are received by the Couchbase cluster while this
						command is running will also be passed along by the connector meaning new or
						changed items are part of the dump. However, items removed while the dump is
						running will not be removed from the output.</p>
				</li>
				<li>
					<p><codeph>BACKFILL_##</codeph>—Streams all key mutations for a given amount of
						time (in minutes). This is best used to sample a bucket in a cluster for a
						period of time.</p>
				</li>
			</ul><p>For the <codeph>--table</codeph> value for the <codeph>BACKFILL</codeph> table
				that a time should be put in place of the brackets. For example
					<codeph>BACKFILL_5</codeph> means stream key mutations in the Couchbase server
				for 5 minutes and then stop the stream.</p>
			<b>Connect string</b>
			<p>A connect string option is required to connect to Couchbase. This can be specified
				with <codeph>--connect</codeph> as an argument to the <cmdname>sqoop</cmdname>
				command. The following example shows some connect
				strings:</p><codeblock><codeph>http://10.2.1.55:8091/pools
http://10.2.1.55:8091/pools,http://10.2.1.56:8091/pools
</codeph></codeblock><p>When
				creating your connect strings, replace the IP address above with the host name or IP
				address of one or more nodes in your Couchbase Cluster. If you have multiple servers
				you can list them in a comma-separated list.</p>
			<b>Connecting to different buckets</b>
			<p>By default the Couchbase Hadoop Connector connects to the <codeph>default</codeph>
				bucket. If you want to connect to a bucket other than the default bucket you can
				specify the bucket name with the <codeph>--username</codeph> option. If the bucket
				has a password, use the <codeph>--password</codeph> option followed by the
				password.</p><p>Note that there are several variations on how the password may be
				specified to Sqoop. The <codeph>-P</codeph> argument allows the password to be
				supplied on the command line and the <codeph>--password-file</codeph> argument
				allows the password to be read from a file in HDFS. </p>
			<b>Importing</b>
			<p>Importing data to your cluster requires the use of the Sqoop import command followed
				by the parameters <codeph>--connect</codeph> and <codeph>--table</codeph>.</p><p>The
				following example command dumps all items from Couchbase into HDFS. Since the
				Couchbase Java Client has support for a number of different data types, all values
				are normalized to strings when being written to a Hadoop text
				file.</p><codeblock>shell&gt; sqoop import --connect http://10.2.1.55:8091/pools --table DUMP</codeblock><p>The
				following example command streams all item mutations from Couchbase into HDFS for a
				period of 10 minutes.
				</p><codeblock>shell&gt; sqoop import --connect http://10.2.1.55:8091/pools --table BACKFILL_10</codeblock><p>Sqoop
				provides many more options to the import command than are covered in this document.
				Run <codeph>sqoop import help</codeph> for a list of all options and see the Sqoop
				documentation for more details about these options.</p><p>You have a number of
				options for how to supply the password when accessing a bucket. The following
				examples are equivalent for a bucket named <codeph>mybucket</codeph> that uses the
				password <codeph>mypassword</codeph>, given the argument to
					<codeph>--password-file</codeph> contains the password without a newline or
				carriage return.
				</p><codeblock>shell&gt; sqoop import --username mybucket -P --verbose \
    --connect http://10.2.1.55:8091/pools --table DUMP</codeblock><codeblock>shell&gt; sqoop import --username mybucket --password mypassword --verbose \
    --connect http://10.2.1.55:8091/pools --table DUMP</codeblock><codeblock>shell&gt; sqoop import --username mybucket --password-file passwordfile \ 
    --verbose --connect http://10.2.1.55:8091/pools --table DUMP</codeblock><p>Some
				options that may be important in your import are those that define what delimiters
				Sqoop uses when writing the records. The default is the comma (<codeph>,</codeph>)
				character. Through the <cmdname>sqoop</cmdname> command you may specify a different
				delimiter if, for instance, it’s likely that the item’s key or value may contain a
				comma.</p>
			<p>When the import job executes, it also generates a <codeph>.java</codeph> source code
				file that can facilitate reading and writing the records imported by other Hadoop
				MapReduce jobs. If, for instance, the job run was a <codeph>DUMP</codeph>, Sqoop
				generates a <filepath>DUMP.java</filepath> source code file.</p>
			<b>Exporting</b>
			<p>Exporting data to your cluster requires the use of the <codeph>sqoop export</codeph>
				command followed by the parameters <codeph>--connect</codeph>,
					<codeph>--export-dir</codeph>, and <codeph>--table</codeph>.</p>
			<p>The following example exports all records from the files in the HDFS directory
				specified by <codeph>--export-dir</codeph> into Couchbase.</p>
			<codeblock>shell&gt; sqoop export --connect http://10.2.1.55:8091/pools \
    --table couchbaseExportJob \
    --export-dir data_for_export</codeblock>
			<p>Sqoop provides many more options to the export command than we cover in this
				document. Run <codeph>sqoop export help</codeph> for a list of all options and see
				the Sqoop documentation for more details about these options.</p><p>Some options
				that may be important in your export are those that define what delimiters Sqoop
				uses when reading the records from the Hadoop text file to export to Couchbase. The
				default is the comma (<codeph>,</codeph>) character. Through the
					<cmdname>sqoop</cmdname> command you may specify a different
				delimiter.</p><p>When the export job executes, it also generates a
					<codeph>.java</codeph> source code file that shows how the data was read. If,
				for instance, the job run had the argument <codeph>--table
					couchbaseExportJob</codeph>, Sqoop generates a
					<filepath>couchbaseExportJob.java</filepath> source code file.</p>
			<b>List table</b>
			<p>Sqoop has a tool called <codeph>list-tables</codeph>. Couchbase does not have a
				notion of tables, but we use <codeph>DUMP</codeph> and <codeph>BACKFILL_##</codeph>
				as values to the <codeph>--table</codeph> option.</p><p>Since there is no real
				purpose to the <cmdname>list-tables</cmdname> command in the case of the Couchbase
				Hadoop Connector, it is not recommended you use this argument to Sqoop.</p>
			<b>Import all tables</b>
			<p>Sqoop has a tool called <codeph>import-all-tables</codeph>. Couchbase does not have a
				notion of tables. </p><p>Since there is no real purpose to the
					<codeph>import-all-tables</codeph> command in the case of the Couchbase Hadoop
				Connector, it is not recommended you use this argument to Sqoop. </p></section>
	<section id="limitations">
		<title>Limitations</title>
		
			<p>While Couchbase provides many great features to import and export data from Couchbase to
				Hadoop there is some functionality that the connector doesn’t implement in Sqoop. These
				are the known limitations:</p>
			<ul>
				<li>
					<p>Querying: You cannot run queries on Couchbase. All tools that attempt to do this
						will fail with a <codeph>NotSupportedException</codeph>. Querying will be added to
						future Couchbase products designed to integrate with Hadoop.</p>
				</li>
				<li>
					<p><codeph>list-databases</codeph> tool: Even though Couchbase is a multitenant
						system that allows for multiple buckets (which are analogous to databases) here is
						no way of listing these buckets from Sqoop. The list of buckets is available
						through the Couchbase Cluster web console.</p>
				</li>
				<li>
					<p><codeph>eval-sql</codeph> tool: Couchbase does not use SQL, so this tool is not
						appropriate.</p>
				</li>

				<li>The Couchbase Hadoop Connector does not automatically handle some classes of
					failures in a Couchbase cluster or changes to cluster topology while the Sqoop task
					is being run.</li>
			</ul>
		
	</section>
	</conbody>
</concept>
