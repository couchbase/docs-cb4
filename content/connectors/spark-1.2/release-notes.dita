<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_cn2_5ck_r5">
    <title>Release Notes</title>
    <shortdesc>Release notes for the 1.2 version of the Spark connector.</shortdesc>
<conbody>

  <section><title>Couchbase Spark Connector 1.2.1 GA (June 2016)</title> Version 1.2.1 is
       the second stable release of the 1.2.x series. It fortifies support for Spark 1.6
       and brings the following enhancements and bugfixes:

       <p><b>Spark Core</b></p>

       <ul>
           <li>The underlying SDK has been updated to 2.3.0</li>
           <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-52" format="html"
                   scope="external">SPARKC-52</xref>:<codeph>SaveMode</codeph> on writes now doesn't bubble up errors
            if <codeph>IGNORE</codeph> is used.</li>
           <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-51" format="html"
                   scope="external">SPARKC-51</xref>:Added JVM hook to eventually clean up connections properly.</li>
           <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-56" format="html"
                   scope="external">SPARKC-56</xref>: Support for transparent retry on writes like on reads.</li>
       </ul>

       <p><b>Spark SQL</b></p>

       <ul>
           <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-50" format="html"
                   scope="external">SPARKC-50</xref>: The <codeph>(META_)ID</codeph> can now be any type which can be
             converted into a string instead of being an actual string. This
             allows for more flexibility in working with types in the first
             place.</li>
           <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-48" format="html"
                   scope="external">SPARKC-48</xref>: Filters that are <codeph>LIKE</codeph> based now escape
            <codeph>.</codeph> and <codeph>*</codeph> chars.</li>
           <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-53" format="html"
                   scope="external">SPARKC-53</xref>: Deeply nested attributes on filters are properly parsed and
             escaped.</li>
       </ul>

       <p><b>Spark Streaming</b></p>

       <ul>
           <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-55" format="html"
                   scope="external">SPARKC-55</xref>: The <codeph>FromNow</codeph> option now works, allowing you to
            start streaming from the current point in time without getting all
            the previous mutations first.</li>
       </ul>

   </section>


       <section><title>Couchbase Spark Connector 1.2.0 GA (May 2016)</title> Version 1.2.0 is
            the first stable release of the 1.2.x series. It brings support for Spark 1.6
            and the following enhancements and bugfixes:

            <p><b>Spark Core</b></p>

            <ul>
                <li>Support for Apache Spark 1.6.x</li>
                <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-37" format="html"
                        scope="external">SPARKC-37</xref>: Both Java and Scala APIs now export
                    Couchbase view, spatial view, and N1QL query APIs on RDDs in addition to the
                        <codeph>SparkContext</codeph>.</li>
                <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-46" format="html"
                        scope="external">SPARKC-46</xref>: Subdocument Lookups are supported on the
                    RDDs and the SparkContext (only supported with Couchbase Server 4.5).</li>
            </ul>

            <p><b>Spark SQL</b></p>

            <ul>
                <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-41" format="html"
                        scope="external">SPARKC-41</xref>: Manual override of the <codeph>schemaFilter</codeph> has been fixed. It is now possible to define a filter like this:
<codeblock outputclass="language-scala"><![CDATA[sqlContext.read
  .option("bucket","travel-sample")
  .option("schemaFilter", "type = 'airline'")
  .couchbase()]]></codeblock>
                      </li>
                <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-42" format="html"
                        scope="external">SPARKC-42</xref>: The SparkSQL <codeph>count()</codeph> operator now works, a bug has been fixed so that
                      if SparkSQL doesn't pass in required columns they are transformed to a <codeph>*</codeph> query.</li>
                      <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-30" format="html"
                              scope="external">SPARKC-30</xref>: It is now possible to provide a manual schema as well as a custom schema filter.</li>
                              <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-43" format="html"
                                      scope="external">SPARKC-43</xref>: The Java API can now use Spark SQL directly. See the Java API documentation for more information.</li>
                                      <li><xref href="https://www.couchbase.com/issues/browse/SPARKC-47" format="html"
                                              scope="external">SPARKC-47</xref>: SparkSQL Filter expression support has been extended to support all Spark Filters, including nested ones.</li>
            </ul>

            <p><b>Spark Streaming</b></p>

            <ul>
                <li>The internal implementation has been updated to the latest release but <b>is
                        still experimental</b>. Note that the <codeph>FromBeginning</codeph> is
                    implemented, but <codeph>FromNow</codeph> still has some known issues which will
                    be fixed in later releases. Also, cluster rebalance support is not yet available
                    and will follow.</li>
            </ul>
        </section>

        </conbody>
    </concept>
