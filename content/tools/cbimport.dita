<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_fsg_5d3_vx">
  <title>cbimport</title>
  <shortdesc><codeph>cbimport</codeph> is a utility to import data into a Couchbase cluster. </shortdesc>
  <body>
    <p><b>Synopsis</b></p>
    <p><codeblock>cbimport [--version] [--help] &lt;command&gt; [&lt;args&gt;]</codeblock></p>
    <p><b>Description</b></p>
    <p><cmdname>cbimport</cmdname> is used to import data from various sources into Couchbase. For
      more information on how specific commands work, run <codeph>cbimport &lt;command&gt;
        --help</codeph>.</p>
    <p><b>Options</b></p>
    <table frame="all" rowsep="1" colsep="1" id="table_t4f_cq3_vx">
      <title>cbimport options</title>
      <tgroup cols="2">
        <colspec colname="c1" colnum="1" colwidth="1.0*"/>
        <colspec colname="c2" colnum="2" colwidth="2.0*"/>
        <thead>
          <row>
            <entry>Option</entry>
            <entry>Description</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <p>--version</p>
            </entry>
            <entry>Prints the cbimport suite version that the cbimport program came from.</entry>
          </row>
          <row>
            <entry>--help</entry>
            <entry>Prints the synopsis and a list of commands. If a cbimport command is named, this
              option will bring up the manual page for that command.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    <p><b>Commands</b></p>
    <p><cmdname>cbimport csv</cmdname>: Imports data into Couchbase from a CSV file. </p>
    <p><cmdname>cbimport json</cmdname>: Imports data into Couchbase from a JSON file. </p>
    <p><b>Discussion</b></p>
    <p>The <cmdname>cbimport</cmdname> command is used to import data from various sources into a
      Couchbase cluster. Each supported format is a sub-command of the <cmdname>cbimport</cmdname>
      utility.</p>
    
    <section><title>cbimport csv</title><p><codeph>cbimport csv</codeph> imports CSV data into a
        Couchbase cluster.</p><p><b>Synopsis</b></p><p>
        <codeblock>cbimport csv [--cluster &lt;url>] [--bucket &lt;bucket_name>] [--dataset &lt;path>]
          [--username &lt;username>] [--password &lt;password>][--generate-key &lt;key_expr>]
          [--limit-rows &lt;num>] [--skip-rows &lt;num>][--field-separator &lt;char>]
          [--cacert &lt;path>] [--no-ssl-verify] [--threads &lt;num>]
          [--error-log &lt;path>][--log-file &lt;path>]</codeblock>
      </p><p><b>Description</b></p><p>Imports CSV and other forms of separated value type data into
        Couchbase. By default, data files should start with a line containing comma-separated column
        names, followed by one or more lines of comma-separated values. However, if you are
        importing data that using a different field separator, for example tabs, you can use the
          <codeph>--field-separator</codeph> flag to specify that tabs are used instead of
        commas.</p><p>The <cmdname>cbimport</cmdname> command also supports custom key-generation
        for each document in the imported file. Key generation is done with a combination of
        pre-existing fields in a document and custom generator functions supplied by
          <cmdname>cbimport</cmdname>. For details about key generators, see <xref
          href="#topic_fsg_5d3_vx/key-generators-csv" format="dita">Key
          Generators</xref>.</p><p><b>Options</b></p><p>The following tables list the required and
        optional parameters for the <cmdname>cbimport csv</cmdname> command.</p><p>
        <table frame="all" rowsep="1" colsep="1" id="table_thk_3q3_vx">
          <title>Required options for cbimport csv</title>
          <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="2.0*"/>
            <thead>
              <row>
                <entry>Option</entry>
                <entry>Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry> -c,--cluster &lt;url></entry>
                <entry>The host name of a node in the cluster to import data into. See <xref
                    href="#topic_fsg_5d3_vx/host-formats-csv" format="dita">Host Formats</xref> for
                  details about host name specification formats.</entry>
              </row>
              <row>
                <entry>-u,--username &lt;username></entry>
                <entry>The user name for cluster authentication. The user must have the appropriate
                  privileges to write to the bucket in which data will be loaded to.</entry>
              </row>
              <row>
                <entry>-p,--password &lt;password></entry>
                <entry>The password for cluster authentication. The user must have the appropriate
                  privileges to write to the bucket in which data will be loaded to. Specifying this
                  option without a value will allow the user to type a non-echoed password to
                  stdin.</entry>
              </row>
              <row>
                <entry>-b,--bucket &lt;bucket_name></entry>
                <entry>The name of the bucket to import data into.</entry>
              </row>
              <row>
                <entry>-d,--dataset &lt;uri></entry>
                <entry>The URI of the dataset to be loaded. <cmdname>cbimport</cmdname> supports
                  loading data from a local file or from an internet location. When importing data from a local
                  file the path must be prefixed with <filepath>file://</filepath>. If an internet location is used
                  then the path should be prefixed with either <filepath>http://</filepath> or
                    <filepath>https://</filepath>.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p><table frame="all" rowsep="1" colsep="1" id="table_s21_mq3_vx">
        <title>Optional options for cbimport csv</title>
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="2.0*"/>
          <thead>
            <row>
              <entry>Option</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>-g,--generate-key &lt;key_expr></entry>
              <entry>Specifies a key expression used for generating a key for each document
                imported. See <xref href="#topic_fsg_5d3_vx/key-generators-csv" format="dita">Key
                  Generators</xref> for more information on specifying key generators.</entry>
            </row>
            <row>
              <entry>--field-separator &lt;num></entry>
              <entry>Specifies the field separator to use when reading the dataset. By default the
                separator is a comma. To read tab separated files you can specify a tab in this
                field. Note that in bash shell a tab is specified as <codeph>$'\t'</codeph>.</entry>
            </row>
            <row>
              <entry>--limit-rows &lt;num></entry>
              <entry>Specifies that the utility should stop loading data after reading a certain
                amount of rows from the dataset. This option is useful when you have a large dataset
                and only want to partially load it.</entry>
            </row>
            <row>
              <entry>--skip-rows &lt;num></entry>
              <entry>Specifies that the utility should skip some rows before starting to import
                data. If this flag is used together with the <codeph>--limit-rows</codeph> flag then
                cbimport imports the number of rows specified by <codeph>--limit-rows</codeph> after
                skipping the rows specified by <codeph>--skip-rows</codeph>.</entry>
            </row>
            <row>
              <entry>--no-ssl-verify</entry>
              <entry>Skips the SSL verification phase. Specifying this flag will allow a connection
                using SSL encryption, but will not verify the identity of the server you connect to.
                You are vulnerable to a man-in-the-middle attack if you use this flag. Either this
                flag or the <codeph>--cacert</codeph> flag must be specified when using an SSL
                encrypted connection.</entry>
            </row>
            <row>
              <entry>--infer-types</entry>
              <entry>By default all values in a CSV files are interpreted as strings. If infer types
                is set then cbimport will look at each value and decide whether it is a string,
                integer, or boolean value and put the inferred type into the document.</entry>
            </row>
            <row>
              <entry>--omit-empty</entry>
              <entry>Some values in a CSV row will not contain any data. By default these values are
                put into the generated JSON document as an empty string. Use this flag to omit
                fields that contain empty values.</entry>
            </row>
            <row>
              <entry>--cacert</entry>
              <entry>Specifies a CA certificate that will be used to verify the identity of the
                server being connecting to. Either this flag or the <codeph>--no-
                  ssl-verify</codeph> flag must be specified when using an SSL encrypted
                connection.</entry>
            </row>
            <row>
              <entry>-t,--threads &lt;num></entry>
              <entry>Specifies the number of concurrent clients to use when importing data. Fewer
                clients means imports will take longer, but there will be less cluster resources
                used to complete the import. More clients means faster imports, but at the cost of
                more cluster resource usage. This parameter defaults to 1 if it is not specified and
                it is recommended that this parameter is not set to be higher than the number of
                CPUs on the machine where the import is taking place.</entry>
            </row>
            <row>
              <entry>-e,--errors-log &lt;path></entry>
              <entry>Specifies a log file where JSON documents that could not be loaded are written
                to. A document might not be loaded if a key could not be generated for the document
                or if the document is not valid JSON. The errors file is written in the "json lines"
                format (one document per line).</entry>
            </row>
            <row>
              <entry>-l,--log-file</entry>
              <entry>Specifies a log file for writing debugging information about
                  <cmdname>cbimport</cmdname> execution.</entry>
            </row>
          </tbody>
        </tgroup>
      </table><p id="host-formats-csv"><b>Host Formats</b></p><p>When specifying a host for the
        cbimport command the following formats are expected: <ul id="ul_m5j_ns3_vx">
          <li><codeph>couchbase://&lt;addr></codeph></li>
          <li><codeph>&lt;addr>:&lt;port></codeph></li>
          <li><codeph>http://&lt;addr>:&lt;port></codeph></li>
        </ul></p><p>We recommend using the <codeph>couchbase://&lt;addr></codeph> format for
        standard installations. The other two formats allow an option to take a port number which is
        needed for non-default installations where the admin port has been set up on a port other
        that 8091.</p>
      <p id="key-generators-csv"><b>Key Generators</b></p><p>Key generators are used in order to
        generate a unique key for each document loaded. Keys can be generated by using a combination
        of column values (indicated by wrapping the column name with "%"), custom generators
        (currently <codeph>#MONO_INCR#</codeph> which returns a monotonically increasing integer,
        and <codeph>#UUID#</codeph> which returns a UUID), and arbitrary characters for formatting.
        </p><p>Here is an example of a key generation expression. Given the CSV dataset:
      </p><codeblock>fname,age 
alice,40 
barry,36</codeblock>And the key generator expression:
      <codeblock>--generate-key key::%fname%::#MONO_INCR#</codeblock>The following keys are
      generated: <codeblock>key::alice::1 
key::barry::2</codeblock><p>In the example above we
        generate a key using the value in each row of the "fname" field and a custom generator. To
        specify that we want to substitute the value in the "fname" field we put the name of the
        field "fname" between two percent signs. This is an example of field substitution and it
        allows the ability to build keys out of data that is already in the dataset.</p><p>This
        example also contains a generator function <cmdname>MONO_INCR</cmdname> which will increment
        by 1 each time the key generator is called. Since this is the first time this key generator
        was executed it returns 1. If we executed the key generator again it would return 2 and so
        on. </p><p>Any text that isn't wrapped in "%" or "#" is static text and will be in the
        result of all generated keys. If a key needs to contain a "%" or "#" in static text then
        they need to be escaped by providing a double "%" or "#" (ex. "%%" or "##").</p><p>If a key
        cannot be generated because the column specified in the key generator is not present in the
        row then that row will be skipped. To see a list of rows that were not imported due to
        failed key generation, users can specify the <codeph>--errors-log</codeph> &lt;path>
        parameter to dump a list of all rows that could not be
        imported.</p><p><b>Examples</b></p><p>The examples in this section illustrate importing data
        from the following files:</p><p>
        <codeblock spectitle="/data/people.csv">fname,age
alice,40
barry,36</codeblock>
        <codeblock spectitle="/data/people.tsv">fname  age
alice  40
barry  36</codeblock>
      </p><p>To import data from /data/people.csv using a key containing the "fname" column and
        utilizing 4 threads the following command can be run.
        <codeblock>$ cbimport csv -c couchbase://127.0.0.1 -u Administrator -p password \
            -b default -d file:///data/people.csv -g key::%fname% -t 4</codeblock></p><p>
        To import data from /data/people.tsv using a key containing the "fname" column and the UUID
        generator the following command would be run.
        <codeblock>$ cbimport csv -c couchbase://127.0.0.1 -u Administrator -p password \
            -b default -d file:///data/people.tsv --field-separator $'\t' \
            -g key::%fname%::#UUID# -t 4</codeblock></p><p>If
        the dataset in not available on the local machine where the command is run, but is available
        via an HTTP URL we can still import the data using cbimport. If we assume that the data is
        located at http://data.org/people.csv then we can import the data with the following
        command.
        <codeblock>$ cbimport csv -c couchbase://127.0.0.1 -u Administrator -p password \
            -b default -d http://data.org/people.csv -g key::%fname%::#UUID# -t 4</codeblock></p><p><b>Discussion</b></p><p>The
          <cmdname>cbimport csv</cmdname> command is used to quickly import data from various files
        containing CSV, TSV, or other separated format data. While importing CSV the
          <cmdname>cbimport</cmdname> command only utilizes a single reader. As a result importing
        large dataset may benefit from being partitioned into multiple files and running a separate
        cbimport process on each file.</p></section>
    <section><title>cbimport json</title><p><cmdname>cbimport json</cmdname> imports JSON data into
        a Couchbase cluster.</p><p><b>Synopsis</b></p><p>
        <codeblock>cbimport json [--cluster &lt;url>] [--bucket &lt;bucket_name>] [--dataset &lt;path>]
              [--format &lt;data_format>][--username &lt;username>][--password &lt;password>]
              [--generate-key &lt;key_expr>][--cacert &lt;path>][--no-ssl-verify]
              [--threads &lt;num>] [--error-log &lt;path>][--log-file &lt;path>]</codeblock>
      </p><p><b>Description</b></p><p>Imports JSON data into Couchbase. The
          <cmdname>cbimport</cmdname> command supports files that have a JSON document on each line,
        files that contain a JSON list (that is array) where each element is a document, and the
        Couchbase Samples format. The file format can be specified with the --format flag. See <xref
          href="#topic_fsg_5d3_vx/dataset-formats" format="dita">Dataset Formats</xref> for more
        details on the supported file formats. </p><p>The <cmdname>cbimport</cmdname> command also
        supports custom key-generation for each document in the imported file. Key generation is
        done with a combination of pre-existing fields in a document and custom generator functions
        supplied by <cmdname>cbimport</cmdname>. For details about key generators, see <xref
          href="#topic_fsg_5d3_vx/key-generators-json" format="dita">Key Generators</xref>.
          </p><p><b>Options</b></p><p>The following tables list the required and optional parameters
        for the <cmdname>cbimport json</cmdname> command.</p><p>
        <table frame="all" rowsep="1" colsep="1" id="table_thk_3q3_vy">
          <title>Required options for cbimport json</title>
          <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="2.0*"/>
            <thead>
              <row>
                <entry>Option</entry>
                <entry>Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry> -c,--cluster &lt;url></entry>
                <entry>The host name of a node in the cluster to import data into. See <xref
                    href="#topic_fsg_5d3_vx/host-formats-json" format="dita">Host Formats</xref> for
                  details about host name specification formats.</entry>
              </row>
              <row>
                <entry>-u,--username &lt;username></entry>
                <entry>The user name for cluster authentication. The user must have the appropriate
                  privileges to write to the bucket in which data will be loaded to.</entry>
              </row>
              <row>
                <entry>-p,--password &lt;password></entry>
                <entry>The password for cluster authentication. The user must have the appropriate
                  privileges to write to the bucket in which data will be loaded to. Specifying this
                  option without a value will allow the user to type a non-echoed password to
                  stdin.</entry>
              </row>
              <row>
                <entry>-b,--bucket &lt;bucket_name></entry>
                <entry>The name of the bucket to import data into.</entry>
              </row>
              <row>
                <entry>-d,--dataset &lt;uri></entry>
                <entry>The URI of the dataset to be loaded. <cmdname>cbimport</cmdname> supports
                  loading data from a local file or from a URL. When importing data from a local
                  file the path must be prefixed with <filepath>file://</filepath>. If a URL is used
                  then the file should be prefixed with either <filepath>http://</filepath> or
                    <filepath>https://</filepath>.</entry>
              </row>
              <row>
                <entry>-f,--format &lt;format></entry>
                <entry>The format of the dataset specified (lines, list, sample). See <xref
                    href="#topic_fsg_5d3_vx/dataset-formats" format="dita">Dataset Formats</xref>
                  for more details on the formats supported by cbimport.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p><table frame="all" rowsep="1" colsep="1" id="table_s21_mq3_vy">
        <title>Optional options for cbimport json</title>
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="2.0*"/>
          <thead>
            <row>
              <entry>Option</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>-g,--generate-key &lt;key_expr></entry>
              <entry>Specifies a key expression used for generating a key for each document
                imported. This parameter is required for list and lines formats, but not for the
                sample format. See <xref href="#topic_fsg_5d3_vx/key-generators-json" format="dita"
                  >Key Generators</xref> for more information on specifying key generators.</entry>
            </row>
            <row>
              <entry>--no-ssl-verify</entry>
              <entry>Skips the SSL verification phase. Specifying this flag will allow a connection
                using SSL encryption, but will not verify the identity of the server you connect to.
                You are vulnerable to a man-in-the-middle attack if you use this flag. Either this
                flag or the <codeph>--cacert</codeph> flag must be specified when using an SSL
                encrypted connection.</entry>
            </row>
            <row>
              <entry>--cacert</entry>
              <entry>Specifies a CA certificate that will be used to verify the identity of the
                server being connecting to. Either this flag or the <codeph>--no-
                  ssl-verify</codeph> flag must be specified when using an SSL encrypted
                connection.</entry>
            </row>
            <row>
              <entry>-t,--threads &lt;num></entry>
              <entry>Specifies the number of concurrent clients to use when importing data. Fewer
                clients means imports will take longer, but there will be less cluster resources
                used to complete the import. More clients means faster imports, but at the cost of
                more cluster resource usage. This parameter defaults to 1 if it is not specified and
                it is recommended that this parameter is not set to be higher than the number of
                CPUs on the machine where the import is taking place.</entry>
            </row>
            <row>
              <entry>-e,--errors-log &lt;path></entry>
              <entry>Specifies a log file where JSON documents that could not be loaded are written
                to. A document might not be loaded if a key could not be generated for the document
                or if the document is not valid JSON. The errors file is written in the "lines"
                format (one document per line).</entry>
            </row>
            <row>
              <entry>-l,--log-file</entry>
              <entry>Specifies a log file for writing debugging information about
                  <cmdname>cbimport</cmdname> execution.</entry>
            </row>
          </tbody>
        </tgroup>
      </table><p id="host-formats-json"><b>Host Formats</b></p><p>When specifying a host for the
        cbimport command the following formats are expected: <ul id="ul_m5j_ns3_vy">
          <li><codeph>couchbase://&lt;addr></codeph></li>
          <li><codeph>&lt;addr>:&lt;port></codeph></li>
          <li><codeph>http://&lt;addr>:&lt;port></codeph></li>
        </ul></p><p>We recommend using the <codeph>couchbase://&lt;addr></codeph> format for
        standard installations. The other two formats allow an option to take a port number which is
        needed for non-default installations where the admin port has been set up on a port other
        that 8091.</p><p id="dataset-formats"><b>Dataset Formats</b></p><p>The cbimport command
        supports the following formats:<ul id="ul_frg_4sj_vx">
          <li><b>Lines</b>
            <p>The lines format specifies a file that contains one JSON document on every line in
              the file. This format is specified by setting the <codeph>--format</codeph> option to
              "lines". Here's an example of a file in lines format:
              <codeblock>{"key": "mykey1", "value": "myvalue1"}
{"key": "mykey2", "value": "myvalue2"}
{"key": "mykey3", "value": "myvalue3"}
{"key": "mykey4", "value": "myvalue4"}</codeblock></p></li>
          <li><b>List</b><p>The list format specifies a file which contains a JSON list where each
              element in the list is a JSON document. The file may only contain a single list, but
              the list may be specified over multiple lines. This format is specified by setting the
                <codeph>--format</codeph> option to "list". Here's an example of a file in list
              format:
              <codeblock>[
  {
    "key": "mykey1",
    "value": "myvalue1"
  },
  {"key": "mykey2", "value": "myvalue2"},
  {"key": "mykey3", "value": "myvalue3"},
  {"key": "mykey4", "value": "myvalue4"}
]</codeblock></p></li>
          <li><b>Sample</b><p> The sample format specifies a ZIP file or folder containing multiple
              documents. This format is intended to load Couchbase sample data sets. Unlike the
              lines and list formats, the sample format may also contain index, view, and full-text
              index definitions. Here's the folder structure for the sample format:
              <codeblock>+ (root folder)
  + docs
    key1.json
    key2.json
    ...
  + design_docs
    indexes.json
    views.json</codeblock></p><p>All
            documents in the samples format are contained in the <codeph>docs</codeph> folder and there is one file
            per document. Each file name in the <codeph>docs</codeph> folder is the key name for the JSON document
              contained in the file. If the file name contains a <filepath>.json</filepath>
              extension then the extension is excluded from the key name during the import. This
              name can be overridden if the <codeph>--generate-key</codeph> option is specified. The
            <codeph>docs</codeph> folder may also contain sub-folders of documents to be imported. Sub-folders can
              be used to organize large amounts of documents into a more readable categorized
              form.</p><p>The <filepath>design_docs</filepath> folder contains index definitions.
              The file name <filepath>indexes.json</filepath> is reserved for secondary indexes. All
              other file names are used for view indexes.</p></li>
        </ul></p><p id="key-generators-json"><b>Key Generators</b></p><p>Key generators are used in
        order to generate a key for each document loaded. Keys can be generated by using a
        combination of characters, the values of a given field in a document, and custom generators.
        Field substitutions are done by wrapping the field name in "%" and custom generators are
        wrapped in "#". </p><p>Here is an example of a key generation expression. Given the
        document: </p><codeblock>{
  "name": "alice",
  "age": 40
}</codeblock>And the key generator
      expression: <codeblock>--generate-key key::%name%::#MONO_INCR#</codeblock>The following keys
      are generated: <codeblock>key::alice::1 </codeblock><p>In the example above we generate a key
        using both the value of a field in the document and a custom generator. We use the "name"
        field to use the value of the name field as part of the key. This is specified by "%name%"
        which tells the key generator to substitute the value of the field "name" into the
        key.</p><p>This example also contains a generator function <cmdname>MONO_INCR</cmdname>
        which will increment by 1 each time the key generator is called. Since this is the first
        time this key generator was executed it returns 1. If we executed the key generator again it
        would return 2 and so on. The <cmdname>cbimport</cmdname> command currently contains a
        monotonic increment generator (<cmdname>MONO_INCR</cmdname>) and a UUID generator
          (<cmdname>UUID</cmdname>).</p><p>Any text that isn't wrapped in "%" or "#" is static text
        and will be in the result of all generated keys. If a key needs to contain a "%" or "#" in
        static text then they need to be escaped by enclosing it in double quotes "%" or "#" (ex.
        "%%" or "##").</p><p>If a key cannot be generated because the field specified in the key
        generator is not present in the document then the key will be skipped. To see a list of
        document that were not imported due to failed key generation users can specify the
          <codeph>--errors-log</codeph> &lt;path> parameter to dump a list of all documents that
        could not be imported to a file.</p><p><b>Examples</b></p><p>The examples in this section
        illustrate importing data from the following
        files:<codeblock spectitle="/data/lines.json">{"name": "alice", "age": 37}
{"name": "bob", "age": 39}</codeblock><codeblock spectitle="/data/list.json">[
  {"name": "candice", "age": 42},
  {"name": "daniel", "age": 38}
]</codeblock></p>
      To import data from /data/lines.json using a key containing the "name" field and utilizing 4
      threads the following command can be
        run.<codeblock>$ cbimport json -c couchbase://127.0.0.1 -u Administrator -p password \
  -b default -d file:///data/lines.json -f lines -g key::%name% -t 4</codeblock><p>To
        import data from /data/list.json using a key containing the "name" field and the UUID
        generator the following command would be run.
        <codeblock>$ cbimport json -c couchbase://127.0.0.1 -u Administrator -p password \
   -b default -d file:///data/list.json -f list -g key::%name%::#UUID# -t 4</codeblock></p><p>If
        the dataset is not available on the local machine where the command is run, but is available
        via an HTTP URL we can still import the data using <cmdname>cbimport</cmdname>. If we assume
        that the data is located at <filepath>http://data.org/list.json</filepath> and that the
        dataset is in the JSON list format, then we can import the data with the command below.
        <codeblock>$ cbimport json -c couchbase://127.0.0.1 -u Administrator -p password \
   -b default -d http://data.org/list.json -f list -g key::%name%::#UUID# -t 4</codeblock></p><p><b>Discussion</b></p>
      The <cmdname>cbimport json</cmdname> command is used to quickly import data from various files
      containing JSON data. While importing a JSON document, the <cmdname>cbimport</cmdname> command
      only utilizes a single reader. As a result importing large dataset may benefit from being
      partitioned into multiple files and running a separate <cmdname>cbimport</cmdname> process on
      each file.<p/>
    </section>
  </body>
  <related-links>
    <link href="cbexport.dita"></link>
  </related-links>
</topic>
