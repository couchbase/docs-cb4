<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="sdk-operations-java">
    <title>CRUD Document Operations Using the Java SDK with Couchbase Server</title>
    <titlealts><navtitle>Document Operations</navtitle></titlealts>
    <shortdesc>You can access documents in Couchbase using methods of the
        <apiname>couchbase.couchbase.client.java.Bucket</apiname> object.</shortdesc>
    <conbody>

    <p>The methods for
            retrieving documents are <apiname>get()</apiname> and <apiname>lookupIn()</apiname> and
            the methods for mutating documents are <apiname>upsert()</apiname>,
                <apiname>insert()</apiname>, <apiname>replace()</apiname> and
                <apiname>mutateIn()</apiname>. </p>
    <p>Examples are shown using the synchronous API. See the section on
        <xref href="async-programming.dita">Async Programming</xref>
        for other APIs.
    </p>

    <section id="java-additional-options">
        <title>Additional Options</title>
        <p>Update operations also accept a <xref
                    href="../core-operations.dita#devguide_kvcore_generic/expiry">TTL
                    (expiry)</xref> value (<parmname>expiry</parmname>) on the passed document which
                will instruct the server to delete the document after a given amount of time. This
                option is useful for transient data (such as sessions). By default documents do not
                expire. See <xref
                    href="../core-operations.dita#devguide_kvcore_generic/expiry"/>
                for more information on expiration. </p>
        <p>Update operations can also accept a <xref
                    href="../concurrent-mutations-cluster.dita">CAS</xref>
                    (<parmname>cas</parmname>) value on the passed document to protect against
                concurrent updates to the same document. See <xref
                    href="../concurrent-mutations-cluster.dita">CAS</xref> for a description
                on how to use CAS values in your application. Since CAS values are opaque, they are
                normally retreived when a Document is loaded from Couchbase and then used
                subsequently (without modification) on the mutation operations. If a mutation did
                succeed, the returned Document will contain the new CAS value. </p>
    </section>

    <section id="java-mutation-input">
        <title>Document Input and Output Types</title>
        <p>Couchbase stores documents. From an SDK point of view, those documents contain the actual
                value (like a JSON object) and associated metadata. Every document in the Java SDK
                contains the following properties, some of them optional depending on the context: </p>
        <table>
          <tgroup cols="2">
            <thead>
              <row>
                <entry>Name</entry>
                <entry>Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <codeph>id</codeph>
                </entry>
                <entry>The (per bucket) unique identifier of the document.</entry>
              </row>
              <row>
                <entry>
                  <codeph>content</codeph>
                </entry>
                <entry>The actual content of the document.</entry>
              </row>
              <row>
                <entry>
                  <codeph>cas</codeph>
                </entry>
                <entry>The CAS (Compare And Swap) value of the document.</entry>
              </row>
              <row>
                <entry>
                  <codeph>expiry</codeph>
                </entry>
                <entry>The expiration time of the document.</entry>
              </row>
              <row>
                <entry>
                  <codeph>mutationToken</codeph>
                </entry>
                <entry>The optional MutationToken after a mutation.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <p>There are a few different implementations of a
            <codeph>Document</codeph>. Here are a few noteworthy
            document types:
            <ul>
                <li><apiname>JsonDocument</apiname>: The default one in
                most methods, contains a JSON object (as a
                <codeph>JsonObject</codeph>).</li>
                <li><apiname>RawJsonDocument</apiname>: Represents any
                JSON value, stored as a <codeph>String</codeph> (useful
                for when you have your own JSON
                serializer/deserializer).</li>
                <li><apiname>BinaryDocument</apiname>: Used to store
                pure raw binary data (as a <codeph>ByteBuf</codeph>
                from Netty).
                <note type="important">The <codeph>ByteBuf</codeph>
                comes from Netty, and when reading one from the SDK,
                you need to manage its memory by hand by calling
                <codeph>release()</codeph>. See <xref
                href="#sdk-operations-java/java-binary-document">the
                section about binary documents</xref>.
                </note></li>
            </ul>
        </p>
        <p>Because Couchbase Server can store anything and not just JSON files, many document types exist
                to satisfy the general needs of an application. You can also write your own
                    <codeph>Document</codeph> implementations, which is not covered in this
                introduction. </p>
    </section>

    <section id="java-creating-updating-full-docs">
        <title>Creating and Updating Full Documents</title>
        <p>Documents may be created and updated using the <apiname>Bucket#upsert()</apiname>,
                    <apiname>Bucket#insert()</apiname>, and <apiname>Bucket#replace()</apiname>
                family of methods. Read more about the difference between these methods at <xref
                    href="../core-operations.dita#devguide_kvcore_generic/crud-overview"
                /> in the Couchbase developer guide. </p>

        <p>These methods accept a Document instance where the following
            values are considered if set:
            <ul>
                <li><parmname>id</parmname> (mandatory): The ID of the
                document to modify (String).</li>
                <li><parmname>content</parmname> (mandatory): The
                desired new content of the document, this varies per
                document type used.
                If the <codeph>JsonDocument</codeph> is used, the
                document type is a <codeph>JsonObject</codeph>.</li>
                <li><parmname>expiry</parmname> (optional): Specify the
                expiry time for the document. If specified, the document
                will expire and no longer exist after the given number
                of seconds.
                See <xref href="../core-operations.dita#expiry"/>
                for more information.</li>
                <li><parmname>cas</parmname> (optional): The CAS value
                for the document. If the CAS on the server does not
                match the CAS supplied to the method, the operation will
                fail with a <apiname>CASMismatchException</apiname>. See
                <xref href="../concurrent-mutations-cluster.dita"/>
                for more information on the usage of CAS values.</li>
            </ul>
        </p>

        <p>Other optional arguments are also available for more
            advanced usage:
            <ul>
                <li><parmname>persistTo</parmname>,
                <parmname>replicateTo</parmname>: Specify
                <xref href="../durability.dita#concept_gyg_14s_zs">
                durability requirements</xref> for the
                operations.</li>
                <li><parmname>timeout</parmname>,
                <parmname>timeUnit</parmname>: Specify a custom
                timeout which overrides the default timeout
                setting.</li>
            </ul>
        </p>
        <p>Upon success, the returned <apiname>Document</apiname> instance will contain the new
                    <xref href="../concurrent-mutations-cluster.dita#concept_iq4_bts_zs"
                    >CAS</xref> value of the document. If the document is not mutated successfully,
                an exception is raised depending on the type of error. </p>
        <p>Inserting a document works like this:</p>

<codeblock outputclass="language-java"><![CDATA[JsonDocument doc = JsonDocument.create("document_id", JsonObject.create().put("some", "value"));
System.out.println(bucket.insert(doc));]]></codeblock>

<screen>Output: JsonDocument{id='document_id', cas=216109389250560, expiry=0, content={"some":"value"}, mutationToken=null}</screen>

        <p>If the same code is called again, a <codeph>DocumentAlreadyExistsException</codeph> will
                be thrown. If you don't care that the document is overridden, you can use
                    <apiname>upsert</apiname> instead: </p>

<codeblock outputclass="language-java"><![CDATA[JsonDocument doc = JsonDocument.create("document_id", JsonObject.empty().put("some", "other value"));
System.out.println(bucket.upsert(doc));]]></codeblock>

<screen>Output: JsonDocument{id='document_id', cas=216109392920576, expiry=0, content={"some":"other value"}, mutationToken=null}</screen>


        <p>Finally, a full document can be replaced if it existed
            before. If it didn't exist, then a
            <codeph>DocumentDoesNotExistException</codeph> will be
            thrown:
        </p>

<codeblock outputclass="language-java"><![CDATA[JsonDocument doc = JsonDocument.create("document_id", JsonObject.empty().put("more", "content"));
System.out.println(bucket.replace(doc));]]></codeblock>

<screen>Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}</screen>

    </section>

    <section id="java-retrieving-full-docs">
        <title>Retrieving full documents</title>

        <p>You can retrieve documents using the <apiname>Bucket#get()</apiname>,
                    <apiname>Bucket#getAndLock()</apiname>, <apiname>Bucket#getAndTouch()</apiname>
                and <apiname>Bucket#getFromReplica()</apiname>methods. All of those serve different
                distinct purposes and accept different parameters. </p>
        <p>Most of the time you use the <apiname>get()</apiname> method. It accepts one mandatory
                argument: </p>

        <ul>
            <li><parmname>id</parmname>: The document ID to retrieve</li>
        </ul>

<codeblock outputclass="language-java"><![CDATA[System.out.println(bucket.get("document_id"));]]></codeblock>

<screen>Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}</screen>

        <p>Other overloads are available for advanced purposes:</p>

        <ul>
            <li><parmname>document</parmname>: Instead of just passing an Id a full document can be passed in. If so, the ID is extracted and used.</li>
            <li><parmname>target</parmname>: A custom Document type (other than <codeph>JsonDocument</codeph>) can be specified.</li>
            <li><parmname>timeout</parmname>, <parmname>timeUnit</parmname>: Specify a custom
            timeout which overrides the default timeout setting.</li>
        </ul>

<codeblock outputclass="language-java"><![CDATA[// Use a Document where ID is extracted
JsonDocument someDoc = JsonDocument.create("document_id");
System.out.println(bucket.get(someDoc));]]></codeblock>

<screen>Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}</screen>

<codeblock outputclass="language-java"><![CDATA[// A custom Document type, here it returns the plain raw JSON String, encoded.
RawJsonDocument doc = bucket.get("document_id", RawJsonDocument.class);
String content = doc.content();
System.out.println(content);]]></codeblock>

<screen>Output: {"more":"content"}</screen>

<codeblock outputclass="language-java"><![CDATA[// Wait only 1 second instead of the default timeout
JsonDocument doc = bucket.get("document_id", 1, TimeUnit.SECONDS);]]></codeblock>

        <p>It is also possible to read from a replica if you want to explicitly trade availability
                for consistency during the timeframe when the active partition is not reachable (for
                example during a node failure or netsplit). </p>

        <p><codeph>getFromReplica</codeph> has one mandatory argument as well:</p>

        <ul>
            <li><parmname>id</parmname>: The document ID to retrieve</li>
        </ul>

        <p>Since you can have 0 to 3 replicas (and they can change at runtime of your application)
                the <codeph>getFromReplica</codeph> returns Lists or Iterators. It is recommended to
                use the Iterator APIs since they provide more flexibility during error conditions
                (since only partial responses may be retreived). </p>

<codeblock outputclass="language-java"><![CDATA[Iterator<JsonDocument> docIter = bucket.getFromReplica("document_id");
while(docIter.hasNext()) {
    JsonDocument replicaDoc = docIter.next();
    System.out.println(replicaDoc);
}]]></codeblock>


    <p>Other overloads are available for advanced purposes:</p>

    <ul>
        <li><parmname>replicaMode</parmname>: Allows to configure from
        which replicas to read from (defaults to all).</li>
        <li><parmname>document</parmname>: Instead of just passing an Id
        a full document can be passed in. If so, the ID is extracted and
        used.</li>
        <li><parmname>target</parmname>: A custom Document type (other
        than <codeph>JsonDocument</codeph>) can be specified.</li>
        <li><parmname>timeout</parmname>, <parmname>timeUnit</parmname>:
        Specify a custom timeout which overrides the default timeout
        setting.</li>
    </ul>

    <note type="tip">In general, always use the <parmname>ReplicaMode.ALL</parmname>
        option and not <parmname>ReplicaMode.FIRST</parmname> and
        similar to just get the first replica. The reason is that is
        that ALL will also try the active node, leading to more reliable
        behavior during failover. If you just need the first replica use
        the iterator approach and <codeph>break;</codeph> once you have
        enough data from the replicas.</note>

    <note type="important">Since a replica is updated asynchronously and eventually
        consistent, reading from it may return stale and/or outdated
        results!</note>

    <p>If you need to use pessimistic write locking on a document you can use the
                    <parmname>getAndLock</parmname> which will retreive the document if it exists
                and also return its <parmname>CAS</parmname> value. You need to provide a time that
                the document is maximum locked (and the server will unlock it then) if you don't
                update it with the valid cas. Also note that this is a pure write lock, reading is
                still allowed. </p>

<codeblock outputclass="language-java"><![CDATA[// Get and Lock for max of 10 seconds
JsonDocument ownedDoc = bucket.getAndLock("document_id", 10);

// Do something with your document
JsonDocument modifiedDoc = modifyDocument(ownedDoc);

// Write it back with the correct CAS
bucket.replace(modifiedDoc);]]></codeblock>

    <p>If the document is locked already and you are trying to lock it
        again you will receive a
        <codeph>TemporaryLockFailureException</codeph>.</p>

    <p>It is also possible to fetch the document and reset its
          expiration value at the same time. See
          <xref href="document-operations.dita#java-modifying-expiration"> Modifying Expiration</xref>
          for more information.</p>
    </section>

    <section id="java-removing-full-docs">
        <title>Removing full documents</title>

        <p>You can remove documents using the <apiname>Bucket.remove()</apiname> method. This method
                takes a single mandatory argument: </p>
        <ul>
            <li><parmname>id</parmname>: The ID of the document to remove.</li>
        </ul>

        <p>Some additional options:</p>
        <ul>
            <li><parmname>persistTo</parmname>, <parmname>replicateTo</parmname>: Specify
                    <xref href="../durability.dita#concept_gyg_14s_zs"
                    >durability requirements</xref> for the operations.</li>
            <li><parmname>timeout</parmname>, <parmname>timeUnit</parmname>: Specify a custom
            timeout which overrides the default timeout setting.</li>
        </ul>

        <p>If the <codeph>cas</codeph> value is set on the Document overload, it is used to provide
                optimistic currency, very much like the <codeph>replace</codeph> operation. </p>

<codeblock outputclass="language-java"><![CDATA[// Remove the document
JsonDocument removed = bucket.remove("document_id");]]></codeblock>

<codeblock outputclass="language-java"><![CDATA[JsonDocument loaded = bucket.get("document_id");

// Remove and take the CAS into account
JsonDocument removed = bucket.remove(loaded);]]></codeblock>

    </section>

    <section id="java-modifying-expiration"><title>Modifying expiration</title><p>Many methods
                support setting the expiry value as part of their other primary operations: </p><ul>
                <li><apiname>Bucket#touch</apiname>: Resets the expiry time for the given document
                    ID to the value provided.</li>
                <li><apiname>Bucket#getAndTouch</apiname>: Fetches the document and resets the
                    expiry to the given value provided.</li>
                <li><apiname>Bucket#insert</apiname>, <apiname>Bucket#upsert</apiname>,
                        <apiname>Bucket#replace</apiname>: Stores the expiry value alongside the
                    actual mutation when set on the <codeph>Document</codeph> instance.</li>
            </ul><p>The following example stores a document with an expiry, waits a bit longer and
                as a result no document is found on the subsequent get: </p><codeblock outputclass="language-java"><![CDATA[int expiry = 2; // seconds
JsonDocument stored = bucket.upsert(
    JsonDocument.create("expires", expiry, JsonObject.create().put("some", "value"))
);

Thread.sleep(3000);

System.out.println(bucket.get("expires"));]]></codeblock><screen>null</screen>You may also use the
                <apiname>Bucket#touch()</apiname> method to modify expiration without fetching or
            modifying the document:<codeblock>bucket.touch("expires", 2);</codeblock></section>

    <section id="java-atomic-modifications">
            <title>Atomic Document Modifications</title>Additional atomic document modifications can
            be performing using the Java SDK. You can modify a <xref
                href="../core-operations.dita#devguide_kvcore_generic/devguide_kvcore_counter_generic"
                >counter document</xref> using the <apiname>Bucket.counter()</apiname> method. You
            can also use the <apiname>Bucket.append()</apiname> and
                <apiname>Bucket.prepend()</apiname> methods to perform <xref
                href="../core-operations.dita#devguide_kvcore_generic/devguide_kvcore_append_prepend_generic"
                >raw byte concatenation</xref>. </section>

    <section id="java-batching-ops">
        <title>Batching Operations</title>
        <p>Since the Java SDK uses RxJava as its asynchronous foundation, all operations can be
                    <xref href="../batching-operations.dita#concept_qfq_5jg_1t"
                    >batched</xref> in the SDK using the <xref
                    href="async-programming.dita#async-programming-java"> asynchronous
                    API</xref> via <codeph>bucket.async()</codeph> (and optionally revert back to
                blocking). </p>
        <p>For implicit batching use these operators:
            <codeph>Observable.just()</codeph> or
            <codeph>Observable.from()</codeph> to generate an observable
            that contains the data you want to batch on.
            <codeph>flatMap()</codeph> to send those events against the
            Couchbase Java SDK and merge the results asynchronously.
            <codeph>last()</codeph> if you want to wait until the last
            element of the batch is received. <codeph>toList()</codeph>
            if you care about the responses and want to aggregate them
            easily. If you have more than one subscriber, use
            <codeph>cache()</codeph> to prevent accessing the network
            over and over again with every subscribe.
        </p>
        <p>The following example creates an observable stream of 6 keys
            to load in a batch, asynchronously fires off
            <codeph>get()</codeph> requests against the SDK (notice the
            <codeph>bucket.async().get(...)</codeph>), waits until the
            last result has arrived, and then converts the result into a
            list and blocks at the very end. This pattern can be reused
            for mutations like <codeph>upsert</codeph> (as shown further
            down):</p>
        <codeblock outputclass="language-java"><![CDATA[Cluster cluster = CouchbaseCluster.create();
Bucket bucket = cluster.openBucket();

List<JsonDocument> foundDocs = Observable
    .just("key1", "key2", "key3", "key4", "inexistentDoc", "key5")
    .flatMap(new Func1<String, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(String id) {
            return bucket.async().get(id);
        }
    })
    .toList()
    .toBlocking()
    .single();

for (JsonDocument doc : foundDocs) {
    System.out.println(doc.id());
}]]></codeblock>
<screen>key1
key2
key3
key4
key5</screen>
        <p>Note that this always returns a list, but it may contain 0
            to 6 documents (here 5) depending on how many are actually
            found. Also, at the very end the observable is converted
            into a blocking one, but everything before that, including
            the network calls and the aggregation, is happening
            completely asynchronously.</p>
        <p>Inside the SDK, this provides much more efficient resource
            utilization because the requests are very quickly stored in
            the internal Request RingBuffer and the I/O threads are able
            to pick batches as large as they can. Afterward, whatever
            server returns a result first it is stored in the list, so
            there is no serialization of responses going on.</p>
        <p>Batching mutations: The previous Java SDK only provided bulk
            operations for get(). With the techniques shown above, you
            can perform any kind of operation as a batch operation. The
            following code generates a number of fake documents and
            inserts them in one batch. Note that you can decide to
            either collect the results with <codeph>toList()</codeph> as
            shown above or just use <codeph>last()</codeph> as shown
            here to wait until the last document is properly inserted:</p>
<codeblock outputclass="language-java"><![CDATA[// Generate a number of dummy JSON documents
int docsToCreate = 100;
List<JsonDocument> documents = new ArrayList<JsonDocument>();
for (int i = 0; i < docsToCreate; i++) {
    JsonObject content = JsonObject.create()
        .put("counter", i)
        .put("name", "Foo Bar");
    documents.add(JsonDocument.create("doc-"+i, content));
}

// Insert them in one batch, waiting until the last one is done.
Observable
    .from(documents)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(final JsonDocument docToInsert) {
            return bucket.async().insert(docToInsert);
        }
    })
    .last()
    .toBlocking()
    .single();]]></codeblock>
    </section>

    <section id="java-subdocs">
        <title>Operating with Sub-Documents</title>
        <note type="tip">Sub-Document API is available starting Couchbase Server version 4.5. See <xref href="../subdocument-operations.dita#subdoc-operations"/> for
            an overview.</note> 
        <p>Sub-document operations save network bandwidth by allowing
            you to specify <i>paths</i> of a document to be retrieved or
            updated. The document is parsed on the server and only the
            relevant sections (indicated by <i>paths</i>) are
            transferred between client and server. You can execute
            <xref href="../subdocument-operations.dita#subdoc-operations" >sub-document</xref>
            operations in the Java SDK using the
            <apiname>Bucket#lookupIn()</apiname> and
            <apiname>Bucket#mutateIn()</apiname> methods.</p>
        <p>Each of these methods accepts a <parmname>key</parmname> as
            its mandatory first argument and give you a builder that you
            can use to chain several <i>command specifications</i>, each
            specifying the path to be impacted by the specified
            operation and a document field operand. You may find all the
            operations in the <apiname>LookupInBuilder</apiname> and
            <apiname>MutateInBuilder</apiname> classes.</p>

<codeblock outputclass="language-java"><![CDATA[bucket.lookupIn("docid")
    .get("path.to.get")
    .exists("check.path.exists")
    .execute();

boolean createParents = true;
bucket.mutateIn("docid")
    .upsert("path.to.upsert", value, createParents)
    .remove("path.to.del"))
    .execute();]]></codeblock>

        <p>All sub-document operations return a special
            <apiname>DocumentFragment</apiname> object rather than a
            <apiname>Document</apiname>. It shares the
            <codeph>id()</codeph>, <codeph>cas()</codeph> and
            <codeph>mutationToken()</codeph> fields of a document, but
            in contrast with a normal <apiname>Document</apiname>
            object, a <apiname>DocumentFragment</apiname> object
            contains multiple results with multiple statuses, one
            result/status pair for every input operation. So it exposes
            method to get the <codeph>content()</codeph> and
            <codeph>status()</codeph> of each spec, either by index or
            by path. It also allows to check that a response for a
            particular spec <codeph>exists()</codeph>:</p>
<codeblock outputclass="language-java"><![CDATA[DocumentFragment<Lookup> res =
bucket.lookupIn("docid")
    .get("foo")
    .exists("bar")
    .exists("baz")
    .execute();

// First result
res.content("foo");
// or
res.content(0);]]></codeblock>

        <p>Using the <codeph>content(...)</codeph> methods will raise
            an exception if the individual spec did not complete
            successfully. You can also use the
            <codeph>status(...)</codeph> methods to return an error code
            (a <apiname>ResponseStatus</apiname>) rather than throw an
            exception.</p>
    </section>

    <section id="java-formats-non-json">
        <title>Formats and Non-JSON Documents</title>
        <note type="tip">See <xref href="../nonjson.dita#devguide_nonjson"/> for a general overview of using non-JSON documents with Couchbase</note>
        <p>The Java SDK defines several concrete implementations of a
            <apiname>Document</apiname> to represent the various data
            types that it can store. Here is the complete list of
            document types:
            <table>
                <title>Documents with JSON content</title>
                <tgroup cols="2">
                <thead><row>
                    <entry>Document Name</entry><entry>Description</entry>
                </row></thead>
                <tbody>
                <row>
                    <entry><apiname>JsonDocument</apiname></entry>
                    <entry>The default, which has a <apiname>JsonObject</apiname> at the top level content.</entry>
                </row>
                <row>
                    <entry><apiname>RawJsonDocument</apiname></entry>
                    <entry>Stores any JSON value and should be used if custom JSON serializers such as Jackson or GSON are already in use.</entry>
                </row>
                <row>
                    <entry><apiname>JsonArrayDocument</apiname></entry>
                    <entry>Similar to JsonDocument, but has a <apiname>JsonArray</apiname> at the top level content.</entry>
                </row>
                <row>
                    <entry><apiname>JsonBooleanDocument</apiname></entry>
                    <entry>Stores JSON-compatible Boolean values.</entry>
                </row>
                <row>
                    <entry><apiname>JsonLongDocument</apiname></entry>
                    <entry>Stores JSON compatible long (number) values.</entry>
                </row>
                <row>
                    <entry><apiname>JsonDoubleDocument</apiname></entry>
                    <entry>Stores JSON compatible double (number) values.</entry>
                </row>
                <row>
                    <entry><apiname>JsonStringDocument</apiname></entry>
                    <entry>Stores JSON compatible <apiname>String</apiname> values. Input is automatically wrapped with quotes when stored.</entry>
                </row>
                <row>
                    <entry><apiname>EntityDocument</apiname></entry>
                    <entry>Used with the <apiname>Repository</apiname> implementation to write and read POJOs into JSON and back.</entry>
                </row>
                </tbody>
                </tgroup>
            </table>

            <table>
                <title>Documents with other content</title>
                <tgroup cols="2">
                <thead><row>
                    <entry>Document Name</entry><entry>Description</entry>
                </row></thead>
                <tbody>
                <row>
                    <entry><apiname>BinaryDocument</apiname></entry>
                    <entry>Can be used to store arbitrary binary data.</entry>
                </row>
                <row>
                    <entry><apiname>SerializableDocument</apiname></entry>
                    <entry>Stores objects that implement <apiname>Serializable</apiname> through default Java object serialization.</entry>
                </row>
                <row>
                    <entry><apiname>LegacyDocument</apiname></entry>
                    <entry>Uses the <apiname>Transcoder</apiname> from the 1.x SDKs and can be used for full cross-compatibility between the old and new versions.</entry>
                </row>
                <row>
                    <entry><apiname>StringDocument</apiname></entry>
                    <entry>Can be used to store arbitrary strings. They will not be quoted, but stored as-is and flagged as "String".</entry>
                </row>
                </tbody>
                </tgroup>
            </table>
        </p>
        <p>You can implement a custom document type and associated
            transcoder if none of the pre-configured options are
            suitable for your application. A custom transcoder converts
            intputs to their serialized forms, and deserializes encoded
            data based on the item flags. There is an
            <apiname>AbstractTranscoder</apiname> that can serve as the
            basis for a custom implementation, and custom transcoders
            should be registered with a <apiname>Bucket</apiname> when
            calling <apiname>Cluster#openBucket</apiname> (a list of
            custom transcoders can be passed in one of the overloads).</p>
    </section>

    <section id="java-binary-document">
        <title>Correctly Managing BinaryDocuments</title>
        <p>The <codeph>BinaryDocument</codeph> can be used to store and
            read arbitrary bytes. It is the only default codec that
            directly exposes the underlying low-level Netty
            <codeph>ByteBuf</codeph> objects.</p>
		<note type="important">Because the raw data is exposed, it is important to free it
            after it has been properly used. Not freeing it will result
            in increased garbage collection and memory leaks and should
            be avoided by all means. See <xref href="#sdk-operations-java/binary-memory"/>.</note>
        <p>Because binary data is arbitrary anyway, it is backward
            compatible with the old SDK regarding flags so that it can
            be read and written back and forth. Make sure it is not
            compressed in the old SDK and that the same encoding and
            decoding process is used on the application side to avoid
            data corruption.</p>
        <p>Here is some demo code that shows how to write and read raw
            data. The example writes binary data, reads it back, and
            then frees the pooled resources:</p>
        <codeblock outputclass="language-java"><![CDATA[// Create buffer out of a string
ByteBuf toWrite = Unpooled.copiedBuffer("Hello World", CharsetUtil.UTF_8);

// Write it
bucket.upsert(BinaryDocument.create("binaryDoc", toWrite));

// Read it back
BinaryDocument read = bucket.get("binaryDoc", BinaryDocument.class);

// Print it
System.out.println(read.content().toString(CharsetUtil.UTF_8));

// Free the resources
ReferenceCountUtil.release(read.content());]]></codeblock>
    </section>

    <section id="binary-memory">
        <title>Correctly Managing Buffers</title>
        <p><codeph>BinaryDocument</codeph> allows users to get the
            rawest form of data out of Couchbase. It  exposes Netty's
            <codeph>ByteBuf</codeph>, byte buffers that can have various
            characteristics (on- or off-heap, pooled or unpooled). In
            general, buffers created by the SDK are pooled and off heap.
            You can disable the pooling in the
            <codeph>CouchbaseEnvironment</codeph> if you absolutely need
            that.</p>
        <p>As a consequence, the memory associated with the ByteBuf
            must be a little bit more managed by the developer than
            usual in Java.</p>
        <p>Most notably, these byte buffers are reference counted, and
            you need to know three main methods associated to buffer
            management:
            <ul><li>
                <codeph>refCnt()</codeph> gives you the current
                reference count. When it hits 0, the buffer is released
                back to its original pool, and it cannot be used
                anymore.
            </li>
            <li>
                <codeph>release()</codeph> will decrease the reference
                count by 1 (by default).
            </li>
            <li>
                <codeph>retain()</codeph> is the inverse of release,
                allowing you to prepare for multiple consumptions by
                external methods that you know will each release the
                buffer.
            </li></ul></p>
        <p>You can also use
            <codeph>ReferenceCountUtil.release(something)</codeph> if
            you don't want to check if <codeph>something</codeph> is
            actually a <codeph>ByteBuf</codeph> (will do nothing if it's
            not something that is <apiname>ReferenceCounted</apiname>).</p>
        <note type="important">
            The SDK bundles the Netty dependency into a different
            package so that it doesn't clash with a dependency to
            another version of Netty you may have. As such, you need to
            use the classes and packages provided by the SDK
            (<codeph>com.couchbase.client.deps.io.netty</codeph>) when
            interacting with the API. For example, the
            <codeph>ByteBuf</codeph> for the content of a
            <codeph>BinaryDocument</codeph> is a
            <codeph>com.couchbase.client.deps.io.netty.buffer.ByteBuf</codeph>.
        </note>

        <p><b>What happens if I don't release?</b></p>
        <p>Basically, you leak memory... Netty will by default inspect
            a small percentage of <codeph>ByteBuf</codeph> creations and
            usage to try and detect leaks (in which case it will output
            a log, look for the "LEAK" keyword).</p>
        <p>You can tune that to be more eagerly monitoring all buffers
            by calling
            <codeph>ResourceLeakDetector.setLevel(PARANOID)</codeph>.
            <note type="important" >Note that this incurs quite an
                overhead and should only be activated in tests. In
                production (prod), setting it to
                <codeph>ADVANCED</codeph> is not as heavy as paranoid
                and can be a good middle ground.</note></p>

        <p><b>What happens if I release twice (or the SDK releases once more after I do)?</b></p>
        <p>Netty will throw
            <codeph>IllegalReferenceCountException</codeph>. The buffer
            that has RefCnt = 0 cannot be interacted with anymore since
            it means it has been freed back into the pool.
        </p>

        <p><b>When must I release?</b></p>
        <p>When the SDK creates a <codeph>BinaryDocument</codeph> for
            you, basically GET-type operations.
        </p>
        <p>Mutative operations, on the other hand, will take care of
            the buffer you pass in for you, at the time the buffer is
            written on the wire.
        </p>

        <p><b>When must I usually retain?</b></p>
        <p>When you do a write, the buffer will usually be released by
            the SDK calling <codeph>release()</codeph>. But if you
            implement a kind of fallback behavior (for instance attempt
            to <codeph>insert()</codeph> a doc, catch
            <codeph>DocumentAlreadyExistException</codeph> and then
            fallback to an <codeph>update()</codeph> instead), that
            means the SDK would attempt to release twice, which won't
            work.
        </p>
        <p>
            In this case you can <codeph>retain()</codeph> the buffer
            before the first attempt, let the catch block do the extra
            release if something goes wrong. You have to manage the
            extra release if the first write succeeds, and think about
            catching other possible exceptions (here also an extra
            release is needed):
        </p>
<codeblock outputclass="language-java"><![CDATA[byteBuffer.retain(); //prepare for potential multi usage (+1 refCnt, refCnt = 2)
try {
   bucket.append(document);
   // refCnt = 2 on success
   byteBuffer.release(); //refCnt = 1
} catch (DocumentDoesNotExistException dneException) {
   // buffer is released on errors, refCnt = 1
   //second usage will also release, but we want to be at refCnt = 1 for the finally block
   byteBuffer.retain(); //refCnt = 2
   bucket.insert(document); //refCnt = 1
} // other uncaught errors will still cause refCnt to be released down to 1
finally {
   //we made sure that at this point refCnt = 1 in any case (success, caught exception, uncaught exception)
   byteBuffer.release(); //refCnt = 0, returned to the pool
}]]></codeblock>
    </section>

    </conbody>
</concept>
