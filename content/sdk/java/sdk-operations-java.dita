<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_ofb_4d5_xv">
  <title>SDK Operations</title><shortdesc>The Java SDK offers methods you can use to accomplish the basic CRUD operations of creating, retrieving, updating, and deleting documents.</shortdesc>
  <body>
   
   	<section>
			<title>Document Basics</title>
   		<p>This section describes the properties and types of <codeph>Document</codeph> objects and how to use
   			them.</p>

			<p>The <codeph>Document</codeph> class encapsulates the consolidated representation of all
				attributes that relate to a document stored on a Couchbase Server cluster. It includes
				the document's identifier and related metadata. A <codeph>Document</codeph> object
				contains the following properties:</p>

			<table>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Name</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								<codeph>id</codeph>
							</entry>
							<entry>The (per bucket) unique identifier of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>content</codeph>
							</entry>
							<entry>The actual content of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>cas</codeph>
							</entry>
							<entry>The CAS (Compare And Swap) value of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>expiry</codeph>
							</entry>
							<entry>The expiration time of the document.</entry>
						</row>
						<row>
							<entry>
								<codeph>mutationToken</codeph>
							</entry>
							<entry>The optional MutationToken after a mutation.</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<p>There are a few different implementations of a <codeph>Document</codeph>, the most
				prominent one is the <codeph>JsonDocument</codeph>.</p>

			<p>Because Couchbase Server can store anything and not just JSON, many document types exist to
				satisfy the general needs of an application. You can also write your own
					<codeph>Document</codeph> implementations, which is not covered in this
				introduction.</p>

			<p>
				<note>Every <codeph>Document</codeph> has an associated <codeph>Transcoder</codeph> that handles
					serialization and deserialization to and from the target wire format. This
					conversion is transparent but needs to be taken into account when custom
					documents are implemented.</note>
			</p>

			<p>The following <codeph>Document</codeph> types are supported out of the box:</p>

			<p>
				<b>Documents with JSON content:</b>
			</p>

			<table>
				<tgroup cols="4">
					<thead>
						<row>
							<entry>Document Name</entry>
							<entry>Description</entry>
							<entry>Compatible: 2.x SDKs</entry>
							<entry>Compatible: 1.x Java SDK</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								<xref href="#documents-basics/json"><codeph>JsonDocument</codeph></xref>
							</entry>
							<entry>The default, which has a <codeph>JsonObject</codeph> at the top level
								content.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonarray"><codeph>JsonArrayDocument</codeph></xref>
							</entry>
							<entry>Similar to <codeph>JsonDocument</codeph>, but has a
									<codeph>JsonArray</codeph> at the top level content.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonBooleanDocument</codeph></xref>
							</entry>
							<entry>Stores JSON-compatible Boolean values.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonLongDocument</codeph></xref>
							</entry>
							<entry>Stores JSON compatible long (number) values.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonDoubleDocument</codeph></xref>
							</entry>
							<entry>Stores JSON compatible double (number) values.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonvalue"><codeph>JsonStringDocument</codeph></xref>
							</entry>
							<entry>Stores JSON compatible String values. Input is automatically wrapped
								with <codeph>"..."</codeph>.</entry>
							<entry>Yes</entry>
							<entry>Partially</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/jsonraw"><codeph>RawJsonDocument</codeph></xref>
							</entry>
							<entry>Stores any JSON value and should be used if custom JSON serializers such
								as Jackson or GSON are already in use.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<codeph>EntityDocument</codeph>
							</entry>
							<entry>Used with the Repository implementation to write and read POJOs into JSON and back.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<p>
				<b>Documents with other content:</b>
			</p>

			<table>
				<tgroup cols="4">
					<thead>
						<row>
							<entry>Document Name</entry>
							<entry>Description</entry>
							<entry>Compatible: 2.x SDKs</entry>
							<entry>Compatible: 1.x Java SDK</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								<xref href="#documents-basics/binary"><codeph>BinaryDocument</codeph></xref>
							</entry>
							<entry>Can be used to store arbitrary binary data.</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/serializable"><codeph>SerializableDocument</codeph></xref>
							</entry>
							<entry>Stores objects that implement <codeph>Serializable</codeph> through
								default Java object serialization.</entry>
							<entry>No</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/legacy"><codeph>LegacyDocument</codeph></xref>
							</entry>
							<entry>Uses the <codeph>Transcoder</codeph> from the 1.x SDKs and can be used
								for full cross-compatibility between the old and new versions.</entry>
							<entry>No</entry>
							<entry>Yes</entry>
						</row>
						<row>
							<entry>
								<xref href="#documents-basics/string"><codeph>StringDocument</codeph></xref>
							</entry>
							<entry>Can be used to store arbitrary strings. They will not be quoted, but
								stored as-is and flagged as "String".</entry>
							<entry>Yes</entry>
							<entry>Yes</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<p>
				<note type="other" othertype="Backward compatibility">Other than the
						<codeph>LegacyDocument</codeph> class, which strives for full backward
					compatibility, all <codeph>Document</codeph> types are trying best effort in that
					regard. Specific constraints are noted in each document description, but for all
					types keep in mind that compression is not supported other than on the
						<codeph>LegacyDocument</codeph>.</note>
			</p>

		
<sectiondiv>
	<p><b>CAS and Expiry</b></p>

			<p>Every <codeph>Document</codeph> also contains the <codeph>expiry</codeph> and
					<codeph>cas</codeph> properties. They are considered meta information and are
				optional. An expiration time of <codeph>0</codeph> means that no expiration is set at
				all, and a <codeph>CAS</codeph> value 0 means it won't be used.</p>

			<p>You can set the <codeph>expiry</codeph> to control when the document should be
				deleted:</p>

			<codeblock outputclass="language-java"><![CDATA[// Expire in 10 seconds.
JsonDocument.create("id", 10, content);]]></codeblock>

			<codeblock outputclass="language-java"><![CDATA[// Expire in 1 day.
JsonDocument.create("id", TimeUnit.DAYS.toSeconds(1), content);]]></codeblock>

			<p>The expiration time starts when the document has been successfully stored on the server,
				not when the document was created on the application server. Any expiration time larger
				than 30 days in seconds is considered absolute (as in a Unix time stamp), anything
				smaller is considered relative in seconds.</p>

			<p>The <codeph>CAS</codeph> value can either be set by you directly or is populated by the
				SDK when the <codeph>Document</codeph> is loaded from the server (which is the
				recommended way to use it).</p>

			<p>For detailed information about how to utilize CAS for optimistic concurrency control, see
					<xref href="#topic_ofb_4d5_xv/updatingdocuments" format="dita"/> .</p>
</sectiondiv>
<sectiondiv id="json">

		<p><b><codeph>JsonDocument</codeph></b></p>

			<p>Couchbase Server uses the JSON format as a first-class citizen. It is used for querying
				(via both views and N1QL) and represents the main storage format that should be
				used.</p>

			<p>The <codeph>JsonDocument</codeph> class has factory methods named
					<codeph>create()</codeph> that you use to create documents. If you do not want to
				pass in an expiration time or CAS value (just the ID and content) you do it like
				this:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonDocument doc = JsonDocument.create("id", content);]]></codeblock>

			<p>The content needs to be of type <codeph>JsonObject</codeph>, which ships with the Java SDK. It
				works very much like a <codeph>Map</codeph> object but makes sure only data types
				understood by JSON are used.</p>

			<p>An empty JSON document can be created like this:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty();]]></codeblock>

			<p>After it is created, you can use the various <codeph>put()</codeph> methods to insert
				data:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonArray friends = JsonArray.empty()
	.add(JsonObject.empty().put("name", "Mike Ehrmantraut"))
	.add(JsonObject.empty().put("name", "Jesse Pinkman"));

JsonObject content = JsonObject.empty()
	.put("firstname", "Walter")
	.put("lastname", "White")
	.put("age", 52)
	.put("aliases", JsonArray.from("Walt Jackson", "Mr. Mayhew", "David Lynn"))
	.put("friends", friends);]]></codeblock>

			<p>This generates a JSON document like this (unordered, because the actual content is
				stored in a <codeph>Map</codeph>):</p>

			<codeblock outputclass="language-json"><![CDATA[{
   "firstname":"Walter",
   "aliases":[
	  "Walt Jackson",
	  "Mr. Mayhew",
	  "David Lynn"
   ],
   "age":52,
   "friends":[
	  {
		 "name":"Mike Ehrmantraut"
	  },
	  {
		 "name":"Jesse Pinkman"
	  }
   ],
   "lastname":"White"
}]]></codeblock>

			<p>
				<image href="images/document-jsonobject.png" id="image_i31_yqb_1" width="650px"/>
			</p>

			<p>In addition, the <codeph>JsonObject</codeph> and <codeph>JsonArray</codeph> classes
				provide convenience methods to generate and modify them.</p>

			<p>The <codeph>JsonDocument</codeph> can then be passed into the various operations on the
					<codeph>Bucket</codeph>:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonDocument walter = JsonDocument.create("user:walter", content);
JsonDocument inserted = bucket.insert(walter);]]></codeblock>

			<p>If you want to read values out of the <codeph>JsonDocument</codeph>, you can use either
				the typed or untyped getter methods.</p>

			<codeblock outputclass="language-java"><![CDATA[int age = content.getInt("age");
String name = content.getString("firstname") + content.getString("lastname");]]></codeblock>

			<p>
				<note>If you are accessing values that potentially do not exist, you need to use boxed
					values (<codeph>Integer</codeph>, <codeph>Long</codeph>, <codeph>Boolean</codeph>)
					instead of their unboxed variants (<codeph>int</codeph>, <codeph>long</codeph>,
						<codeph>boolean</codeph>) to avoid getting <codeph>NullPointerException</codeph>
					exceptions. If you use unboxed variants, make sure to catch them properly.</note>
			</p>

</sectiondiv>
   		<sectiondiv id="jsonarray">


		<p><codeph><b>JsonArrayDocument</b></codeph></p>

			<p>The <codeph>JsonArrayDocument</codeph> class works exactly like the
					<codeph>JsonDocument</codeph> class, with the main difference that you can have a
				JSON array at the top level content (instead of an object).</p>

			<p>So if you create a <codeph>JsonArrayDocument</codeph> like this:</p>

			<codeblock outputclass="language-java"><![CDATA[JsonArray content = JsonArray.from("Hello", "World", 1234);
bucket.upsert(JsonArrayDocument.create("docWithArray", content));]]></codeblock>

			<p>It will look like this on the server:</p>

			<p>
				<image href="images/document-jsonarray.png" id="image_json-array" width="650px" />
			</p>

			<p>If you want to read the <codeph>JsonArrayDocument</codeph> back, you need to tell the SDK that
				you explicitly want to deviate from the default. Do it for every document type other
				than <codeph>JsonDocument</codeph>:</p>

			<codeblock outputclass="language-java"><![CDATA[bucket.get("docWithArray", JsonArrayDocument.class);]]></codeblock>

   		</sectiondiv>
   		<sectiondiv id="jsonraw">

	
		<p><codeph><b>RawJsonDocument</b></codeph></p>

			<p>The <codeph>JsonObject</codeph> and <codeph>JsonArray</codeph> types have been added for
				developer convenience. In a lot of places, custom JSON handling is already in place
				through libraries like Jackson or Google GSON.</p>

			<p>Of course, we want to provide the best of both worlds, and this is where the
					<codeph>RawJsonDocument</codeph> comes into play. You can store and read the
				already stringified JSON, but the SDK properly marks it as JSON, so it is
				cross-compatible with all other documents.</p>

			<p>Here is how you can read and write raw JSON data. For clarity, a plain string is used
				but it is up to you to wire this up with Jackson or a similar JSON processor:</p>

			<codeblock outputclass="language-java"><![CDATA[// write the raw data
String content = "{\"hello\": \"couchbase\", \"active\": true}";
bucket.upsert(RawJsonDocument.create("rawJsonDoc", content));

// read the raw data
// prints RawJsonDocument{id='rawJsonDoc', cas=..., expiry=0, content={"hello": "couchbase", "active": true}}
System.out.println(bucket.get("rawJsonDoc", RawJsonDocument.class));

// read it parsed
// prints true
System.out.println(bucket.get("rawJsonDoc").content().getBoolean("active"));]]></codeblock>

			<p>
				<image href="images/document-rawjson.png" id="image_i31_yqb_3p" width="650px"/>
			</p>

			<p>
				<note>If you use the <codeph>RawJsonDocument</codeph> type, the SDK does not perform any
					validation because the expectation is that a JSON-compatible library is used,
					and additional overhead will be avoided.</note>
			</p>
   		</sectiondiv>
   		<sectiondiv id="jsonvalue">

		
		<p><b>JSON value documents</b></p>

			<p>The JSON specification also allows you to store different values as content, and it also
				specifies how these values need to be encoded. Because the type system of Java is
				not as rich as it could be, different document types are provided to represent
				different values that can be stored. Because the encoding is clearly defined, these
				JSON values are also compatible with other 2.0 SDKs.</p>

			<p>A word on compatibility with the 1.X Java SDK: in a best-effort way the SDK tries to read
				properly flagged data from the old SDKs, but it stores it under the new format,
				which is not readable by the old SDKs anymore. So if you care about back-and-forth
				compatibility only read those values from the new SDK or use the
					<codeph>LegacyDocument</codeph> right away. Another option is to use strings
				only on the old SDK, and then working with it back and forth should be safe.</p>

			<p>Backward compatibility for JSON value documents works only if the actual content is not
				compressed.</p>

			<p>The following documents exist, which all work similarly except the content type that can
				be stored:</p>

			<ul>
				<li>
					<codeph>JsonBooleanDocument</codeph>
				</li>
				<li>
					<codeph>JsonLongDocument</codeph>
				</li>
				<li>
					<codeph>JsonDoubleDocument</codeph>
				</li>
				<li>
					<codeph>JsonStringDocument</codeph>
				</li>
			</ul>

			<p>They are all encoded and decoded based on their <xref href="http://json.org"
					format="html" scope="external">JSON specification</xref>.</p>
   		</sectiondiv>
   		<sectiondiv id="binary">

		
		<p><codeph><b>BinaryDocument</b></codeph></p>
			<p>The <codeph>BinaryDocument</codeph> can be used to store and read arbitrary bytes. It is
				the only default codec that directly exposes the underlying low-level Netty
					<codeph>ByteBuf</codeph> objects.</p>
			<p>
				<note type="important">Because the raw data is exposed, it is important to free it after it has
						been properly used. Not freeing it will result in increased garbage
						collection and memory leaks and should be avoided by all means. See <xref
							href="#topic_ofb_4d5_xv/binary-memory" format="dita">Correctly Managing Buffers</xref>.</note>
			</p>
			<p>Because binary data is arbitrary anyway, it is backward compatible with the old SDK regarding
				flags so that it can be read and written back and forth. Make sure it is not
				compressed in the old SDK and that the same encoding and decoding process is used on
				the application side to avoid data corruption.</p>
			<p>Here is some demo code that shows how to write and read raw data. The example writes
				binary data, reads it back, and then frees the pooled resources:</p>
			<codeblock outputclass="language-java"><![CDATA[// Create buffer out of a string
ByteBuf toWrite = Unpooled.copiedBuffer("Hello World", CharsetUtil.UTF_8);

// Write it
bucket.upsert(BinaryDocument.create("binaryDoc", toWrite));

// Read it back
BinaryDocument read = bucket.get("binaryDoc", BinaryDocument.class);

// Print it
System.out.println(read.content().toString(CharsetUtil.UTF_8));

// Free the resources
ReferenceCountUtil.release(read.content());]]></codeblock>
   		</sectiondiv>
   		<sectiondiv id="binary-memory">
		
		<p><b>Correctly managing buffers</b></p>
			<p>
				<codeph>BinaryDocument</codeph> allows users to get the rawest form of data out of
				Couchbase. It  exposes Netty's <codeph>ByteBuf</codeph>, byte buffers that can have
				various characteristics (on- or off-heap, pooled or unpooled). In general, buffers
				created by the SDK are pooled and off heap. You can disable the pooling in the
					<codeph>CouchbaseEnvironment</codeph> if you absolutely need that. </p>
			<p>
				As a consequence, the memory associated with the ByteBuf must be a little bit more managed by the developer than usual in Java.
			</p>
			<p> Most notably, these byte buffers are reference counted, and you need to know three main
				methods associated to buffer management: <ul>
					<li><codeph>refCnt()</codeph> gives you the current reference count. When it
						hits 0, the buffer is released back to its original pool, and it cannot be
						used anymore.</li>
					<li><codeph>release()</codeph> will decrease the reference count by 1 (by
						default).</li>
					<li><codeph>retain()</codeph> is the inverse of release, allowing you to prepare
						for multiple consumptions by external methods that you know will each
						release the buffer. </li>
				</ul></p>
			<p>You can also use <codeph>ReferenceCountUtil.release(something)</codeph> if you don't want to
				check if <codeph>something</codeph> is actually a <codeph>ByteBuf</codeph> (will do
				nothing if it's not something that is <apiname>ReferenceCounted</apiname>). </p>
			<note type="important">
				The SDK bundles the Netty dependency into a different package so that it doesn't clash with a dependency to another version of Netty you may have. As such, you need to use the classes and packages provided by the SDK (<codeph>com.couchbase.client.deps.io.netty</codeph>) when interacting with the API. For example, the <codeph>ByteBuf</codeph> for the content of a <codeph>BinaryDocument</codeph> is a <codeph>com.couchbase.client.deps.io.netty.buffer.ByteBuf</codeph>.
			</note>

			<p><b>What happens if I don't release?</b></p>
			<p>Basically, you leak memory... </p>
			<p>Netty will by default inspect a small percentage of <codeph>ByteBuf</codeph>
				creations and usage to try and detect leaks (in which case it will output a log,
				look for the "LEAK" keyword). </p>
			<p>You can tune that to be more eagerly monitoring all buffers by calling
					<codeph>ResourceLeakDetector.setLevel(PARANOID)</codeph>. <note type="important"
					>Note that this incurs quite an overhead and should only be activated in tests.
					In production (prod), setting it to <codeph>ADVANCED</codeph> is not as heavy as
					paranoid and can be a good middle ground.</note>
			</p>
			<p><b>What happens if I release twice (or the SDK releases once more after I do)?</b></p>
			<p>Netty will throw an <codeph>IllegalReferenceCountException</codeph>. The buffer that has
				RefCnt = 0 cannot be interacted with anymore since it means it has been freed back
				into the pool. </p>
			<p><b>When must I release?</b></p>
			<p>When the SDK creates a <codeph>BinaryDocument</codeph> for you, basically GET-type operations. </p>
			<p>Mutative operations, on the other hand, will take care of the buffer you pass in for you, at
				the time the buffer is written on the wire. </p>
			<p><b>When must I usually retain?</b></p>
			<p>When you do a write, the buffer will usually be released by the SDK calling
					<codeph>release()</codeph>. But if you implement a kind of fallback behavior
				(for instance attempt to <codeph>insert()</codeph> a doc, catch
					<codeph>DocumentAlreadyExistException</codeph> and then fallback to an
					<codeph>update()</codeph> instead), that means the SDK would attempt to release
				twice, which won't work. </p>
			<p>In this case you can <codeph>retain()</codeph> the buffer before the first attempt, let the
				catch block do the extra release if something goes wrong. You have to manage the
				extra release if the first write succeeds, and think about catching other possible
				exceptions (here also an extra release is needed): </p>
<codeblock outputclass="language-java"><![CDATA[byteBuffer.retain(); //prepare for potential multi usage (+1 refCnt, refCnt = 2)
try {
   bucket.append(document);
   // refCnt = 2 on success
   byteBuffer.release(); //refCnt = 1
} catch (DocumentDoesNotExistException dneException) {
   // buffer is released on errors, refCnt = 1
   //second usage will also release, but we want to be at refCnt = 1 for the finally block
   byteBuffer.retain(); //refCnt = 2
   bucket.insert(document); //refCnt = 1
} // other uncaught errors will still cause refCnt to be released down to 1
finally {
   //we made sure that at this point refCnt = 1 in any case (success, caught exception, uncaught exception)
   byteBuffer.release(); //refCnt = 0, returned to the pool
}]]></codeblock>

   		</sectiondiv>
   		<sectiondiv id="SerializableDocument">

		
			<p><codeph><b>SerializableDocument</b></codeph></p>

			<p>Any object that implements <codeph>Serializable</codeph> can be safely encoded and
				decoded using the built-in Java serialization mechanism. While it is very convenient, it
				can be slow in cases where the POJOs are very complex and deeply nested. It is backward
				compatible with the old SDK unless the data has been compressed previously.</p>

			<p>Here is an example that serializes a POJO, deserializes it later, and then prints one of
				its properties:</p>

			<codeblock outputclass="language-java"><![CDATA[import java.io.Serializable;

public class User implements Serializable {

	private final String username;

	public User(String username) {
		this.username = username;
	}

	public String getUsername() {
		return username;
	}

}]]></codeblock>

			<codeblock outputclass="language-java"><![CDATA[// Create the User and store it
bucket.upsert(SerializableDocument.create("user::michael",  new User("Michael")));

// Read it back
SerializableDocument found = bucket.get("user::michael", SerializableDocument.class);

// Print a property to verify
System.out.println(((User) found.content()).getUsername());]]></codeblock>

   		</sectiondiv>
   		<sectiondiv id="legacy">
   			
		
		<p><codeph><b>LegacyDocument</b></codeph></p>

			<p>The <codeph>LegacyDocument</codeph> is intended to be 1:1 compatible (including compression)
				with the 1.x Java SDK. For better compatibility with the other 2.0 SDKs, we
				recommend to move to JSON type documents (and other compatible ones), but the
					<codeph>LegacyDocument</codeph> is very helpful during data migration and
				side-by-side usage.</p>

			<p>Because the old and new SDKs don't share artifacts or namespaces, they can be used at
				the same time. If you're using Maven, you can add both a 1.x SDK and a 2.x SDK as
				dependencies in the <filepath>pom.xml</filepath> file. For example:</p>

			<codeblock outputclass="language-xml"><![CDATA[<dependencies>
	<dependency>
		<groupId>com.couchbase.client</groupId>
		<artifactId>java-client</artifactId>
		<version>2.2.7</version>
	</dependency>
	<dependency>
		<groupId>com.couchbase.client</groupId>
		<artifactId>couchbase-client</artifactId>
		<version>1.4.11</version>
	</dependency>
</dependencies>]]></codeblock>

			<p>Here is a snippet that writes a value using the old SDK and reads it out with the new
				one:</p>

			<codeblock outputclass="language-java"><![CDATA[// Open bucket on the new SDK
Cluster cluster = CouchbaseCluster.create();
Bucket bucket = cluster.openBucket();

// Open bucket on the old SDK
CouchbaseClient client = new CouchbaseClient(
	Arrays.asList(URI.create("http://127.0.0.1:8091/pools")),
	"default",
	""
);

// Create document on old SDK
client.set("fromOld", "Hello from Old!").get();

// Create document on new SDK
bucket.upsert(LegacyDocument.create("fromNew", "Hello from New!"));

// Read old from new
System.out.println(bucket.get("fromOld", LegacyDocument.class));

// Read new from old
System.out.println(client.get("fromNew"));

// Shutdown old client
client.shutdown();

// Shutdown new client
cluster.disconnect();]]></codeblock>

			<p>This prints:</p>

			<codeblock><![CDATA[LegacyDocument{id='fromOld', cas=1097880624822, expiry=0, content=Hello from Old!}
Hello from New!]]></codeblock>

   		</sectiondiv>
   		<sectiondiv id="string">

		<p><codeph><b>StringDocument</b></codeph></p>

			<p>This document type provides an SDK 2.0 cross-compatible way to exchange strings. It should not
				be mistaken with the <codeph>JsonStringDocument</codeph> that automatically quotes
				it and also flags it as JSON. It is also backward compatible unless compression was
				used previously.</p>

			<p>If a <codeph>String</codeph> is stored through it, it is explicitly flagged as a
				non-JSON string. The usage is straightforward:</p>

			<codeblock outputclass="language-java"><![CDATA[// Create the document
bucket.upsert(StringDocument.create("stringDoc", "Hello World"));

// Prints:
// StringDocument{id='stringDoc', cas=1424054670330, expiry=0, content=Hello World}
System.out.println(bucket.get("stringDoc", StringDocument.class));]]></codeblock>

			<p>You can use the <codeph>cbc</codeph> command line tool to compare the flags and actual
				content compared to the <codeph>JsonStringDocument</codeph>:</p>

			<codeblock outputclass="language-java"><![CDATA[bucket.upsert(StringDocument.create("stringDoc", "Hello World"));]]></codeblock>

			<codeblock outputclass="language-bash"><![CDATA[└──╼ cbc cat stringDoc
stringDoc            CAS=0x55668b55f010000, Flags=0x4000000. Size=11
Hello World]]></codeblock>

			<codeblock outputclass="language-java"><![CDATA[bucket.upsert(JsonStringDocument.create("jsonStringDoc", "Hello World"));]]></codeblock>

			<codeblock outputclass="language-bash"><![CDATA[└──╼ cbc cat jsonStringDoc
jsonStringDoc        CAS=0x84d77eb55f010000, Flags=0x2000000. Size=13
"Hello World"]]></codeblock>

			<p>You can see that the JSON string got automatically quoted and also has different flags
				applied to it.</p>
   		</sectiondiv>
		</section>
  
  	
  	
  	
  	
  	
  	
  	
  	
  	<section id="creatingdocuments"><title>Creating Documents</title>
  	
  	<p> This section describes how to create documents by using the <codeph>insert()</codeph> or
					<codeph>upsert()</codeph> methods.</p>
  		
  	
  			
  	<sectiondiv>
  			<p><b>Insert</b></p>
  				
  				<p>The <codeph>insert</codeph> method allows you to store a <codeph>Document</codeph> if it does
  					not already exist in the bucket. If it does exist, the synchronous API throws a
  					<codeph>DocumentAlreadyExistsException</codeph> (or the
  					<codeph>Observable</codeph> propagates it to <codeph>onErro</codeph> in the case
  					of the asynchronous API).</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
//this will throw DocumentAlreadyExistException:
JsonDocument inserted = bucket.insert(doc);]]></codeblock>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
Observable<JsonDocument> inserted = bucket.async().insert(doc);
inserted.subscribe(
	System.out::println,
	//this will be called with DocumentAlreadyExistException:
	Throwable::printStackTrace
);]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it is
  					respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is inserted because its type is
  					inferred from the method argument and the corresponding <codeph>Transcoder</codeph> is
  					used to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is different from the
  					<codeph>Document</codeph> passed in. The returned document references some
  					values like its <codeph>id</codeph> and <codeph>content</codeph> but also has the
  					<codeph>CAS</codeph> value set.</p>
  				
  	</sectiondiv>
  			
  		<sectiondiv>
  				<p><b>Upsert</b></p>
  				
  				<p>The <codeph>upsert</codeph> method works similar to <codeph>insert</codeph>, but it also
  					overrides an already stored <codeph>Document</codeph> (so there is no
  					<codeph>DocumentAlreadyExistsException</codeph> thrown by the synchronous API
  					nor propagated by the asynchronous API).</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
JsonDocument inserted = bucket.upsert(doc);]]></codeblock>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
Observable<JsonDocument> inserted = bucket.async().upsert(doc);
inserted.subscribe(
	System.out::println,
	//this won't be called:
	Throwable::printStackTrace
);]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it is
  					respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is upserted because its type is
  					inferred from the method argument and the corresponding <codeph>Transcoder</codeph> is
  					used to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is a different one compared to the
  					<codeph>Document</codeph> passed in. It references some values like its
  					<codeph>id</codeph> and <codeph>content</codeph> but also has the
  					<codeph>CAS</codeph> value set.</p></sectiondiv>
  			
  			
  		<sectiondiv>
  				<p><b>Durability Requirements</b></p>
  				
  				<p>If no durability requirements are set on the <codeph>insert</codeph> or
  					<codeph>upsert</codeph> methods, the operation will succeed when the server
  					acknowledges the document in its managed cache layer. While this is a performant
  					operation, there might be situations where you want to make sure that your document
  					has been persisted or replicated so that it survives power outages and other node
  					failures.</p>
  				
  				<p>Both methods provide overloads to supply such requirements:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[D insert(D document, PersistTo persistTo);
D insert(D document, ReplicateTo replicateTo);
D insert(D document, PersistTo persistTo, ReplicateTo replicateTo);

D upsert(D document, PersistTo persistTo);
D upsert(D document, ReplicateTo replicateTo);
D upsert(D document, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
  				
  				<note type="tip">The synchronous API also provides the same methods with custom timeouts, whereas the asynchronous API in <codeph>AsyncBucket</codeph> would rely on RxJava's <codeph>timeout</codeph> operator.</note>
  				
  				<p>You can configure either just one or both of the requirements when inserting or upserting. From an application point of view nothing needs to be changed when working with the response, although there is something that need to be kept in mind:</p>
  				
  				<p>The internal implementation first performs a regular <codeph>insert</codeph> or
  					<codeph>upsert</codeph> operation and afterward starts polling the specifically
  					affected cluster nodes for the state of the document. If something fails during this
  					operation (and failing the <codeph>Observable</codeph>), the original operation
  					might have succeeded nonetheless.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Insert the document and make sure it is persisted to the master node
bucket.insert(document, PersistTo.MASTER);

// Insert the document and make sure it is replicate to one replica node
bucket.insert(document, ReplicateTo.ONE);

// Insert the document and make sure it is persisted to one node and replicated to two
bucket.insert(document, PersistTo.ONE, ReplicateTo.TWO);]]></codeblock>
  				
  		</sectiondiv>
  			
  		<sectiondiv>
  				<p><b>Batching</b></p>
  				
  				<p>Because everything is asynchronous internally, batching <codeph>inserts</codeph> or
  					<codeph>upserts</codeph> can be achieved with the  <codeph>Observable</codeph>
  					functionality of the <codeph>AsyncBucket</codeph>.</p>
  				
  				<p>A combination of <codeph>just()</codeph> and <codeph>flatMap()</codeph> is used to store them without blocking:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument doc1 = JsonDocument.create("id1", content);
JsonDocument doc2 = JsonDocument.create("id2", content);
JsonDocument doc3 = JsonDocument.create("id3", content);

Observable
    .just(doc1, doc2, doc3)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(JsonDocument document) {
            return bucket.async().insert(document);
        }
    }).subscribe();]]></codeblock>
  				
  				<p>For the blocking API, batching is currently not supported. It is recommended to fall back on the asynchronous API for best performance.</p>
  				
  			
  		</sectiondiv>   
      
    </section>
   
  		
  	
  	
  	
  	
  	
  	
  	<section id="updatingdocuments"><title>Updating Documents</title>
  		
  	<p>This section describes how to update documents by using the <codeph>replace()</codeph> or
					<codeph>upsert()</codeph> methods.</p>
  		
  		
  			
  			<sectiondiv>
  				<p><b>Replace</b></p>
  				
  				<p>The <codeph>replace</codeph> method replaces the <codeph>Document</codeph> if it exists, but fails with a <codeph>DocumentDoesNotExistException</codeph> otherwise.</p>
  				
  				<p>In addition, if the <codeph>CAS</codeph> value is set on the <codeph>Document</codeph> (not
  					equal to <codeph>0</codeph>), it is respected and passed to the server. If the
  					<codeph>CAS</codeph> value does not match the current server value, it fails
  					with a <codeph>CASMismatchException</codeph>.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
JsonDocument inserted = bucket.replace(doc)]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it will be respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is replaced: its type is inferred
  					from the method argument, and the corresponding <codeph>Transcoder</codeph> is used
  					to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is a different one compared to the
  					<codeph>Document</codeph> passed in. It references some values like its
  					<codeph>id</codeph> and <codeph>content</codeph> but also has the new
  					<codeph>CAS</codeph> value set.</p>
  				
  				<p>The following asynchronous sample will automatically take the <codeph>CAS</codeph> value into account because it is populated from the <codeph>get()</codeph> call and respected on the <codeph>replace()</codeph> call.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[bucket.async()
    .get("id")
    .map(new Func1<JsonDocument, JsonDocument>() {
        @Override
        public JsonDocument call(JsonDocument document) {
            modifyDocumentSomehow(document);
            return document;
        }
    })
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(JsonDocument document) {
            return bucket.async().replace(document);
        }
    }).subscribe();]]></codeblock>
  				
  				<p>Since this operation can fail if there is a <codeph>CASMismatchException</codeph>, a common pattern is to retry the complete process until it succeeds:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[Observable
    .defer(() -> bucket.async()get("id"))
    .map(document -> {
        document.content().put("modified", new Date().getTime());
        return document;
    })
    .flatMap(doc -> bucket.async().replace(doc))
    .retryWhen(attempts ->
        attempts.flatMap(n -> {
            if (!(n.getThrowable() instanceof CASMismatchException)) {
                return Observable.error(n.getThrowable());
            }
            return Observable.timer(1, TimeUnit.SECONDS);
        })
    )
    .subscribe();]]></codeblock>
  				<p>This code snippet uses <codeph>defer()</codeph>. This was necessary before SDK 2.2.0 to always do a fresh <codeph>get()</codeph>, but is now already done by the SDK.</p>
  				<note type="remember">Before version 2.2.0, a Subject was directly used in the SDK internally,
  					caching the value and, therefore, a resubscribe would just return the same value.
  					<codeph>defer()</codeph> makes sure to create a new one on every resubscribe. </note>
  				<p>Afterward, it artificially modifies the document and then tries to store it through a
  					<codeph>replace()</codeph> call. If it succeeds all is good, if it fails with an
  					<codeph>Exception</codeph> the <codeph>retryWhen()</codeph> block is executed.
  					In this block, the code checks if it is a <codeph>CASMismatchException</codeph> and
  					if so, executes a <codeph>timer</codeph> before retrying. Other errors could be
  					handled in there as well (even with different retry strategies), but in this
  					example, other errors are passed along.</p>
  				
  				<p>Note that since 2.1.2, such code is easier to write by use of the <codeph>RetryBuilder</codeph>:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[Observable
    .defer(() -> bucket.async().get("id"))
    .map(document -> {
        document.content().put("modified", new Date().getTime());
        return document;
    })
    .flatMap(doc -> bucket.async().replace(doc))
    .retryWhen(RetryBuilder
      .anyOf(CASMismatchException.class)
      .delay(Delay.fixed(1, TimeUnit.SECONDS))
      .once() //alternatively use max(n) to attempt n times total
      .build())
    .subscribe();]]></codeblock>
  				
  			</sectiondiv>
  			
  		<sectiondiv>
  			<p><b>Upsert</b></p>
  				
  				<p>The <codeph>upsert</codeph> method works similar to <codeph>replace</codeph>, but it also stores the <codeph>Document</codeph> if it does not exist (so there is no <codeph>DocumentDoesNotExistException</codeph> thrown).</p>
  				
  				<p>It also does not use the <codeph>CAS</codeph> value to handle concurrent updates, even when set on the document. Use <codeph>replace</codeph> instead.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonObject content = JsonObject.empty().put("name", "Michael");
JsonDocument doc = JsonDocument.create("docId", content);
JsonDocument inserted = bucket.upsert(doc)]]></codeblock>
  				
  				<p>If the <codeph>Document</codeph> also has the <codeph>expiry</codeph> time set, it will be respected and picked up by the server.</p>
  				
  				<p>It doesn't matter what type of <codeph>Document</codeph> is upserted; its type is inferred
  					from the method argument, and the corresponding <codeph>Transcoder</codeph> is used
  					to encode it.</p>
  				
  				<p>The <codeph>Document</codeph> returned, as a result, is a different one compared to the
  					<codeph>Document</codeph> passed in. It references some values like its
  					<codeph>id</codeph> and <codeph>content</codeph> but also has the
  					<codeph>CAS</codeph> value set.</p></sectiondiv>
  			
  			
  		<sectiondiv>
  				<p><b>Durability Requirements</b></p>
  				
  				<p>If no durability requirements are set on the <codeph>replace</codeph> or <codeph>upsert</codeph> methods, the operation will succeed when the server acknowledges the document in its managed cache layer. While this is a performant operation, there might be situations where you want to make sure that your document has been persisted and/or replicated so that it survives power outages and other node failures.</p>
  				
  				<p>Both methods provide overloads to supply such requirements:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[D replace(D document, PersistTo persistTo);
D replace(D document, ReplicateTo replicateTo);
D replace(D document, PersistTo persistTo, ReplicateTo replicateTo);

D upsert(D document, PersistTo persistTo);
D upsert(D document, ReplicateTo replicateTo);
D upsert(D document, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
  				
  				<p>You can configure either just one or both of the requirements when inserting or upserting. From an application point of view nothing needs to be changed when working with the response, although there is something that need to be kept in mind:</p>
  				
  				<p>The internal implementation first performs a regular <codeph>replace</codeph> or
  					<codeph>upsert</codeph> operation and afterward starts polling the specifically
  					affected cluster nodes for the state of the document. If something fails during this
  					operation (and failing the <codeph>Observable</codeph>), the original operation
  					might have succeeded nonetheless.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Update the document and make sure it is persisted to the master node
bucket.replace(document, PersistTo.MASTER);

// Update the document and make sure it is replicate to one replica node
bucket.replace(document, ReplicateTo.ONE);

// Update the document and make sure it is persisted to one node and replicated to two
bucket.replace(document, PersistTo.ONE, ReplicateTo.TWO);]]></codeblock>
  		</sectiondiv>
  			
  			
  		<sectiondiv>
  				<p><b>Batching</b></p>
  				
  				<p>Because everything is asynchronous internally, batching <codeph>replaces</codeph> or
  					<codeph>upserts</codeph> can be achieved with the <codeph>Observable</codeph>
  					functionality of the <codeph>AsyncBucket</codeph>.</p>
  				
  				<p>A combination of <codeph>just()</codeph> and <codeph>flatMap()</codeph> is used to store them without blocking:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument doc1 = JsonDocument.create("id1", content);
JsonDocument doc2 = JsonDocument.create("id2", content);
JsonDocument doc3 = JsonDocument.create("id3", content);

Observable
    .just(doc1, doc2, doc3)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(JsonDocument document) {
            return bucket.async().replace(document);
        }
    }).subscribe();]]></codeblock>
  				
  		</sectiondiv>		
      
      
    </section>
    
  	
  	<section><title>Retrieving Documents</title>
  		
  	<p>This section describes how to load documents using the various <codeph>get()</codeph>
  			methods.</p>
  	
  			
  		<sectiondiv>
  				<p><b>Regular reads</b></p>
  				
  				<p>To read a document use the <codeph>get()</codeph> method. Either pass in the
  					<codeph>id</codeph> of the <codeph>Document</codeph> or the
  					<codeph>Document</codeph> from which the <codeph>id</codeph> is taken from.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument loadedFromId = bucket.get("id");
JsonDocument loadedFromDoc = bucket.get(JsonDocument.create("id"));]]></codeblock>
  				
  				<p>Both methods have the same effect. The latter method is helpful if you are already
  					dealing with <codeph>Document</codeph> instances in your code and you don't want to
  					extract the <codeph>ID</codeph> out of them on your own.</p>
  				
  				<p>When only the ID is passed in, there is no way to figure out which
  					<codeph>Document</codeph> type should be used, so <codeph>JsonDocument</codeph> is
  					selected as a sensible default. If you want to override this, you can pass in a specific
  					<codeph>Document</codeph> type like this:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[LegacyDocument loaded = bucket.get("legacyId", LegacyDocument.class);]]></codeblock>
  				
  				<p>If the document is not found, the blocking API returns null.:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[JsonDocument found = bucket.get("notexisting");
if (found == null) {
// doc not found
} else {
// doc found
}]]></codeblock>
  				
  				<p>If you are dealing with asynchronous code, an empty <codeph>observable</codeph> is returned
  					instead. This aligns with how <codeph>Observable</codeph> objects are supposed to
  					work by contract, but also makes it easier to deal with the implementation later. If
  					no document is returned, the subsequent operations are just not executed, which
  					avoids having null checks all over the place (if for example a Document would be
  					returned but with the content set to <codeph>null</codeph>).</p>
  				<codeblock outputclass="language-java"><![CDATA[Observable<JsonDocument> updateIfFound = bucket.async().get(potentialNonExistingKey)
	.map(doc -> doc.content())
	.filter(jsonObject -> jsonObject.containsKey("test"))
	.subscribe(
		//onNext only invoked if the key could be retrieved
		data -> System.out.println("Data exists and has test key"),
		error::printStackTrace,
		() -> System.out.println("Completed"));]]></codeblock>
  				
  		</sectiondiv>
  			
  		<sectiondiv>
  				<p><b>Reading from replica</b></p>
  				
  				<p>A regular read always reads the document from its master node. If this node is down or
  					not available, the document cannot be loaded. Reading from replica allows you to load
  					the document from one or more replica nodes instead.</p>
  				
  				<p>
  					<note>When replica reads are used, always use them under the assumption that the data
  						returned is stale. There is no way to guarantee that the data is up-to-date on the
  						replica node unless the proper durability requirements have been set and succeeded on
  						write operations. Only use replica reads if you understand the implications.</note>
  				</p>
  				
  				<p>You can either read the data from one specific replica or all of the available
  					replicas:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Read from all available replicas and the master node and return all responding
bucket.getFromReplica("id", ReplicaMode.ALL);

// Read only from the first replica
bucket.getFromReplica("id", ReplicaMode.FIRST);

// Read only from the second replica
bucket.getFromReplica("id", ReplicaMode.SECOND);

// Read only from the third replica
bucket.getFromReplica("id", ReplicaMode.THIRD);]]></codeblock>
  				
  				<note type="note">If <codeph>ReplicaMode.ALL</codeph> is used, requests are sent to the master
  					node and all configured replicas.</note>
  				
  				<p>The main goal is to get responses back as fast as possible, but because more requests are
  					sent, more responses can arrive. You can use this to either compare all of the
  					responding documents and draw conclusions, or just pick the first one arriving:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[bucket
	.async()
    .getFromReplica("id", ReplicaMode.ALL)
    .first()
    .subscribe();]]></codeblock>
  				
  				<p>In addition, you can add operations to filter based on some of your assumptions. Imagine
  					you have a <codeph>version</codeph> field in your document and you want to only use the
  					replica information if it this specific version:</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[bucket
	.async()
    .getFromReplica("id", ReplicaMode.ALL)
    .filter(document -> document.content().getInt("version") > 5)
    .first()
    .subscribe();]]></codeblock>
  		</sectiondiv>
  			
  		<sectiondiv id="read-and-lock">
  		<p><b>Reading and locking</b></p>
  				
  				<p>Reading and locking works very similar to a regular read, but in addition the
  					<codeph>Document</codeph> is <b>write locked</b> (not read locked) on the server side
  					for the given amount of time.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Get and lock for 10 seconds
JsonDocument doc = bucket.getAndLock("id", 10);]]></codeblock>
  				
  				<p>
  					<note>You can only write lock a document for a maximum of 30 seconds. If an invalid lock
  						time (less than 0 or greater than 30 seconds) is provided, 15 seconds is used as the
  						default.</note>
  				</p>
  				
  				<p>The <codeph>Document</codeph> is unlocked under the following conditions:</p>
  				
  				<ul>
  					<li>The <codeph>unlock()</codeph> command is used.</li>
  					<li>The <codeph>Document</codeph> is replaced with the correct CAS value.</li>
  					<li>30 seconds are over, and the server unlocks it for you.</li>
  				</ul>
  				<p>The following example shows the case with optimistic locking, where the locked document
  					is automatically released on error so it can be used by other clients.</p>
  				<codeblock outputclass="language-java"><![CDATA[
JsonDocument doc = JsonDocument.create(
	"stats",
	JsonObject.empty().put("sold", 0).put("bought", 0)
);
bucket.upsert(doc);

doc = bucket.getAndLock("stats", 20);
try {
    doc.content().put("sold", doc.content().getInt("sold") + 1);
    // Fake a processing error
    if (new Random().nextInt(100) < 30) {
        throw new RuntimeException("processing error");
    }
} catch (RuntimeException ex) {
    bucket.unlock("stats", doc.cas());
}
]]></codeblock>
  		</sectiondiv>
  			
  			<sectiondiv>
  				<p><b>Reading and touching</b></p>
  				
  				<p>Reading and touching works very similar to a regular read, but it also refreshes the
  					expiration time of the document to the specified value.</p>
  				
  				<codeblock outputclass="language-java"><![CDATA[// Get and set the new expiration time to 4 seconds
JsonDocument doc = bucket.getAndTouch("id", 4);]]></codeblock>
  				
  				<p>You can also use the <codeph>touch()</codeph> command if you do not want to read the
  					document and just refresh its expiration time.</p>
  				
  				<note type="important">If you specify an expiration time greater than 30 days in seconds (60
  					seconds * 60 minutes * 24 hours * 30 days = 2,592,000 seconds), it is considered an
  					absolute timestamp instead of a relative one.</note>
  				
  			</sectiondiv>
      
      
    </section>
  	
  	
  	
  	
  	
  	
  	
  	
  	<section><title>Deleting Documents</title> This section describes how to delete documents using
			the <codeph>remove()</codeph> method. <sectiondiv>
				<p><b>Removing</b></p>
				<p>You can remove a document by utilizing the <codeph>remove()</codeph> method.</p>
				<codeblock outputclass="language-java"><![CDATA[// Remove the document by its ID.
JsonDocument doc = bucket.remove("id");]]></codeblock>
				<p>If you pass in a document which also has the CAS value populated, the SDK will
					make sure to only delete the document if they match:</p>
				<codeblock outputclass="language-java"><![CDATA[// Create document with some content
JsonDocument stored = bucket.upsert(JsonDocument.create("mydoc", JsonObject.create()));

// Delete it with the CAS check included
JsonDocument removed = bucket.remove(stored);]]></codeblock>
				<p>If successful, the returned <codeph>Document</codeph> has the <codeph>id</codeph>
					and <codeph>cas</codeph> fields populated, all other fields are set to their
					default values.</p>
				<p>If the document is not found, a <codeph>DocumentNotFoundException</codeph> is
					raised. When the document is found but the <codeph>cas</codeph> values do not
					match, a <codeph>CASMismatchException</codeph> is raised.</p>
			</sectiondiv><sectiondiv>
				<p><b>Durability Requirements</b></p>
				<p>If no durability requirements are set on the <codeph>remove</codeph> method, the
					operation will succeed when the server acknowledges the document delete in its
					managed cache layer. While this is a performant operation, there might be
					situations where you want to make sure that your document deletion has been
					persisted or replicated so that it survives power outages and other node
					failures.</p>
				<p>The remove method provides overloads to supply such requirements:</p>
				<codeblock outputclass="language-java"><![CDATA[JsonDocument remove(String id, PersistTo persistTo);
JsonDocument remove(String id, ReplicateTo replicateTo);
JsonDocument remove(String id, PersistTo persistTo, ReplicateTo replicateTo);

D remove(D document, PersistTo persistTo);
D remove(D document, ReplicateTo replicateTo);
D remove(D document, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
				<p>You can configure either just one or both of the requirements when removing. From
					an application point of view nothing needs to be changed when working with the
					response, although there is something that need to be kept in mind:</p>
				<p>The internal implementation first performs a regular <codeph>remove</codeph>
					operation and afterwards starts polling the specifically affected cluster nodes
					for the state of the document. If something fails during this operation (and
					failing the <codeph>Observable</codeph>), the original operation might have
					succeeded nonetheless.</p>
				<codeblock outputclass="language-java"><![CDATA[// Remove the document and make sure the delete is persisted.
JsonDocument doc = bucket.remove("id", PersistTo.MASTER);

// Remove the document and make sure the delete is replicated.
JsonDocument doc = bucket.remove("id", ReplicateTo.ONE);

// Remove the document and make sure the delete is persisted and replicated.
JsonDocument doc = bucket.remove("id", PersistTo.MASTER, ReplicateTo.ONE);]]> </codeblock>
			</sectiondiv></section>
  		
  		
  		<section><title>Atomic Operations</title>
  			<p>The <codeph>CouchbaseBucket</codeph> class provides atomic operations such as
  				<codeph>counter()</codeph>, <codeph>append()</codeph>, and <codeph>prepend()</codeph>.</p>
  			<sectiondiv>
  					<p><b>Counter</b></p>
  					
  					
  					<p>The <codeph>counter()</codeph> method allows you to increment or decrement a document with
  						a numerical content atomically. The method only accepts and returns a
  						<codeph>JsonLongDocument</codeph>. The value stored in the document is incremented or
  						decremented depending on the given <codeph>delta</codeph>: if the delta value is a positive
  						number, the value is incremented, and if it is a negative number, the value is decremented.
  						You can also pass in an initial value and an expiration time.</p>
  					
  					
  					<codeblock outputclass="language-java"><![CDATA[// Increase the counter by 5 and set the initial value to 0 if it does not exist
JsonLongDocument doc = bucket.counter("id", 5, 0);]]></codeblock>
  					
  					<p>The resulting document contains the new counter value. A very common use case is to implement
  						an increasing <codeph>AUTO_INCREMENT</codeph> like counter, where every new user just
  						gets a new ID (here using the asynchronous API):</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[bucket.async()
    .counter("user::id", 1, 1)
    .map(new Func1<JsonLongDocument, String>() {
        @Override
        public String call(JsonLongDocument counter) {
            return "user::" + counter.content();
        }
    })
    .flatMap(new Func1<String, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(String id) {
            return bucket.insert(JsonDocument.create(id, JsonObject.empty()));
        }
    }).subscribe();]]></codeblock>
  					
  					
  					<p>This code increases the counter by one, and then maps the returned number onto a custom
  						document ID (here the code prefixes <codeph>user::</codeph>). Afterward, the
  						<codeph>insert</codeph> method is executed with the generated ID and an empty document
  						content. Because a <codeph>counter</codeph> operation is atomic, the code is guaranteed to
  						deliver different user IDs, even when called at the same time from multiple threads.</p>
  					
  					<p>The counter always needs to be
  						greater than or equal to zero because negative values are not allowed. If you want to
  						decrement a counter, make sure to set it to a value greater than zero initially.</p>
  					
  					
  					
  					<p>If the initial value is omitted and the counter doesn't exists, this is signaled to the
  						user by propagating a <codeph>DocumentDoesNotExistException</codeph> (since 2.2.0). You can
  						avoid that by providing an explicit initial value, which could be the same as the delta or
  						even an arbitrary initial value (the delta won't be added to it at counter creation):</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[// Increase the counter by 5 or create the counter with a value of 4 if it does not exist
JsonLongDocument doc = bucket.counter("id", 5, 4);]]></codeblock>
  					
  					<p>If you want to set an expiration time, you need to provide both the initial value and the
  						expiration time. This constraint is imposed by the API because just exposing the expiration
  						time would be ambiguous with the initial value (<codeph>long</codeph> and
  						<codeph>int</codeph>).</p>
  					
  					
  					<codeblock outputclass="language-java"><![CDATA[// Increment by 5, initial 5 and 3 second expiration
JsonLongDocument doc = bucket.counter("id", 5, 5, 3);]]></codeblock>
  					
  			</sectiondiv>
  				
  		<sectiondiv>	
  				<p><b>Append &amp; Prepend</b></p>
  					
  					<p>Appending and prepending values to existing documents is also possible. Both the
  						<codeph>append</codeph> and <codeph>prepend</codeph> operation are atomic so that they can
  						be used without further synchronization.</p>
  					
  					
  					<p>
  						<note>Both operations only work on binary documents, ideally strings or byte arrays. It does not
  							work on JSON documents because it doesn't do any further inspection. Applying
  							one of those operations on a JSON document will render it invalid.</note>
  					</p>
  					
  					<p>A <codeph>Document</codeph> needs to be created before values can be appended or prepended.
  						Here is an example that creates a document and then appends a string to it:</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[bucket
    .insert(LegacyDocument.create("doc", "Hello, "))
    .flatMap(doc ->
        bucket.append(LegacyDocument.create("doc", "World!"))
    )
    .flatMap(bucket::get)
    .toBlocking()
    .forEach(doc -> System.err.println(doc.content()));]]></codeblock>
  					
  					<p>When executed, this code prints <codeph>Hello, World!</codeph>.</p>
  		</sectiondiv>
  				
  			<sectiondiv>	
  					<p><b>Durability Requirements</b></p>
  					
  					
  					<p>If no durability requirements are set on the <codeph>append</codeph>, <codeph>prepend</codeph>
  						or <codeph>counter</codeph> methods, the operation will succeed when the server
  						acknowledges the document in its managed cache layer. While this is a performant
  						operation, there might be situations where you want to make sure that your document
  						has been persisted or replicated so that it survives power outages and other node
  						failures.</p>
  					
  					
  					<p>All atomic operations provide overloads to supply such durability requirements:</p>
  					
  					<codeblock outputclass="language-java"><![CDATA[D append(D document, PersistTo persistTo);
D append(D document, ReplicateTo replicateTo);
D append(D document, PersistTo persistTo, ReplicateTo replicateTo);

D prepend(D document, PersistTo persistTo);
D prepend(D document, ReplicateTo replicateTo);
D prepend(D document, PersistTo persistTo, ReplicateTo replicateTo);

JsonLongDocument counter(String id, long delta, PersistTo persistTo);
JsonLongDocument counter(String id, long delta, ReplicateTo replicateTo);
JsonLongDocument counter(String id, long delta, PersistTo persistTo, ReplicateTo replicateTo);

JsonLongDocument counter(String id, long delta, long initial, PersistTo persistTo);
JsonLongDocument counter(String id, long delta, long initial, ReplicateTo replicateTo);
JsonLongDocument counter(String id, long delta, long initial, PersistTo persistTo, ReplicateTo replicateTo);

JsonLongDocument counter(String id, long delta, long initial, int expiry, PersistTo persistTo);
JsonLongDocument counter(String id, long delta, long initial, int expiry, ReplicateTo replicateTo);
JsonLongDocument counter(String id, long delta, long initial, int expiry, PersistTo persistTo, ReplicateTo replicateTo);]]></codeblock>
  					
  					<p>You can configure either just one or both of the requirements. From an application point of view nothing needs to be
  						changed when working with the response, although there is something that need to be kept in mind:</p>
  					
  					<p>The internal implementation first performs a regular operation and afterward starts polling
  						the specifically affected cluster nodes for the state of the document. If something
  						fails during this operation (and failing the <codeph>observable</codeph>), the
  						original operation might have succeeded nonetheless.</p>
  					
  			</sectiondiv>
  			
  			
  		</section>		
  		


  	
  	<section><title>Bulk Operations</title> Bulk operations allow you to operate on more than one
			document at the same time. <sectiondiv>
				<p><b>Introduction</b></p>
				<p>To get better resource utilization, you need to perform all types of operations
					in batches. Because of the asynchronous nature of the underlying core package,
					you can utilize RxJava's operations to provide implicit batching facilities
					combined with the asynchronous operations of the SDK.</p>
				<p>If you understand the general approach to batching, you can apply it to any
					operation against the SDK, not just with <codeph>get()</codeph> calls like in
					the 1.x series SDK.</p>
			</sectiondiv><sectiondiv>
				<p><b>Batching with RxJava</b></p>
				<p>Implicit batching is performed by utilizing a few operators: <ul>
						<li><codeph>Observable.just()</codeph> or <codeph>Observable.from()</codeph>
							to generate an <codeph>observable</codeph> that contains the data you
							want to batch on.</li>
						<li><codeph>flatMap()</codeph> to send those events against the Couchbase
							Java SDK and merge the results asynchronously.</li>
						<li><codeph>last()</codeph> if you want to wait until the last element of
							the batch is received.</li>
						<li><codeph>toList()</codeph> if you care about the responses and want to
							aggregate them easily.</li>
						<li>If you have more than one subscriber, using <codeph>cache()</codeph> to
							prevent accessing the network over and over again with every
							subscribe.</li>
					</ul></p>
				<p>The following example creates an observable stream of 5 keys to load in a batch,
					asynchronously fires off <codeph>get()</codeph> requests against the SDK, waits
					until the last result has arrived, and then converts the result into a list and
					blocks at the very end:</p>
				<codeblock outputclass="language-java"><![CDATA[Cluster cluster = CouchbaseCluster.create();
Bucket bucket = cluster.openBucket();

List<JsonDocument> foundDocs = Observable
    .just("key1", "key2", "key3", "key4", "key5")
    .flatMap(new Func1<String, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(String id) {
            return bucket.async().get(id);
        }
    })
    .toList()
    .toBlocking()
    .single();]]></codeblock>
				<p>Note that this always returns a list, but it may contain 0 to 5 documents,
					depending on how many are actually found. Also, at the very end the observable
					is converted into a blocking one, but everything before that, including the
					network calls and the aggregation, is happening completely asynchronously.</p>
				<p>Inside the SDK, this provides much more efficient resource utilization because
					the requests are very quickly stored in the internal <codeph>Request
						RingBuffer</codeph> and the I/O threads are able to pick batches as large as
					they can. Afterward, whatever server returns a result first it is stored in the
					list, so there is no serialization of responses going on.</p>
				<p>If you wrap the code in a helper method, you can provide very nice encapsulated
					batching semantics:</p>
				<codeblock outputclass="language-java"><![CDATA[public List<JsonDocument> bulkGet(final Collection<String> ids) {
    return Observable
        .from(ids)
        .flatMap(new Func1<String, Observable<JsonDocument>>() {
            @Override
            public Observable<JsonDocument> call(String id) {
                return bucket.async().get(id);
            }
        })
        .toList()
        .toBlocking()
        .single();
}]]></codeblock>
			</sectiondiv>
			<sectiondiv>
				<p><b>Batching mutations</b></p>
				<p>The previous Java SDK only provided bulk operations for <codeph>get()</codeph>.
					With the techniques shown above, you can perform any kind of operation as a
					batch operation.</p>
				<p>The following code generates a number of fake documents and inserts them in one
					batch. Note that you can decide to either collect the results with
						<codeph>toList()</codeph> as shown above or just use <codeph>last()</codeph>
					as shown here to wait until the last document is properly inserted.</p>
				<codeblock outputclass="language-java"><![CDATA[// Generate a number of dummy JSON documents
int docsToCreate = 100;
List<JsonDocument> documents = new ArrayList<JsonDocument>();
for (int i = 0; i < docsToCreate; i++) {
    JsonObject content = JsonObject.create()
        .put("counter", i)
        .put("name", "Foo Bar");
    documents.add(JsonDocument.create("doc-"+i, content));
}

// Insert them in one batch, waiting until the last one is done.
Observable
    .from(documents)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(final JsonDocument docToInsert) {
            return bucket.async().insert(docToInsert);
        }
    })
    .last()
    .toBlocking()
    .single();]]></codeblock>
			</sectiondiv>
			<sectiondiv>
				<p><b>Performance</b></p>
				<p>Here are two code samples, both synchronous, that showcase serialized and batched
					loading of documents. Note that more important than the absolute operations per
					second is the relative improvement with the same workload.</p>
				<codeblock outputclass="language-java"><![CDATA[// Serialized workload of loading documents
while(true) {
    List<JsonDocument> loaded = new ArrayList<JsonDocument>();
    int docsToLoad = 10;
    for (int i = 0; i < docsToLoad; i++) {
        JsonDocument doc = bucket.get("doc-" + i);
        if (doc != null) {
            loaded.add(doc);
        }
    }
}]]></codeblock>
				<p>
					<image href="images/batching-single.png" id="image_uht_xb3_yv"/></p>
				<codeblock outputclass="language-java"><![CDATA[// Same workload, utilizing batching effects
while(true) {
    int docsToLoad = 10;
    Observable
        .range(0, docsToLoad)
        .flatMap(new Func1<Integer, Observable<JsonDocument>>() {
            @Override
            public Observable<JsonDocument> call(Integer i) {
                return bucket.async().get("doc-"+i);
            }
        })
        .toList()
        .toBlocking()
        .single();
        
}]]></codeblock>
				<p>
					<image href="images/batching-bulk.png" id="image_batch_bulk"/>
				</p>
			</sectiondiv>
			<sectiondiv>
				<p><b>Error Handling &amp; Recovery</b></p>
				<p>Technically speaking, error handling in bulk operations is similar to generic
						<codeph>Observable</codeph> error handling, but because the topic is
					strongly related the most important concepts are covered here as well.</p>
				<p>In general, the following questions come up:</p>
				<ul>
					<li>How can I implement best effort loading and just return the values that were
						successful?</li>
					<li>What are <codeph>BackpressureExceptions</codeph> and how can I handle
						them?</li>
					<li>How can I retry individual operations in the batch when they fail?</li>
				</ul>
				<p>When handling these situations, an important fact to remember is that as soon as
					an error happens inside an <codeph>observable</codeph>, the whole thing is
					terminated. If you want the whole stream to complete, error handling needs to be
					as close as possible to the original source. Let's take the bulk loading of
					documents as an example which we are going to modify to be more resilient:</p>
				<codeblock outputclass="language-java"><![CDATA[Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id);
    })
    .subscribe();]]></codeblock>
				<p>To implement the best effort use case, you can ignore all errors on each
						<codeph>get()</codeph> result before it gets merged and flattened into the
					original stream. It is strongly recommended to log the error, because otherwise
					you'll never know what went wrong in the first place for each failing
					operation.</p>
				<codeblock outputclass="language-java"><![CDATA[Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id)
            .doOnError(System.err::println) // print the error, log,...
            .onErrorResumeNext(Observable.empty()); // on error resume with an empty sequence
    })
    .subscribe();]]></codeblock>
				<p>There is a slight variation to that which you can use instead. RxJava provides a
						<xref href="http://reactivex.io/documentation/operators/merge.html"
						format="html" scope="external">mergeDelayError</xref> operator that merges
					individual observables, emits all items and then at the very end fails the
					observable with a <codeph>CompositeException</codeph>. This composite exception
					contains all errors that have happened so you can do something with them at a
					later point.</p>
				<p>Very often you want a complete result and therefore you need to retry individual
					operations if an error happened. It is recommended to retry based on a defined
					strategy for specific exception types and propagate the error for unknown
					exceptions or those types which are known to be permanent. For a full list of
					errors that can happen and their implications, see the Javadoc API reference for
					the <codeph>Bucket</codeph> methods you are using.</p>
				<p>Since the <codeph>BackpressureException</codeph> has been frequently referenced
					in the past, we are going to use that one as an example. The same logic of
					course applies to all other types as well.</p>
				<p>The <codeph>BackpressureException</codeph> is used to shed load on the request
					side and fails your Observable quickly if the underlying system is in an
					overload condition. The reason for this is that somehow requests are produced
					more quickly than responses can be generated (because that includes the actual
					network round trip). This is common in bulk scenarios since it could be that you
					are requesting a very large set of documents at the same time which puts
					temporary pressure on the client.</p>
				<p>To solve that, we can apply a delayed retry algorithm onto the Observable so it
					is retried at a later point. We are making use of the <codeph>Delay</codeph>
					construct shipped with the 2.1 SDK which provides a very convenient way to
					generated increasing delays. You also want to stop retrying at some point so the
					operation is not retried forever.</p>
				<p>Since 2.1.2, the <codeph>RetryBuilder</codeph> API has been introduced to help
					you build retry scenarios. The following code retries with an exponential
					backoff (with a 100 millisecond ceiling), but stops after 10 attempts and
					propagates the error.</p>
				<codeblock outputclass="language-java"><![CDATA[
Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id)
            .retryWhen(RetryBuilder
                .anyOf(BackpressureException.class)
                .delay(Delay.exponential(TimeUnit.MILLISECONDS, 100))
                .max(10)
                .build()
            );
    })
    .subscribe();]]></codeblock>
				<p>For reference, this is how you would have written the retry feature prior to
					2.1.2:</p>
				<codeblock outputclass="language-java"><![CDATA[final Delay delay = Delay.exponential(TimeUnit.MILLISECONDS, 100);
  					
Observable
    .from(docIds)
    .flatMap(id -> {
        return bucket
            .async()
            .get(id)
            .retryWhen(notification ->
                notification
                    .zipWith(Observable.range(1, 11), Tuple::create)
                    .flatMap(att ->
                        att.value2() == 9 || !(att.value1() instanceof BackpressureException)
                            ? Observable.error(att.value1())
                            : Observable.timer(delay.calculate(att.value2()), delay.unit())
                    )
            );
    })
    .subscribe();]]></codeblock>
				<p>This code zips the error with a range that indicates the number of attempts we
					want to retry. If this is over 10 attempts or the error is not a backpressure
					exception, the error will be propagated.</p>
				<p>Finally, you always want to chain in <codeph>timeout()</codeph> calls so you have
					a last resort error and you can be sure that the code you're relying on isn't
					stuck forever. You can also use methods like <codeph>onErrorReturn()</codeph> to
					return a stubbed object or a fixed entity that you know will never fail (so in
					the worst case you can provide a reduced user experience instead of failing
					completely).</p>
			</sectiondiv></section>
  	
  	
  	
  	
  	<section id="durability"><title>Durability Requirements</title>Specify durability requirements to
			regulate Couchbase Server's behavior. <p>If you do not specify any durability
				requirements, the server will respond with a success message if the data has been
				acknowledged and processed in the managed cache. Since persistence and replication
				are asynchronous tasks and happen eventually, there is a time gap where a node
				failure can lead to data loss. This time gap is exactly between the points when data
				has neither been replicated to another node nor persisted to disk.</p>
			<p>For most use cases, this time gap is totally acceptable since it is usually very
				small. For the important mutation operations, we need stronger guarantees. If you
				change the server to acknowledge only once the data has been either replicated or
				persisted makes the whole system slower by default. Therefore, Couchbase Server has
				another way to handle such situations:</p>
			<ol>
				<li>The SDK will perform the normal mutation without durability requirements.</li>
				<li>If the server returns with a successful response, the client will start
					polling.</li>
				<li>The client polls all the affected nodes for this mutation until either the
					desired state is reached, or it can’t be reached for a reason.</li>
				<li>In the successful case, the operation will also complete towards the application
					layer, and in the failure case the client will error out the operation, leaving
					the user to decide what's next. </li>
			</ol><p>The following code will make sure that the document has been persisted on the
				active node for this document ID and also replicated to one of the configured
				replicas:
				</p><codeblock outputclass="language-java">JsonDocument docPersistedAndReplicated = bucket.upsert(docToStore, PersistTo.MASTER, ReplicateTo.ONE);</codeblock><p>In
				this example, the client will poll two nodes until completion: the active node for
				this document and also the configured replica. If any of the constraints cannot be
				fulfilled, an exception will be raised. </p><p>If you wonder why in the failure case
				we put the burden on you to figure out what next: the SDK has no idea of your SLAs
				or what you intend to do to when the operation fails. Sometimes it might be fine to
				proceed and log the error, in other cases you may want sophisticated retry
				mechanisms where the SDK can guide you with functionality, but not the actual
				execution semantics. </p><sectiondiv>
				<p><b><codeph>PersistTo</codeph> and <codeph>ReplicateTo</codeph></b></p>
				<p>The way to specify your intent to define durability requirements is by passing in
						<codeph>PersistTo</codeph> or <codeph>ReplicateTo</codeph> enums as
					parameters on mutation operations.</p>
				<p>The following states can be set, and all of the combinations between the two are
					supported. Note that the <codeph>NONE</codeph> states are the defaults and, in
					this case, the durability requirement won’t be used. The <codeph>NONE</codeph>
					state is useful nonetheless if you define it based on a system property and need
					a fallback to the default.</p>
				<ul>
					<li><codeph>PersistTo:</codeph> NONE, MASTER, ONE, TWO, THREE, FOUR</li>
					<li><codeph>ReplicateTo:</codeph> NONE, ONE, TWO, THREE</li>
				</ul>
				<p>Since you can have a maximum of three replicas, the maximum number of replicas
					you can check is three as well. For persisting the maximum number is four,
					because the active node can be checked in addition to the three replicas.</p>
				<p>All options (other than NONE and MASTER) will return as soon as the requirement
					is fulfilled. So for example if you have two replicas configured on the bucket,
					and you specify <codeph>ReplicateTo.ONE</codeph>, as soon as either one of the
					two reports success for the document ID the condition is assumed to be
					fulfilled.</p>
				<p>If you provide both <codeph>PersistTo</codeph> and <codeph>ReplicateTo</codeph>
					both conditions are linked together with a logical “and”, meaning that both
					conditions must be satisfied so that the SDK can report success.</p>
				<p>Keep in mind that if the durability requirement failed, it could very well be
					that the original mutation operation succeeded. See the common failure scenarios
					for more context. This is due to the effect that the actual mutation and the
					subsequent polling are individual operations that can also fail individually. As
					an example, if you specify <codeph>ReplicateTo.ONE</codeph> and you have no
					replica configured, the original mutation will complete without issues, but the
					durability requirement will fail.</p>
			</sectiondiv><sectiondiv>
				<p><b>API support</b></p>
				<p>The following mutation operations provide support for durability
					requirements:</p>
				<ul>
					<li><codeph>insert</codeph></li>
					<li><codeph>upsert</codeph></li>
					<li><codeph>replace</codeph></li>
					<li><codeph>remove</codeph></li>
				</ul>
				<p>Here is an example that inserts a document and waits until it is persisted on the
					active node:</p>
				<codeblock outputclass="language-java">bucket.insert(document, PersistTo.MASTER);</codeblock>
				<p>The following example replaces a document and waits until it is replicated to at
					least one replica:</p>
				<codeblock outputclass="language-java">bucket.replace(document, ReplicateTo.ONE);</codeblock>
				<p>While not obvious in the first place, the same holds true for removal operations.
					If you want to make sure that the deletion of the document survives node
					failures under any conditions, you can use the same semantics. The following
					example makes sure that the removal is persisted on at least two nodes in the
					cluster (like the master and one replica):</p>
				<codeblock>bucket.remove("docid", PersistTo.TWO);</codeblock>
				<p>In this last example also specifying <codeph>ReplicateTo.ONE</codeph> would be
					redundant, since persisting on a replica means it first needs to be replicated
					to it. </p>
			</sectiondiv><sectiondiv>
				<p><b>Exceptions and errors</b></p>
				<p>The asynchronous API will send error notifications to your subscriber in the
					failure case. Since the synchronous API just wraps the asynchronous one, those
					errors will be converted into actual exceptions.</p>
				<p>Every exception that happens during the durability requirement polling will be
					wrapped into a
						<codeph>com.couchbase.client.java.error.DurabilityException</codeph>. The
					original exception is carried as part of the
						<codeph>DurabilityException</codeph>, so you can always call
						<codeph>Throwable#getCause()</codeph> on it.</p>
				<p>After you do that, you will come across the following errors:</p>
				<ul>
					<li><codeph>DocumentConcurrentlyModifiedException</codeph>: If the observed
						document has been concurrently modified by another caller.</li>
					<li><codeph>ReplicaNotConfiguredException</codeph>: There are not enough
						replicas configured to fulfill the durability requirement in the first
						place.</li>
					<li><codeph>ReplicaNotAvailableException</codeph>: There are currently not
						enough replicas in the cluster to fulfill the durability requirement right
						now.</li>
					<li><codeph>DocumentMutationLostException</codeph>: The mutation has been lost
						during a hard failover. Only applies to enhanced durability.</li>
				</ul>
				<p>These errors can happen for a subset of the documents, depending on which node
					has been failed over, and no replica is available at this point. Other errors
					will affect all documents, for example if no replica is configured on the bucket
					at all.</p>
				<p>Keep in mind that because the polling for durability happens after the original
					mutation, every failure on that one will propagate immediately and never trigger
					the polling in the first place. This means that if you specify durability
					requirements, you need to handle those errors in addition to the errors on the
					original mutation. </p>
			</sectiondiv><sectiondiv>
				<p><b>Failure modes and their impact</b></p>
				<p>An advanced feature of the SDK are different failure modes, but in the context of
					durability requirements they become more important. By default, the SDK uses
						the<codeph>BestEffortRetryStrategy</codeph>, but you can plug in a different
					one, such as the <codeph>FailFastRetryStrategy</codeph>) or write your mode. If
					you want to change it, you can do it in the environment like this:</p>
				<codeblock outputclass="language-java">CouchbaseEnvironment env = DefaultCouchbaseEnvironment
    .builder()
    .retryStrategy(FailFastRetryStrategy.INSTANCE)
    .build();</codeblock>
				<p>Keep in mind that since this is a global setting, all operations will be affected
					by it. Now, what does it change in the context of durability requirements?</p>
				<p>If the SDK detects an issue during the polling activity, it will either continue
					to retry (best effort) or bail out immediately (fail fast). If a node goes down
					and does not come back up quickly enough using the best effort strategy, you’ll
					hit the specified client side timeout. With the fail fast strategy, you’ll very
					quickly get a DurabilityException that contains the root cause, such as a
						<codeph>RequestCancelledException</codeph>.</p>
				<p>If you use the fail fast retry strategy, you are trading more complex retry code
					on your side for faster feedback cycles in the failure case. A common reason to
					enable fail fast strategy is if you use some more sophisticated libraries on top
					of the SDK, for example <xref href="https://github.com/Netflix/Hystrix"
						format="html" scope="external">Hystrix</xref></p>
			</sectiondiv><sectiondiv>
				<p><b>Common failure scenarios</b></p>
				<p>Durability constraints in general span more than one node and the statistical
					chance for operation failures is higher. As a result, it is even more important
					for production stability to consider what can go wrong and in what ways to
					react.</p>
				<p>This section focuses on failures that come up because of invalid cluster
					setup.</p>
				<p>Lots of durability failures occur because the bucket doesn’t have enough replicas
					configured, or the number of nodes is not sufficient enough. For example, there
					are two replicas configured, but there are only two nodes in the cluster.</p>
				<p>If you have one replica configured on the bucket but you issue a mutation with
						<codeph>ReplicateTo.TWO</codeph>, you’ll get the following error:</p>
				<codeblock>Exception in thread "main" com.couchbase.client.java.error.DurabilityException: 
				Durability requirement failed: Not enough replicas configured on the bucket.
    at com.couchbase.client.java.CouchbaseAsyncBucket$18$1.call(CouchbaseAsyncBucket.java:549)
    at com.couchbase.client.java.CouchbaseAsyncBucket$18$1.call(CouchbaseAsyncBucket.java:545)
    at rx.internal.operators.OperatorOnErrorResumeNextViaFunction$1.onError(OperatorOnErrorResumeNextViaFunction.java:99)
    ...
Caused by: com.couchbase.client.core.ReplicaNotConfiguredException: Not enough replicas configured on the bucket.
    at com.couchbase.client.core.message.observe.ObserveViaCAS$6$2.call(ObserveViaCAS.java:144)
    at com.couchbase.client.core.message.observe.ObserveViaCAS$6$2.call(ObserveViaCAS.java:136)
    at rx.internal.operators.OperatorMap$1.onNext(OperatorMap.java:55)
</codeblock>
				<p>When you have a replica configured on the bucket, but not enough nodes in the
					cluster to replicate the data, the behavior varies on the retry strategy. By
					default (best effort), you’ll get a timeout because the client tries as long as
					possible (since the node can be added to the cluster at runtime):</p>
				<codeblock>Exception in thread "main" java.lang.RuntimeException: java.util.concurrent.TimeoutException
    at com.couchbase.client.java.util.Blocking.blockForSingle(Blocking.java:75)
    at com.couchbase.client.java.CouchbaseBucket.upsert(CouchbaseBucket.java:375)
    at com.couchbase.client.java.CouchbaseBucket.upsert(CouchbaseBucket.java:370)
    at test.MainTest.main(MainTest.java:30)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
Caused by: java.util.concurrent.TimeoutException
    ... 9 more
</codeblock>
				<p>If you choose fail fast, the error will be a <codeph>DurabilityException</codeph>
					which contains a <codeph>ReplicaNotAvailableException</codeph>:</p>
				<codeblock>Exception in thread "main" com.couchbase.client.java.error.DurabilityException: 
				Durability requirement failed: Replica number 1 not available for bucket default
    at com.couchbase.client.java.CouchbaseAsyncBucket$18$1.call(CouchbaseAsyncBucket.java:549)
    at com.couchbase.client.java.CouchbaseAsyncBucket$18$1.call(CouchbaseAsyncBucket.java:545)
    at rx.internal.operators.OperatorOnErrorResumeNextViaFunction$1.onError(OperatorOnErrorResumeNextViaFunction.java:99)
    at rx.internal.operators.OperatorMap$1.onError(OperatorMap.java:49)
    at rx.internal.operators.OperatorMap$1.onError(OperatorMap.java:49)
    at rx.internal.operators.OperatorTake$1.onError(OperatorTake.java:62)
    ...
Caused by: com.couchbase.client.core.ReplicaNotAvailableException: Replica number 1 not available for bucket default
    at com.couchbase.client.core.node.locate.KeyValueLocator.errorObservables(KeyValueLocator.java:202)
    at com.couchbase.client.core.node.locate.KeyValueLocator.locateForCouchbaseBucket(KeyValueLocator.java:127)
    at com.couchbase.client.core.node.locate.KeyValueLocator.locate(KeyValueLocator.java:79)
    ...
</codeblock>
				<p>Note the subtle difference between a
						<codeph>ReplicaNotConfiugredException</codeph> and a
						<codeph>ReplicaNotAvailableException</codeph>. This difference allows you to
					troubleshoot your cluster effectively if it is not set up properly or if it is
					suffering from a temporary failure condition.</p>
			</sectiondiv>
			<sectiondiv><p><b>Understanding and handling node failures</b></p><p>In addition to
					wrong cluster setup, the most common issue with durability requirements is when
					the cluster topology changes at runtime in a way that makes it not possible to
					fulfill the requirements anymore.</p><p>In practice, this is commonly referred
					to as a “node failure”and can happen because the
						<codeph>couchbase-server</codeph> service is restarted, the node is powered
					down or it is not reachable across the network. In all these cases, operations
					will start to fail. The failure is caused either because the active partitions
					do not respond, such as when the normal mutation fails or the replicas do not
					respond.</p>If you use the best effort strategy, you will run into your
				operation timeouts. In the fast fail case, you’ll see the
					<codeph>DurabilityExceptions</codeph> with the different underlying
					causes.<p>The difference between strategies is that only a subset of the
					operations will fail. If you have a 5-node cluster, only 1/5th of the mutations
					will fail, and the durability requirement will only fail if the down node
					involved is the only one that can help fulfilling. If you have two replicas
					configured, one node is down, and you use <codeph>ReplicateTo.ONE</codeph>, the
					other replica can still help make the requirement succeed.</p><p>Once you
					manually trigger the node failover or the auto-failover feature kicks in, the
					replicas are promoted to active, and the regular mutations on those partitions
					will start to work again. Here is the catch: since replicas had been promoted to
					active a few seconds ago, those replicas are now gone.</p>You will either have
				more than one replica configured when there is a spare one sitting in the cluster
				answering the durability requirement requests, or you’ll still get
					<codeph>ReplicaNotAvailableExceptions</codeph>.<p>Once you click on the
					Rebalance button, the cluster manager makes sure that enough replicas are again
					in place and the data is shuffled around. At some point,
						the<codeph>ReplicaNotAvailableExceptions</codeph> are gone, and everything
					is good again.</p>What you have to do when you hit such an exception depends
				very much of the application type. If you need to make sure the data is stored and
				you don’t mind waiting for the cluster to settle again, you can retry the operation
				with an exponential backoff. If you are running an interactive application, most of
				the time you need to propagate the error up the stack or fail in a controlled
					way.<p>While you should plan for node failures, they don’t happen every hour. If
					you plan accordingly, you can minimize production impact. Multiple replicas
					strategies, which use the enterprise edition rack-awareness feature and the
					transparent, enhanced durability, provide a strong foundation to build always-on
					services even in the failure case.</p>
			</sectiondiv><sectiondiv>
				<p><b>Handling concurrent document modifications</b></p>
				<p>One error you might encounter in a concurrent application, even if all nodes are
					up and running, is the <codeph>DocumentConcurrentlyModifiedException</codeph>.
					To understand its roots, we need to peel back a layer and see how the polling
					mechanism works.</p>
				<p>A successful mutation returns the new CAS value as part of the response. Because
					the CAS value changes all the time when the document changes, we can use it to
					track the replication and persistence on the server side. So here is roughly
					what happens inside the SDK.</p>
				<ol>
					<li>The mutation is performed.</li>
					<li>If successful, the document ID is sent to each participating server through
						the internal <codeph>observe</codeph> command.</li>
					<li>All responses are collected. They contain a status (persisted, replicated)
						and the CAS of the document on each server.</li>
					<li>On the master (active) node, if the CAS value is different from the one of
						our mutation, a <codeph>DocumentConcurrentlyModifiedException</codeph> is
						raised.</li>
				</ol>
				<p>Why?</p>
				<p>Imagine that the application A mutates document D and gets a CAS returned. Then
					it starts polling. Slightly afterward, application B also mutates document D and
					the CAS value changes again. The managed cache on the server performs
					deduplication, or the document can be replicated between poll cycles. Therefore,
					the CAS returned to application A might never be observed since it already
					changed to the CAS from application B.</p>
				<p>To detect this scenario, once the CAS on the master changes the SDK will raise a
						<codeph>DocumentConcurrentlyModifiedException</codeph>. How to react depends
					on the nature of the application. If from the semantics the latter operation
					subsumes the previous one, ignoring the error might be acceptable. If not,
					application A needs to fetch the document again, perform its changes or raise an
					error. In general, error handling is very similar to a
						<codeph>CASMismatchException</codeph>.</p>
			</sectiondiv>
			<sectiondiv>
				<p><b>Enhanced durability requirements with 4.0+</b></p>
				<p>The “next generation observe” removes the possibility of a
						<codeph>DocumentConcurrentlyModifiedException</codeph> happening.</p>
				<p>Couchbase Server 4.0 introduces a new feature that allows the SDK to be more
					accurate during the observe poll cycle, especially in the concurrent and
					failover cases. Instead of using the CAS to verify mutations, it uses sequence
					numbers and partition UUIDs.</p>
				<p>To enable them, all you need to do is enable mutation tokens on the
					environment:</p>
				<codeblock outputclass="language-java">CouchbaseEnvironment env = DefaultCouchbaseEnvironment
    .builder()
    .mutationTokensEnabled(true)
    .build();</codeblock>
				<p>The tradeoff here is an extra 16 byte overhead on every mutation. Every mutation
					returns the partition UUID and the sequence number which are then used for the
					enhanced durability requirements.</p>
				<p>As a result, the new <codeph>MutationToken</codeph> on the
						<codeph>Document</codeph> will be set and, as a result, the polling logic
					will automatically fall back to the enhanced requirements. Only enable enhanced
					durability on a minimum node version of 4.0, because the SDK will not check if
					each node supports the new observe option.</p>
				<p>Every mutation on the server side increases the sequence number. If application A
					and B update document D, such as the sequence one and two on the document, if we
					observe sequence two we can be sure that sequence one has also been replicated
					or persisted. That happens because the SDK uses sequence numbers instead of the
					CAS value.</p>
				<p>If a hard failover happens, a new partition UUID is created, and the server will
					return with a different response. From this response, the SDK can reliably infer
					if a mutation has been lost: the replica took over, but the last replicated
					sequence did not include the mutation we have been polling for. In this case, a
						<codeph>DocumentMutationLostException</codeph> will be raised. In general,
					it is recommended that the application re-performs the operation if this
					exception is encountered to avoid data loss.</p>
			</sectiondiv>
			<sectiondiv>
				<p><b>Performance considerations</b></p>
				<p>Couchbase Server is widely recognized for its excellent and predictable
					performance. One of the reasons for that is its managed cache, which allows it
					to return a response very quickly without taking replication or persistence
					latency into account.</p>
				<p>Again, it’s all tradeoffs. If you need to make sure data is replicated and/or
					persisted your network or disk performance will be the dominant factor. If you
					need high throughput and durability requirements, make sure (and measure) to
					have fast disks (SSD) and/or fast network.</p>
				<p>Because more than one node is in general involved and more round trips are
					needed, think about realistic timeouts you want to set and measure them in
					production. If you hit performance issues in production, make use of the new
					built-in metrics feature to gather operation latency percentile information and
					adjust timeouts based on those measurements. All timeouts you set on the
					blocking API need to take the original mutation and all subsequent polls into
					account until the durability requirement is met.</p>
				<p>One more thing you can tune in the environment is the
						<codeph>observeIntervalDelay</codeph>. It allows you to tune the delays
					between subsequent polls. By default, an exponential delay between 10
					microseconds and 100 milliseconds is used. That way there will be a few very
					quick polls followed by some with a longer pause, the ceiling at 100ms (so it
					does not grow exponentially out of bounds). The following example fixes it at 50
					microseconds:</p>
				<codeblock outputclass="language-java">CouchbaseEnvironment env = DefaultCouchbaseEnvironment
    .builder()
    .observeIntervalDelay(Delay.fixed(50, TimeUnit.MICROSECONDS))
    .build();</codeblock>
				<p>You want to tune the delay according to your network and disk performance
					characteristics. Too frequent polls just overload the network unnecessarily
					while too high poll delays will increase the latency for the overall operation
					(and subsequently reduce application throughput). If you are uncertain, work
					with Couchbase Support to find the optimal settings for your environment.</p>
			</sectiondiv>
		</section>
  	
  	
  	

  		

  
  </body>
</topic>
