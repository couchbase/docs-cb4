<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_ghd_55f_nbb">
  <title>Best Practices for Deploying Couchbase on Microsoft Azure</title>
  <body>
    <p>The Azure Resource Manager (ARM) templates aim to configure Couchbase according to our
      recommended best practices on Azure. Couchbase recommends ARM for all deployments on Azure. We
      do not recommend Azure Service Manager (ASM), also known as Classic.</p>
    <section id="azure-compute"><title>Compute</title>
      <p>A variety of compute types support premium storage. Any such node will work well with
        Couchbase, though some may be more cost effective. DS v3, ES v3, FS and GS machines are the
        most commonly used. While one core machines will deploy successfully, <xref
          href="https://developer.couchbase.com/documentation/server/current/install/pre-install.html"
          format="html" scope="external">we recommend machines with 4 or more cores</xref> for
        production applications.</p>
      <p>For a majority of applications the DS14 v2 will be a good balance of price and
        performance.</p>
      <p>We recommend using VM Scale Sets (VMSS) instead of stand alone VMs as it improves reliability
        and simplifies the addition and removal of nodes.</p>
    </section>
    
    <p id="azure-memory"><b>Memory Allocation</b></p>
      <p>Couchbase recommends allocating 85% of system memory to the database. When using MDS this can
        be tuned between data, query, etc. The templates currently allocate 50% for data and 15% for
        index. This can be adjusted after deployment.</p>
    
    <p id="azure-ft-ha"><b>Fault Tolerance and High Availability</b></p>
      <p>Couchbase is a strongly consistent database with peer-to-peer replication for handling node
        failures. Replicas are not needed to increase read performance due to a built-in managed
        caching layer. For deployments in Azure, we typically recommend one replica. In the event of a
        single node failure, replicas elsewhere in the cluster can be automatically promoted if
        automatic failover is <xref
          href="https://developer.couchbase.com/documentation/server/current/clustersetup/automatic-failover.html"
          format="html" scope="external">enabled</xref>. For most scenarios, the downed node will
        recover in a matter of minutes, obviating the need for additional replicas.</p>
      <p>A minimum of 3 nodes required for the data service to support automatic failover, and a
        minimum of two nodes for the query, index and FTS service to support high availability.
        Depending on your performance and topology needs, these services can be collocated or
        separated but the minimum node count requirement does not change.</p>
      <p>Azure currently has availability zones in preview.  However, the GA model for resileince is based on Availability Sets that are made up of Fault Domains (FD) and Upgrade Domains (UD). VMSS default to configuring 5 FDs, each with their own UD.  We recommend configuring a Server Group for each FD.  The templates don't yet automate this step.</p>
    
    <section id="azure-storage"><title>Storage</title>
      <p>We recommend using <xref
        href="https://docs.microsoft.com/en-us/azure/storage/storage-premium-storage" format="html"
        scope="external">Azure Premium Storage</xref> for data drives. Ephemeral drives present a
        risk of data loss. Standard Storage is based on spinning magnetic disks (HDD) and does not
        perform well enough for most database applications. HDD is sufficient for OS disks.</p>
      <p>Premium Storage comes in a variety of sizes. We recommend a 1TB P30 drive as the upper end.
        Large drives can lead to overly dense nodes that suffer from long rebuild times. It's usually
        preferable to scale horizontally instead.</p>
      <p>We strongly recommend using managed disks for both the OS and data disks. The older Storage
        Account mechanism has a higher potential for bottlenecks and is more complex.</p>
      <p>Microsoft recommends disabling Premium Storage caching for mixed read/write workloads like
        Couchbase.</p>
    </section>
    <section id="azure-network"><title>Network</title>
      <p>There are three potential network setups in Azure. Those are detailed below.</p>
      <p><b>Public IPs</b></p>
      <p>The Couchbase recommended setup is to attach a public IP to each node. The public IP can be
        used to connect application drivers and replicate across geographies with XDCR.</p>
      <p>The templates configure each Couchbase node with the public DNS. Because the public DNS
        resolves to a NAT based IP, we recommend adding a record to <codeph>/etc/hosts</codeph>
        on each node to resolve its public DNS to <codeph>127.0.0.1</codeph>. That allows
        Couchbase to bind to the IP underlying the public DNS.</p>
      <p>Traffic between public IPs in Azure is routed over the Azure backbone, not the public
        internet. The backbone has a bandwidth in 100s-1000s G. This means that traffic is limited by
        the network cap of a VM. Larger VMs have a 10G network cap.</p>
      <p><b>VPN Gateways</b></p>
      <p>An alternative is to use <xref
        href="https://azure.microsoft.com/en-us/pricing/details/vpn-gateway/" format="html"
        scope="external">VPN Gateways</xref>. The highest performance VPN Gateway has a 1.25G cap.
        Note that this cap is for the entire deployment, not a single node. As a result, traffic for
        most clusters will bottleneck on the gateway. Latency is also an issue with this setup.</p>
      <p>VPN Gateway setup is complex. A gateway must be configured in each region and then
        unidirectional connections created between them. When connecting multiple regions, the number
        of connections required will grow exponentially.</p>
      <p>We do not recommend VPN Gateways given their complexity of configuration and poor
        performance. It is our understanding that Microsoft intends VPN gateways for client to server
        connections, not high performance clustered applications like Couchbase.</p>
      <p><b>Express Route</b></p>
      <p>An <xref href="https://azure.microsoft.com/en-us/pricing/details/expressroute/" format="html"
        scope="external">Express Route</xref> circuit is a leased line. Microsoft works with
        providers like Verizon and Equinix to provide these lines. The highest bandwidth line is 10G
        and costs $50,000 per month. Note this is the bandwidth for the entire deployment, not a single
        node.</p>
      <p>Express Route also has a setup time measured in weeks to months as it includes both manual
        setup tasks and contract negotiations.</p>
      <p>Express Route traffic is routed through wherever the circuit exists. So, if you are running a
        deployment with nodes in New York and San Francisco and your Express Route circuit is in
        London, all traffic will be routed across the Atlantic and back.</p>
      <p>While Express Route is useful for on-prem/Azure hybrid solutions we do not recommend it for
        Azure to Azure XDCR communication.</p>
    </section>
    <section id="azure-security"><title>Security</title>
      <p>The template automatically sets up a username and password for the Couchbase Web
        Administrator. The template also configures a <xref
          href="https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-nsg"
          format="html" scope="external">Network Security Group (NSG)</xref> that closes off unused
        ports. This configuration can be further secured by specifying CIDR blocks to whitelist and
        blocking others.</p>
      <p>Azure automatically configures disk encryption for Managed Disks that use Premium Storage.
        More detail is available <xref
          href="https://azure.microsoft.com/en-us/blog/azure-managed-disks-sse" format="html"
          scope="external">here</xref>.</p>
      <p>The template does not currently configure SSL. We recommend setting it up for production
        applications.</p>
      <p>These templates open Sync Gateway access to the internet over ports 4984 and 4985. We
        typically recommend securing the admin interface for access from
        <codeph>127.0.0.1</codeph> only. That can be done by editing the
        <filepath>/home/sync_gateway/sync_gateway.json</filepath> file.</p>
    </section>
  </body>
</topic>
