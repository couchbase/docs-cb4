<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_nzk_yln_vs">
  <title>Failing over a Node</title><shortdesc>Failover is the process in which a node of a Couchbase cluster is removed quickly as
    opposed to a regular removal and rebalancing. </shortdesc>
  <body>
    <p>There are three types of failover: <term>graceful</term>, <term>hard</term>, and
        <term>automatic</term>.</p>

    <note type="note">To fully understand how failover works, see 
      <xref href="../understanding-couchbase/clusters-and-availability/clusters-and-availability.dita"/> and
      <xref href="../understanding-couchbase/services-and-indexes/services/services.dita"/>
      to learn how
      data and services are distributed across a Couchbase
      Cluster.</note>
    <dl>
      <dlentry>
        <dt><xref href="setup-failover-graceful.dita#topic_ysk_ycm_zs">Graceful failover</xref></dt>
        <dd>Graceful failover is the proactive ability to remove a Data service node from the
          cluster in an orderly and controlled fashion. It is an online operation with zero downtime
          that is achieved by promoting replica vBuckets on the remaining cluster nodes to active
          and the active vBuckets on the affected node to dead. This type of failover is primarily
          used for planned maintenance of the cluster.</dd>
      </dlentry>
    </dl>

<dl>
  <dlentry>
    <dt><xref href="hard-failover.dita#topic_a4s_d24_vs">Hard failover</xref></dt>
    <dd>Hard failover is the ability to drop a node quickly from the cluster when it has become
          unavailable or unstable. Dropping a node is achieved by promoting replica vBuckets on the
          remaining cluster nodes to active. Hard failover is primarily used when there is an
          unplanned outage of a node.</dd>
  </dlentry>
</dl>
<dl>
  <dlentry>
    <dt><xref href="automatic-failover.dita#topic_fcf_chm_zs">Automatic failover</xref></dt>
    <dd>Automatic failover is the built-in ability to have the Cluster Manager detect and determine
          when a node is unavailable and then initiate a hard failover.</dd>
  </dlentry>
</dl>
    <p>Keep in mind that each of the Couchbase Services reacts differently to failover. For example,
      if you have the services separated on different nodes, you can failover and recover nodes
      separately. If you are mixing the services on the same nodes, it is slightly more complex.
      Failover regarding all services will be discussed in this section.</p>   
    
    <section><title>Choosing a Failover Solution when Nodes are Unhealthy</title>
      <p>As a node failover has the potential to reduce the performance of your cluster, 
        you should consider how best to handle a failed node situation and also size your cluster to plan for failover.</p> 
   <dl>
     <dlentry>
       <dt>Manual or monitored failover</dt>
       <dd>Performing manual failover through monitoring can take two forms, either by human
            monitoring or by using a system external to the Couchbase Server cluster. An external
            monitoring system can monitor both the cluster and the node environment so that you can
            make a more data-driven decision. Although automated failover has inherent issues,
            choosing whether to use manual or monitored failover is not without potential problems. </dd>
       
     </dlentry>
   </dl> 
      <dl>
        <dlentry>
          <dt>Human intervention</dt>
          <dd>One option is to have a human operator respond to alerts and
            make a decision. Humans are uniquely capable of considering a wide range of data,
            observations, and experiences to resolve a situation in a best possible way. Many
            organizations disallow automated failover because they want a human to consider the
            implications. The drawback is that human intervention is slower than using a
            computer-based monitoring system.</dd>
        </dlentry>
      </dl>
    <dl>
      <dlentry>
        <dt>External monitoring</dt>
        <dd>Another option is to have a system monitoring the cluster via the Couchbase REST API.
            Such an external system can failover nodes successfully because it can take into account
            system components that are outside the scope of Couchbase Server. <p>For example,
              monitoring software can observe that a network switch is failing and that there is a
              dependency on that switch by the Couchbase cluster. The system can determine that
              failing Couchbase Server nodes will not help the situation and will, therefore, not
              failover the node. The monitoring system can also determine that components around
              Couchbase Server are functioning and that various nodes in the cluster are healthy.</p><p> If
              the monitoring system determines the problem is only with a single node and remaining
              nodes in the cluster can support aggregate traffic, then the system may safely
              failover the node using the REST API or command-line tools. </p></dd>
      </dlentry>
    </dl> 
   
   
   <dl>
     <dlentry>
       <dt>Automatic failover</dt>
       <dd>With the <xref href="automatic-failover.dita#topic_fcf_chm_zs">automatic failover</xref>,
            the Cluster Manager handles the detection, determination of, and initiation of the
            processes to failover a node without user intervention and without identification of the
            issue that caused the node failure. Once the problem has been identified and fixed, it
            still requires you to initiate a rebalance to return the cluster to a healthy
              state.<p>If you do not use automatic failover and a node becomes unhealthy, it will be
              up to the administrators to intervene and initiate a <xref
                href="hard-failover.dita#topic_a4s_d24_vs">hard failover</xref> of the
            node.</p></dd>
     </dlentry>
   </dl>
   </section> 
    <section>
      <title>The Difference Between Graceful and Hard Failover</title>
      <p>A hard failover is reactionary and usually comes when a node of the cluster is in an
        unhealthy or unstable state (such as when a node is down). It would be used during an
        unplanned outage. With a node failed, the cluster is without the full accompaniment of
        active vBuckets. A hard failover will eject the unhealthy node from the cluster and promote
        the necessary replica vBuckets on the remaining nodes to bring the active vBucket count back
        to 1024 for each bucket</p>
      <p>A graceful failover, which is a coordinated synchronization and removal of the node,  is a
        proactive action initiated from a stable state of the cluster. It might be used, for
        example, during a software or OS upgrade.  The cluster always has the full 1024 active
        vBuckets for each Bucket throughout the process.</p>
    </section>
    
  </body>
</topic>
