<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_fcf_chm_zs">
  
  <title>
    Using Automatic Failover</title> 
  
  <shortdesc>
    A node or group can be failed over automatically, when it either
    becomes unavailable or experiences continuous disk-access problems.
  </shortdesc>
  
  <body>
    
    <section>
      <title>
        Understanding Automatic Failover
      </title>
    
    <p>
      <i>Automatic Failover</i> &#8212; or <i>auto-failover</i> &#8212; can be configured to
      fail over a node or group automatically: no
      immediate administrator intervention is required.
      Specifically, the Cluster Manager autonomously detects and verifies that the node or group is unavailable,
      and then initiates the <i>hard</i> failover process.
      Auto-failover does not fix or identify problems that may have occurred. Once
      appropriate fixes have been applied to the cluster by the administrator, a rebalance is required.
      Automatic failover is always <i>hard</i> failover.    
    </p>
      
    </section>
    
    <section><title>Automatic Failover Timeout</title>
      <p>By default, there is a 120 second delay before a node will be automatically failed over by
        the Cluster Manager. This timeout can be increased to any value up to 3600 seconds, and
        decreased to any value above 5 seconds. This 5 second minimum delay is needed by the
        software to confirm that the node is indeed down by performing multiple checks of that
        node’s status. This helps prevent false alarms, such as failover of a functioning but slow
        node or failover due to ephemeral network connection issues. <b>It is strongly recommended
          that you do not lower the failover timeout from its default of 120 seconds unless you have
          very reliable infrastructure underlying each of your Couchbase Server nodes.</b></p>
     
     
   </section> 
    <section><title>Resetting the Automatic Failover Counter</title><p>After a node has been
        automatically failed over, Couchbase Server increments an internal counter that indicates to
        the cluster if a node has been failed over. The counter prevents the Cluster Manager from
        automatically failing over additional nodes until you identify any issues that caused the
        failover and resolve them.</p><p>Reset the automatic failover only:</p><ul>
        <li><i>after</i> the node’s issue is resolved, rebalance has occurred, and the cluster was
          restored to a fully functioning state.<p>or</p></li>
        <li>if the node has the <xref
            href="../architecture/data-service-core-data-access.dita#concept_tcf_byn_vs">Data
            service</xref> running and each bucket has additional replicas beyond the existing ones
          to handle another node auto-failover.</li>
      </ul><p>You can reset this counter with the Couchbase Web Console by clicking on the
          <uicontrol>Reset Quota</uicontrol> button. You can also reset the counter using the REST API:
        <codeblock outputclass="language-bash">>$ curl -i -u cluster-username:cluster-password http://localhost:8091/settings/autoFailover/resetCount</codeblock></p>Once
      the counter has been reset, the Cluster Manager could failover another node of the cluster. So
      again, reset the counter only if the criteria for automatic failover are met.</section>
    <section><title>When Not to Use Automatic Failover</title>
      <p>Automatic failover is appropriate in most environments. However, it is disabled by default
        to ensure that administrators are explicitly aware of the behavior of their cluster.</p>  
      <p>There are a few situations where auto-failover would not advisable to use or when the
        environment is known to contain instability that could introduce false positives even when
        the nodes did not fail: </p>
      <ul>
        <li>
          <i>Node failure</i>. A server-node within the cluster is unavailable (due to a network failure, 
          out-of-memory problem, or other node-specific issue).
          
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Disk read/write failure</i>. Attempts to read from or write to disk on a particular node have resulted in 
          a significant rate of failure, for longer than a specified time-period. The node is removed by 
          auto-failover, even though the node continues to be contactable.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Group failure</i>. An administrator-defined group of server-nodes within the cluster is unavailable (perhaps 
          due to a network or power failure that has affected an individual, physical rack of machines, or a specific subnet).
          <p>
            
          </p>
        </li>
        
      </ul>
      
    </section>
    
    <section>
      
      <title>
        Auto-Failover Constraints
      </title>
      
      <p>
        Couchbase Server applies the following constraints to auto-failover’s handling of events:
      </p>
      
      
      <ul>
        <li>
          Auto-failover occurs only on the occurrence of one event at a time. If multiple events occur 
          concurrently, or if a second event follows the first prior to auto-failover’s completion, 
          auto-failover is not triggered.
          <p>
            
          </p>
         
        </li>
        
        <li>
          Auto-failover occurs sequentially up to an administrator-specified maximum number of events. After 
          this maximum number of auto-failovers has been triggered, no further auto-failover occurs, until the 
          count is manually reset by the administrator.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          Auto-failover is never triggered by Couchbase Server when data-loss might result. Therefore, even a single 
          event may not be responded to; 
          and an administrator-specified maximum number of events may not be reached.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          Auto-failover is never triggered when the resulting number of available nodes within the cluster would not 
          constitute a majority of the number available before the problem-occurrence. For example, if one of 
          three nodes or groups were to go offline, auto-failover could be triggered; but if one of two nodes 
          or groups were to go offline, auto-failover could not be triggered. (This prevents the situation 
          where functioning nodes or groups, finding they cannot intercommunicate, 
          fail each other over, and then attempt to continue independently of one another, each 
          representing itself as the cluster.)
          
          <p>
            
          </p>
          
        </li>
      </ul>
      
      <p>
        Auto-failover can be enabled only when the cluster contains a minimum of three nodes; and should be 
        configured for groups only when a minimum of three groups have been defined. Additionally, auto-failover 
        should be configured only when the cluster contains sufficient resources to handle all possible results: 
        workload-intensity on remaining nodes may increase significantly. Auto-failover is for intra-cluster use 
        only: it does not work with Cross Datacenter Replication.
      </p>
        
      <p>
        Auto-failover may take significantly longer if the non-available node is that on which the <i>orchestrator</i> 
        is running, since <i>time-outs</i> must occur before available nodes can elect a new orchestrator-node and thereby 
        continue.
      </p>
        
      <p>
        See 
        <xref href="../settings/configure-alerts.dita" scope="local" format="dita">Email Alerts</xref>, for details 
        on configuring email alerts related to failover.
      </p>
      
      <p> See <xref href="../understanding-couchbase/clusters-and-availability/groups.dita#groups"
        />, for information on server groups and group failover. </p>
        
    </section>
    
    <section>
      
      <title>
        Configuring Auto-Failover
      </title>
      
      <p>
        Auto-failover is configured by means of parameters that include the following. 
        
      </p>
      
      <ul>
        <li>
          <i>Timeout</i>. The number of seconds that must elapse, after a node or group has become unavailable, before auto-failover 
          is triggered. The default is 120. 
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Maximum count</i>. The maximum number of failover events that can occur sequentially and be handled by auto-failover. The 
          maximum-allowed value is 3, the default is 1. This parameter is available in Enterprise Edition only: in Community
          Edition, the maximum number of failover events that can occur sequentially and be handled by auto-failover is always 1.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Count</i>. The number of failover events that have occurred. The default value is 0. The value is incremented by
          1 for every automatic-failover event that occurs, up to the defined maximum count: beyond this point, no further
          automatic failover can be triggered until the count is reset to 0 through administrator-intervention. 
          
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Enablement of disk-related automatic failover; with corresponding time-period</i>. Whether automatic 
          failover is enabled to handle continuous
          read-write failures. If it is enabled,
          a number of seconds can also be specified: this is the length of a constantly recurring time-period against which 
          failure-continuity on a particular node is evaluated. If at least 60% of
          the most recently elapsed instance of the time-period has consisted of continuous failure, failover is automatically triggered. 
          The default value for the enablement of disk-related automatic failover is false. The default value for the 
          corresponding time-period is 120.
          This parameter is available in Enterprise Edition only.
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Group failover enablement</i>. Whether or not groups should be failed over. A group failover is considered to 
          be a single 
          event, even if many nodes are included in the group. The default value is false. This parameter is available in
          Enterprise Edition only.
          <p>
            
          </p>
        </li>
        
      </ul>
      
      
      <p>
        For more detailed information,
        see the documentation provided for specifying
        <xref href="../settings/change-failover-settings.dita" scope="local" format="dita">Node Availability</xref> with
        Couchbase Web Console UI,
        for
        <xref href="../rest-api/rest-cluster-autofailover-intro.dita" scope="local" format="dita">Managing Auto-Failover</xref>
        with the REST API, and
        <xref href="../cli/cbcli/couchbase-cli-setting-autofailover.html" scope="local" format="html">setting-autofailover</xref>
        with the CLI.
      </p>
    </section>
    
  </body>
</topic>
