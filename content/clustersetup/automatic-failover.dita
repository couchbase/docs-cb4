<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_fcf_chm_zs">
  
  <title>
    Using Automatic Failover</title> 
  
  <shortdesc>
    A node or group can be failed over automatically, when it either
    becomes unavailable or experiences continuous disk-access problems.
  </shortdesc>
  
  <body>
    
    <section>
      <title>
        Understanding Automatic Failover
      </title>
    
    <p>
      <i>Automatic Failover</i> &#8212; or <i>auto-failover</i> &#8212; can be configured to
      fail over a node or group automatically: no
      immediate administrator intervention is required.
      Specifically, the Cluster Manager autonomously detects and verifies that the node or group is unavailable,
      and then initiates the <i>hard</i> failover process.
      Auto-failover does not fix or identify problems that may have occurred. Once
      appropriate fixes have been applied to the cluster by the administrator, a rebalance is required.
    </p>
      
    </section>
    
    <section>
      
      <title>
        Failover Events
      </title>
      
      <p>
        Auto-failover occurs in response to failover events. Events are of three kinds:
      </p>
      
      <ul>
        <li>
          <i>Node failure</i>. A server-node within the cluster is unavailable (perhaps due to a network failure, 
          out-of-memory problem, or other node-specific issue).
          
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Disk read/write failure</i>. Attempts to read from or write to disk on a particular node have resulted in 
          a significant rate of failure for longer than a specified time-period. The node is removed by 
          auto-failover, even though the node continues to be contactable.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Group failure</i>. An administrator-defined group of server-nodes within the cluster is unavailable (perhaps 
          due to a network or power failure that has affected an individual, physical rack of machines, or a specific subnet).
          <p>
            
          </p>
        </li>
        
      </ul>
      
    </section>
    
    <section>
      
      <title>
        Auto-Failover Constraints
      </title>
      
      <p>
        Couchbase Server applies the following constraints to auto-failover’s handling of events:
      </p>
      
      
      <ul>
        <li>
          Auto-failover occurs only on the occurrence of one event at a time. If multiple events occur 
          concurrently, or if a second event follows the first prior to auto-failover’s completion, 
          auto-failover is not triggered.
          <p>
            
          </p>
         
        </li>
        
        <li>
          Auto-failover occurs sequentially up to an administrator-specified maximum number of events. After 
          this maximum number of auto-failovers has been triggered, no further auto-failover occurs, until the 
          count is manually reset by the administrator.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          Auto-failover is never triggered by Couchbase Server when data-loss might result. Therefore, even a single 
          event may not be responded to; 
          and an administrator-specified maximum number of events may not be reached.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          Auto-failover is never triggered when the resulting number of available nodes within the cluster would not 
          constitute a majority of the number available before the problem-occurrence. For example, if one of 
          three nodes or groups were to go offline, auto-failover could be triggered; but if one of two nodes 
          or groups were to go offline, auto-failover could not be triggered. (This prevents the situation 
          where functioning nodes or groups, finding they cannot intercommunicate, 
          fail each other over, and then attempt to continue independently of one another, each 
          representing itself as the cluster.)
          
          <p>
            
          </p>
          
        </li>
      </ul>
      
      <p>
        Auto-failover can be enabled only when the cluster contains a minimum of three nodes; and should be 
        configured for groups only when a minimum of three groups have been defined. Additionally, auto-failover 
        should be configured only when the cluster contains sufficient resources to handle all possible results: 
        workload-intensity on remaining nodes may increase significantly. Auto-failover is for intra-cluster use 
        only: it does not work with Cross Datacenter Replication.
      </p>
        
      <p>
        Auto-failover may take significantly longer if the non-available node is that on which the <i>orchestrator</i> 
        is running: <i>time-outs</i> must occur before available nodes can elect a new orchestrator-node and thereby 
        continue.
      </p>
        
      <p>
        See 
        <xref href="../settings/configure-alerts.dita" scope="local" format="dita">Email Alerts</xref>, for details 
        on configuring email alerts related to failover.
      </p>
      
      <p>
        See
        <xref href="../understanding-couchbase/clusters-and-availability/groups.dita" scope="local" format="dita">Groups</xref>, for
        information on <i>Server Group Awareness</i>.
      </p>
        
    </section>
    
    <section>
      
      <title>
        Configuring Auto-Failover
      </title>
      
      <p>
        Auto-failover is configured by means of parameters that include the following. 
        
      </p>
      
      <ul>
        <li>
          <i>Timeout</i>. The number of seconds that must elapse after a node or group has become unavailable before auto-failover 
          is triggered. The default is 120.
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Count</i>. The maximum number of failover events that can occur sequentially and be handled by auto-failover. The 
          -allowed value is 3, the default is 1.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Disk Issue Timeout</i>. The number of seconds that is the length of a constantly recurring time-bracket against which 
          the rate of read/write failures on a particular node is evaluated. If 60% of checks during any occurrence 
          of the time-bracket 
          report failures, auto-failover can be triggered. The default is 120.
          <p>
            
          </p>
        </li>
        
        <li>
          <i>Group failover enablement</i>. Whether or not groups should be failed over. A group failover is considered to 
          be a single 
          event, even if many nodes are included in the group.
          <p>
            
          </p>
        </li>
        
      </ul>
      
      <p>
        For more detailed information,
        see the documentation provided for specifying
        <xref href="../settings/change-failover-settings.dita" scope="local" format="dita">Node Availability</xref> with
        Couchbase Web Console UI,
        for
        <xref href="../rest-api/rest-cluster-autofailover-intro.dita" scope="local" format="dita">Managing Auto-Failover</xref>
        with the REST API, and
        <xref href="../cli/cbcli/couchbase-cli-setting-autofailover.dita" scope="local" format="dita">setting-autofailover</xref>
        with the CLI.
      </p>
    </section>
    
  </body>
</topic>
