<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_xfr_byq_ft">
	<title>Durability requirements</title>
	<shortdesc>If you do not specify any durability requirements, the server will respond with a success
		message if the data has been acknowledged and processed in the managed cache.</shortdesc>
	<conbody>
		<p>Because persistence and replication are asynchronous tasks and happen eventually, there is a
			time gap where a node failure can lead to data loss. This time gap is exactly between
			the data has neither been replicated to another node nor persisted to disk.</p>
		<p>In general, for most use cases this time gap is totally acceptable (since it is usually
			very small), but for the important mutation operations we need stronger guarantees.
			Changing the server to some more magic (like only acknowledging once it has been replicated
			or persisted) by default makes the whole system slower, so Couchbase has another way to
			handle those situations. Here is how it works:</p>
		<ol>
			<li>The SDK will perform the normal mutation without durability requirements.</li>
			<li>If the server returns with a successful response, the client will start polling.</li>
			<li>The client polls all the affected (more in a bit on this) nodes for this mutation until
				either the desired state is reached or it can’t be reached for a reason.</li>
			<li>In the successful case the operation will also complete towards the application layer,
				and in the failure case the client will error out the operation, leaving the user to
				decide whats next.</li>
		</ol>
		<p>The following code will make sure that the document has been persisted on the active node
			for this document ID and also replicated to one of the configured replicas:</p>
		<codeblock outputclass="language-go">cas, err := myBucket.UpsertDura(docToStore, valueToStore, 1, 1)</codeblock>
		<p>In this example the client will poll two nodes until completion: the active node for this
			document and also the configured replica. If any of the constraints cannot be fulfilled,
			an error will be returned.</p>
		<p>In case you wonder why in the failure case we put the burden on you to figure out what
			next: the SDK has no idea of your SLAs or intents to what do to when the operation fails.
			Sometimes it might be fine to proceed and log the error, in other cases you may want
			sophisticated retry mechanisms where the SDK can guide you with functionality, but not the
			actual execution semantics.</p>
		<section><title>Enhanced durability requirements with 4.0+</title><p>Couchbase Server 4.0
				introduces a new feature that allows the SDK to be more accurate during the observe
				poll cycle, especially in the concurrent and failover cases. Instead of using the
				CAS to verify mutations, it uses sequence numbers and partition UUIDs.</p>They are
			automatically enabled by default when available and will be used for any durability
			operations performed.</section>
		<section>
			<title>Performance considerations</title>
			<p>Couchbase Server is widely recognized for its excellent and predictable performance. One
				of the reasons for that is its managed cache, which allows it to return a response very
				quickly and not having to take replication or persistence latency into account.</p>
			<p>Again, it’s all tradeoffs. If you need to make sure data is replicated and/or persisted
				your network or disk performance will be the dominant factor. If you need high
				throughput and durability requirements, make sure (and measure) to have fast disks (SSD)
				and/or fast network.</p>
			<p>Because more than one node is in general involved and more round trips are needed, think about
				realistic timeouts you want to set and measure them in production. All timeouts you
				set on the blocking API need to take the original mutation and all subsequent polls
				into account until the durability requirement is met.</p>
		</section>


	</conbody>
</concept>
