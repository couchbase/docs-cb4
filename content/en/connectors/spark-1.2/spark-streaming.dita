<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="spark-working-with-rdds">
	<title>Spark Streaming Integration</title>
	<shortdesc>The Couchbase Spark connector works with Spark Streaming by using the Couchbase Server
		replication protocol (called DCP) to receive mutations on the server side as they happen and
		provide them to you in the form of a DStream. The DStream is the primary format used by Spark
		Streaming. You can create and persist DStreams.</shortdesc>
	<conbody>
    <section>
      <title>DStream Creation</title>

      <p><note>Creating DStreams from DCP through Couchbase is highly experimental and not ready to be used in production yet. We appreciate any feedback until we stabilize this module.</note></p>

      <p>The following example will print out all mutations that have happened and all mutations which are going to happen. You can then utilize them through all the common DStream methods.</p>

<codeblock outputclass="language-scala"><![CDATA[val conf = new SparkConf().setMaster("local[*]").setAppName("StreamingSample")
val ssc = new StreamingContext(conf, Seconds(5))

ssc
  .couchbaseStream(from = FromBeginning, to = ToInfinity)
  .filter(!_.isInstanceOf[Snapshot]) // Don't print snapshots, just mutations and deletions
  .print()

ssc.start()
ssc.awaitTermination()]]></codeblock>

      <p>Don't forget to add the implicit import:</p>

<codeblock outputclass="language-scala"><![CDATA[import com.couchbase.spark.streaming._]]></codeblock>

    <p><note>Currently, the connector issues a backfill, which means that all data stored in the bucket
          will be streamed first, so you might want to try it on an empty bucket (<codeph>from =
            FromBeginning</codeph>).</note></p>

    </section>
		<section>
			<title>DStream Persistence</title>

			<p>Since DStreams are RDDs, the same persistence mechanisms apply (see <xref
          href="working-with-rdds.dita#spark-working-with-rdds">Working with RDDs</xref> for more
        information). Here is an example that streams tweets from Twitter and persists them in a
        Couchbase bucket:</p>

<codeblock outputclass="language-scala"><![CDATA[// Configure Spark
val cfg = new SparkConf()
  .setAppName("TwitterFeed")
  .setMaster("local[*]")
  .set("com.couchbase.bucket.twitter", "")

val Array(consumerKey, consumerSecret, accessToken, accessTokenSecret) = args.take(4)
System.setProperty("twitter4j.oauth.consumerKey", consumerKey)
System.setProperty("twitter4j.oauth.consumerSecret", consumerSecret)
System.setProperty("twitter4j.oauth.accessToken", accessToken)
System.setProperty("twitter4j.oauth.accessTokenSecret", accessTokenSecret)

val ssc = new StreamingContext(cfg, Seconds(2))
val stream = TwitterUtils.createStream(ssc, None, Seq())

val hashTags = stream

hashTags.foreachRDD(rdd => {
  rdd
    .map(status => JsonDocument.create(status.getId.toString, JsonObject.create().put("text", status.getText)))
    .saveToCouchbase()

})

ssc.start()
ssc.awaitTermination()]]></codeblock>

    </section>

	</conbody>
</concept>
