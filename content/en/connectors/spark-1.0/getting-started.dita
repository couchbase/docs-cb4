<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE concept
  PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept xml:lang="en-us" id="concept227">
	<title>Getting Started</title>
	<shortdesc>To get started with the Couchbase Spark connector quickly, learn how to add the
		connector to your Spark project and run simple queries. </shortdesc>

	<conbody>

		<section>
			<title>Quickstart</title>

			<p>Because most people using Spark are in the Scala ecosystem, the following examples use Scala
					and its sbt dependency manager.</p>

			<p>Create a new sbt project and add the following content to the <codeph>build.sbt</codeph> file.
				This code includes all the Spark dependencies, as well as the Couchbase Spark connector.
				Just enough to get you started!</p>

			<p>Here is a reference to the <xref
					href="http://docs.couchbase.com/sdk-api/couchbase-spark-connector-1.0.1/"
					format="html" scope="external">Scala docs</xref>.</p>

<codeblock outputclass="language-scala"><![CDATA[name := "my-first-couchbase-spark-project"

organization := "my.organization"

version := "1.0.0-SNAPSHOT"

scalaVersion := "2.10.5"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "1.4.1",
  "org.apache.spark" %% "spark-streaming" % "1.4.1",
  "org.apache.spark" %% "spark-sql" % "1.4.1",
  "com.couchbase.client" %% "spark-connector" % "1.0.1"
)
]]></codeblock>

			<p>Now, under <filepath>src/main/scala/</filepath>, create a <codeph>Quickstart.scala</codeph>
				class with the following skeleton:</p>

<codeblock outputclass="language-scala"><![CDATA[object Quickstart {

	def main(args: Array[String]): Unit = {

	}

}
]]></codeblock>

			<p>If you are not familiar with Scala, this is more or less the equivalent to Java's
					<codeph>public static void main(String[] args)</codeph> method. Because empty methods
				make the compiler sad, fill it with proper code now.</p>

			<p>When it comes to Spark, you always need to set up a configuration and initialize the
					<codeph>SparkContext</codeph> object. The following snippet does that and also
				instructs the Couchbase Spark connector to open a bucket in the background.</p>

<codeblock outputclass="language-scala"><![CDATA[// Configure Spark
val cfg = new SparkConf()
  .setAppName("couchbaseQuickstart") // give your app a name
  .setMaster("local[*]") // set the master to local for easy experimenting
  .set("com.couchbase.bucket.travel-sample", "") // open the travel-sample bucket

// Generate The Context
val sc = new SparkContext(cfg)]]></codeblock>

			<p>By default, the Spark Connector connects to the <codeph>default</codeph> bucket. However, this
				example uses actual data from the <codeph>travel-sample</codeph> bucket that ships with
				Couchbase Server, so the code connects to that bucket. The Couchbase Spark connector
				also supports opening more buckets in parallel.</p>

		</section>

		<section>
			<title>Creating and saving RDDs</title>

			<p>Now that the <codeph>SparkContext</codeph> is created, you can perform operations against
				Couchbase. A common scenario is to create resilient distributed datasets (RDDs) out of
				documents stored in Couchbase. The easiest one is probably by passing in Document IDs as
				strings.</p>

			<p>Before getting further into the actual code, make sure to have the following items imported,
				because otherwise the implicit methods won't be available.</p>

<codeblock outputclass="language-scala"><![CDATA[import com.couchbase.spark._]]></codeblock>

			<p>Use the <codeph>couchbaseGet</codeph> method on the <codeph>SparkContext</codeph> to fetch
				documents from Couchbase and create an RDD.</p>

<codeblock outputclass="language-scala"><![CDATA[sc
	.couchbaseGet[JsonDocument](Seq("airline_10123", "airline_10748"))
	.collect()
	.foreach(println)
]]></codeblock>

			<p>Make sure to specify which document type you want (in this case, a
					<codeph>JsonDocument</codeph>). If you forget to specify the document type, an
				exception is thrown because the connector does not know what format to use for the
				results. Then call <apiname>collect()</apiname> to aggregate the results and print them
				out to the command line.</p>

			<p>Spark outputs lots of information, so you'll find the output somewhere in the logs:</p>

<codeblock outputclass="language-json"><![CDATA[JsonDocument{id='airline_10748', cas=314538279043072, expiry=0, content={"country":"United States","iata":"ZQ","name":"Locair","callsign":"LOCAIR","icao":"LOC","id":10748,"type":"airline"}}
JsonDocument{id='airline_10123', cas=314538278125568, expiry=0, content={"country":"United States","iata":"TQ","name":"Texas Wings","callsign":"TXW","icao":"TXW","id":10123,"type":"airline"}}
]]></codeblock>

			<p>But since just loading data is half the fun, the connector also provides a convenient way to
				save documents. The following code loads documents like before, but then modifies
				its contents and ID before saving it back. You can imagine taking any kind of data
				source, mapping them to documents and just storing them back in Couchbase.</p>

<codeblock outputclass="language-scala"><![CDATA[sc
  .couchbaseGet[JsonDocument](Seq("airline_10123", "airline_10748"))
  .map(oldDoc => {
    val id = "my_" + oldDoc.id()
    val content = JsonObject.create().put("name", oldDoc.content().getString("name"))
    JsonDocument.create(id, content)
  })
  .saveToCouchbase()]]></codeblock>

  		<p>We utilize the <codeph>saveToCouchbase()</codeph> method available on our RDD to store a
				modified version of our original <codeph>JsonDocument</codeph>. Go find your
				modified document in the Couchbase Server UI! Look for
					<codeph>"my_airline_10123"</codeph> which will just have the name of the airline
				as its content.</p>

  		<p>Congratulations! You've successfully performed your first ETL job (extract-transform-load) using Couchbase and Spark. Next up is a whirlwind tour of N1QL and Spark DataFrames.</p>

		</section>

		<section>
			<title>Working with DataFrames</title>

			<p>DataFrames were introduced in Spark 1.3 and have matured even further in Spark 1.4. The nature
				of the queries fits very well with what Couchbase N1QL provides. <note>To try this,
					you need Couchbase Server version 4.0 or later.</note></p>

			<p><note>You need to at least have a primary index created on the <codeph>travel-sample</codeph> bucket to make the following examples work. If you haven't done already, perform a <codeph>CREATE PRIMARY INDEX ON `travel-sample`</codeph> query.</note></p>

			<p>In addition to creating a <codeph>SparkContext</codeph>, you'll need an
					<codeph>SQLContext</codeph>:</p>

<codeblock outputclass="language-scala"><![CDATA[val sql = new SQLContext(sc)]]></codeblock>

			<p>Also, don't forget the Couchbase imports again for all the automatic method goodness:</p>

<codeblock outputclass="language-scala"><![CDATA[import com.couchbase.spark.sql._]]></codeblock>

			<p>Because a <codeph>DataFrame</codeph> is like an <codeph>RDD</codeph> but with a schema and
				Couchbase is a schemaless database at its heart, you need a way to either define or
				infer a schema. The connector has built-in schema inference, but if you have a large
				or diverse data set, you need to give it some clues on filtering.</p>

			<p>Suppose you want a <codeph>DataFrame</codeph> for all airlines, and you know that the JSON
				content has a <codeph>type</codeph> field with the value <codeph>airline</codeph>.
				You can pass this information to the connector for automatic schema inference:</p>

<codeblock outputclass="language-scala"><![CDATA[// Create a DataFrame with Schema Inference
val airlines = sql.read.couchbase(schemaFilter = EqualTo("type", "airline"))

// Print The Schema
airlines.printSchema()]]></codeblock>

			<p>The code automatically infers the schema and prints it in this format:</p>

<pre><![CDATA[root
 |-- META_ID: string (nullable = true)
 |-- callsign: string (nullable = true)
 |-- country: string (nullable = true)
 |-- iata: string (nullable = true)
 |-- icao: string (nullable = true)
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
 |-- type: string (nullable = true)]]></pre>

 			<p>Next you can perform an actual query where you are interested only in the
					<codeph>name</codeph> and <codeph>callsign</codeph>. This example sorts it by the
					<codeph>callsign</codeph> and loads only the first 10 rows.</p>

<codeblock outputclass="language-scala"><![CDATA[airlines
  .select("name", "callsign")
  .sort(df("callsign").desc)
  .show(10)]]></codeblock>

  			<p>The code prints the results on the console like this:</p>

<pre>+-------+--------------------+
|   name|            callsign|
+-------+--------------------+
|   EASY|             easyJet|
|   BABY|             bmibaby|
|MIDLAND|                 bmi|
|   null|          Yellowtail|
|   null|               XOJET|
|STARWAY|   XL Airways France|
|   XAIR|            XAIR USA|
|  WORLD|       World Airways|
|WESTERN|    Western Airlines|
|   RUBY|Vision Airlines (V2)|
+-------+--------------------+</pre>

		</section>

	</conbody>
</concept>
