<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_axp_glg_xs">
  <title>Sizing Guidelines</title> <shortdesc>Evaluate the overall 
    performance and capacity requirements and determine the hardware and other resources.</shortdesc>
  <body>
    <p>When you plan to deploy a Couchbase Server cluster, perhaps the most common (and important)
      question that comes up is: how many nodes do I need and what size do they need to be? To learn
      more, contact Couchbase Support for sizing help. You can also read about sizing in <xref
        href="http://blog.couchbase.com/how-many-nodes-part-1-introduction-sizing-couchbase-server-20-cluster"
        format="html" scope="external">How many nodes</xref>. </p>
    <p>With the introduction of <xref
      href="../clustersetup/services-mds.dita"
        >Multi-Dimensional Scaling (MDS)</xref>, sizing is becoming more challenging, and this
      guideline aims to help users better size their clusters.</p>
    
    <section><title>General Considerations</title> 
    
    <p>The sizing of your Couchbase Server cluster is critical to its overall stability and
        performance. While there are obviously many variables that go into this, the idea is to
        evaluate the overall performance and capacity requirements for your workload and dataset.
        Then divide that into the hardware and resources you have available. Your application wants
        the majority of reads coming out of the cache, and the I/O capacity to handle its writes.
        There needs to be enough capacity in all areas to support everything the system is doing
        while maintaining the required level of performance. </p>
    <p>This guideline will refer to five determining factors one should be aware of when sizing a
        Couchbase Server cluster: RAM, Disk (I/O and space), CPU and network bandwidth.</p>
    </section>
    
    <section><title>Sizing Couchbase Server Resources </title>
    <p>There are several resources you need to consider when sizing a Couchbase Server node: <ul>
          <li>Number of CPUs</li>
          <li>RAM</li>
          <li>Disk space</li>
          <li>Network</li>
        </ul></p>
    
      
        
      
      <dl>
        <dlentry>
          <dt>CPU</dt>
          <dd>CPU refers to the number of cores required to run your workload. </dd>
        </dlentry>
      </dl>
      <dl>
        <dlentry>
          <dt>RAM</dt>
          <dd>RAM is frequently one of the most crucial areas to size correctly. Cached documents
            allow the reads to be served at very low latency and consistently high throughput.</dd>
          <dd>This resource represents the main memory allocated to Couchbase Server and should be
            determined by the following factors: 
            <ul>
              <li>How much free RAM is available beyond OS and other applications.</li>
              <li>How much data do you want to store in main memory.</li>
              <li>How much latency you expect from KV/indexing/query performance. </li>
            </ul><p>The new components that require RAM are:</p><ul>
              <li>Memory-optimized Global Indexes, which enable in-memory index processing and index
                scans with the lowest latency. </li>
              <li>Full Text Search (FTS), where the minimum RAM allocation is 512MB, and recommended
                is 2048MB+ RAM. </li>
            </ul></dd>
          <dd>
            <p>
              <table frame="all" rowsep="1" colsep="1" id="table_j34_3cq_y5">
                <title>Minimum RAM quota for Couchbase Server components</title>
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                  <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                  <thead>
                    <row>
                      <entry>Component</entry>
                      <entry>Minimum RAM</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>Data Service</entry>
                      <entry>1GB</entry>
                    </row>
                    <row>
                      <entry>Indexing Service powered by ForestDB</entry>
                      <entry>256MB</entry>
                    </row>
                    <row>
                      <entry>Indexing Service powered by Memory-Optimized Index</entry>
                      <entry>256MB minimum, 1GB recommended.</entry>
                    </row>
                    <row>
                      <entry>Full text search</entry>
                      <entry>512MB</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </p>
          </dd>
        </dlentry>
      </dl><dl>
        <dlentry>
          <dt>Disk space</dt>
          <dd>Requirements for your disk subsystem are: <ul>
            <li><term>Disk size</term>, which refers to the amount of the disk storage space
              needed to hold your entire data set.</li>
            <li><term>Disk I/O</term>, which is a combination of your sustained read/write rate,
              the need for compacting the database files, and anything else that requires disk
              access. </li>
          </ul><p>To better support your Couchbase Server, keep in mind the following: <ul>
            <li>Disk space continues to grow if fragmentation ratio keeps climbing. To mitigate
              that, add enough buffer in your disk space to store all the data. Also, keep an
              eye on the fragmentation ratio in the UI and trigger compaction process when
              needed. </li>
            <li>SSD is desired but not required. SSD will bring much better performance than HDD
              regarding disk throughput and latency.</li>
          </ul></p></dd>
        </dlentry>
      </dl>
      <table frame="all" rowsep="1" colsep="1" id="table_qzz_b1q_y5">
        <title>Minimal and Recommended Hardware Resources</title>
        <tgroup cols="3">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="1*"/>
          <colspec colname="c3" colnum="3" colwidth="3*"/>
          <thead>
            <row>
              <entry>Resource</entry>
              <entry>Minimum</entry>
              <entry>Recommended</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>CPU </entry>
              <entry>4</entry>
              <entry>8 and above. Both the number and performance of CPUs are important.</entry>
            </row>
            <row>
              <entry>RAM</entry>
              <entry>4GB</entry>
              <entry>16GB and above. More RAM means that more data can be served from memory and
                allow for better performance. </entry>
            </row>
            <row>
              <entry>Disk</entry>
              <entry>8GB</entry>
              <entry>16GB and above</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    
    <dl>
      <dlentry>
        <dt>Network</dt>
        <dd>Enough network bandwidth is vital to the performance of Couchbase Server. A reliable
            high-speed network for intra-cluster and inter-cluster communications has a huge effect
            on overall performance and scalability of Couchbase Server. Most people can do this with
            a 1GB interface, some need 10GB. </dd>
      </dlentry>
    </dl>
    </section>
   
    
    
    
    
    
    <section>
      <title>Data Service Nodes</title> 
     <p>Data Service nodes handle data service operation, such as create/read/update/delete
       (CRUD) operations.</p> 
   
        <p>It is important to keep use cases and application workloads in mind since different
          application workloads have different resource requirements. For example, if your working set
          needs to be fully in memory, you might need large RAM size. On the other hand, if your
          application requires only 5% of data in memory, you will need enough disk space to store all
          the data and fast enough disks for your read/write operations.</p>
        <p>You can start sizing the Data Service nodes by asking the following questions: </p>
        <ol>
          <li>Is your application primarily (or even exclusively) using individual document access? </li>
          <li>Do you plan to use Views? </li>
          <li>Do you plan to use XDCR? Please refer to Couchbase Server 4.0 documentation about different deployment topology. </li>
          <li>Whatâ€™s your working set size and what are your data operation throughput or latency
            requirements? </li>
        </ol>
        
        <p>Answers to the above questions can help you better understand the capacity requirement of
          your cluster and provide a better estimation for sizing. </p>
        <p>Inside Couchbase Server, we looked at performance data and customer use cases to provide
          sizing calculations for each of the areas: CPU, Memory, Disk, and Network.</p>
        <p>A use case for RAM sizing was used as an example with the following data: </p>
        <table><title>Input variables for sizing RAM</title>
          <tgroup cols="2">
            <colspec colname="col1" colwidth="1*"/>
            <colspec colname="col2" colwidth="1*"/>
            <thead>
              <row>
                <entry>Input Variable</entry>
                <entry>value</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry><varname>documents_num</varname></entry>
                <entry>1,000,000</entry>
              </row>
              <row>
                <entry><varname>ID_size</varname></entry>
                <entry>100</entry>
              </row>
              <row>
                <entry><varname>value_size</varname></entry>
                <entry>10,000</entry>
              </row>
              <row>
                <entry><varname>number_of_replicas</varname></entry>
                <entry>1</entry>
              </row>
              <row>
                <entry><varname>working_set_percentage</varname></entry>
                <entry>20% </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        
        
        <table><title>Constants for sizing RAM</title>
          <tgroup cols="2">
            <colspec colname="col1" colwidth="1*"/>
            <colspec colname="col2" colwidth="1*"/>
            <thead>
              <row>
                <entry>Constants</entry>
                <entry>value</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Type of Storage</entry>
                <entry>SSD</entry>
              </row>
              <row>
                <entry><codeph>overhead_percentage</codeph></entry>
                <entry>25%</entry>
              </row>
              <row>
                <entry><codeph>metadata_per_document</codeph></entry>
                <entry>56 for 2.1 and higher, 64 for 2.0.x</entry>
              </row>
              <row>
                <entry><codeph>high_water_mark</codeph></entry>
                <entry>85% </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <p>Based on the provided data, a rough sizing guideline formula would be:</p>
        
        <table><title>Guideline Formula for Sizing a Cluster</title>
          <tgroup cols="2">
            <colspec colname="col1" colwidth="1*"/>
            <colspec colname="col2" colwidth="2*"/>
            <thead>
              <row>
                <entry>Variable</entry>
                <entry>Calculation</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry><varname>no_of_copies</varname></entry>
                <entry><codeph>1 + number_of_replicas</codeph></entry>
              </row>
              <row>
                <entry><varname>total_metadata</varname></entry>
                <entry><codeph>(documents_num) * (metadata_per_document + ID_size) *
                  (no_of_copies)</codeph></entry>
              </row>
              <row>
                <entry><varname>total_dataset</varname></entry>
                <entry><codeph>(documents_num) * (value_size) * (no_of_copies)</codeph></entry>
              </row>
              <row>
                <entry><varname>working_set</varname></entry>
                <entry><codeph>total_dataset * (working_set_percentage)</codeph></entry>
              </row>
              <row>
                <entry>Cluster RAM quota required</entry>
                <entry><codeph>(total_metadata + working_set) * (1 + headroom) /
                  (high_water_mark)</codeph></entry>
              </row>
              <row>
                <entry>number of nodes</entry>
                <entry><codeph>Cluster RAM quota required / per_node_ram_quota</codeph>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <p>Based on the above formula, these are the suggested sizing guidelines:</p>  
        
        
        <table><title>Suggested Sizing Guideline</title>
          <tgroup cols="2">
            <colspec colname="col1" colwidth="1*"/>
            <colspec colname="col2" colwidth="2*"/>
            <thead>
              <row>
                <entry>Variable</entry>
                <entry>Calculation</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry><varname>no_of_copies</varname></entry>
                <entry>= 1 for original and 1 for replica</entry>
              </row>
              <row>
                <entry><varname>total_metadata</varname></entry>
                <entry>= 1,000,000 * (100 + 56) * (2) = 312,000,000</entry>
              </row>
              <row>
                <entry><varname>total_dataset</varname></entry>
                <entry>= 1,000,000 * (10,000) * (2) = 20,000,000,000</entry>
              </row>
              <row>
                <entry><varname>working_set</varname></entry>
                <entry>= 20,000,000,000 * (0.2) = 4,000,000,000</entry>
              </row>
              <row>
                <entry>Cluster RAM quota required</entry>
                <entry>= (312,000,000 + 4,000,000,000) * (1+0.25)/(0.85) = 6,341,176,470</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        
        <p>This tells you that the RAM requirement for whole cluster is <codeph>7GB</codeph>. 
          Note that this amount is in addition to the RAM requirements for the operating system and any 
          other software that runs on the cluster nodes.</p> 
     
     
   </section> 
    
    
    
    
    
    
    
    
    
    <section>
      <title>Index Service Nodes </title> 
 <p>A node running the Index Service must be sized properly to create and maintain
   secondary indexes and to perform index scan for N1QL queries.</p>

        <p> Similarly to the nodes running the data service, there is a set of questions you need to
          answer to take care of your application needs: </p>
        <ol>
          <li>What is the length of the document key?</li>
          <li>Which fields need to be indexed? </li>
          <li>Will you be using simple or compound indexes?</li>
          <li>What is the minimum, maximum, or average value size of the index field?</li>
          
          <li>How many indexes do you need? </li>
          <li>How many documents need to be indexed? </li>
          <li>How often do you want compaction to run? </li>
        </ol>
        <p>Answers to the above questions can help you better understand the capacity requirement of
          your cluster and provide a better estimation for sizing. At Couchbase, we looked at
          performance data and customer use cases to provide sizing calculations for each of the areas:
          CPU, Memory, Disk and Disk I/O.</p>
        <p>Here is a disk size example:</p>
        
        <table frame="all" rowsep="1" colsep="1" id="table_pwg_l2c_3t">
          <title>Disk Sizes</title>
          <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Input variable</entry>
                <entry>Value</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>docID</entry>
                <entry>20 bytes</entry>
              </row>
              <row>
                <entry>Number of index fields</entry>
                <entry>1</entry>
              </row>
              <row>
                <entry>Secondary index</entry>
                <entry>24 bytes</entry>
              </row>
              <row>
                <entry>Number of documents to be indexed</entry>
                <entry>20M</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <p>When we calculate disk usage for the above test cases, there are a few factors we need to keep in mind: </p>
        <ol>
          <li>Compaction is disabled. This case illustrates the worst-case scenario for disk usage.</li>
          <li>Couchbase Server uses an append-only storage format. Therefore, actual disk usage will be
            larger than data size.</li>
          <li>Fragmentation will affect the disk usage. The larger the fragmentation, the more disk you will need. </li>
        </ol>
        <p>Based on our experiment, we noticed that the above index consumes 6GB of disk space. </p>    
      
      
    </section> 
    <section>
      <title>Query Service Nodes</title> 
      <p>A node running the Query Service executes queries for your application
        needs.</p>
    
        <p>Since the Query Service doesnâ€™t need to persist data to disk, there are very minimal resource
          requirements for disk space and disk I/O. You only need to consider CPU and memory.</p>
        <p>There are a few questions that will help size the cluster: </p>
        <ol>
          <li>What types of queries you need to run? </li>
          <li>Do you need to run <codeph>stale=ok</codeph> or <codeph>stale=false</codeph> queries?</li>
          <li>Are the queries simple or complex (requiring JOINs, for example)?</li>
          <li>What are the throughput and latency requirements for your queries? </li>
        </ol>
        <p>Different queries have different resource requirements. A simple query might return results
          within milliseconds while a complex query may require several seconds.</p>
        <p>At Couchbase, we looked at performance data and customer use cases to provide sizing
          calculations. </p>
        <p>Letâ€™s use CPU/Memory utilization for an example for a Query Service. Assume that you have a
          user profile store, which stores userâ€™s name, email and address. You would like to query based
          on userâ€™s email address, and you create a secondary index on email. Now you would like to run
          a query that looks like this: </p>
        <codeblock>Select * from bucket where email = "foo@gmail.com" </codeblock>
        <p>By default, N1QL uses <codeph>stale=ok</codeph> for a consistency model. </p>
        <p>To achieve max throughput of this query, we noticed that it utilized 24 cores completely to
          achieve an 80% latency of 5ms against a bucket of 20M documents.</p>  
      
      
    </section> 
    
    
    
  </body>
</topic>
