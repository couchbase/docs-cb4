<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_axp_glg_xs">
  <title>Sizing Guidelines</title> <shortdesc>Evaluate the overall performance and capacity goals that you have for Couchbase, and use
    that information to determine the necessary resources that you'll need in your
    deployment.</shortdesc>
  <body>
    <p>When you plan to deploy a Couchbase Server cluster, perhaps the most common (and important)
      question that comes up is: <i>How many nodes do I need and what size do they need to
      be?</i></p>
    <p>With the increasing number of Couchbase services and the flexibility of the Couchbase Data
      Platform, the answer to this question can be challenging. This guide aims to help you better
      size your deployment. </p>
    <p>If you want detailed recommendations for your specific deployment, you can contact Couchbase
      Support. You can also find more sizing information on the <xref
        href="http://blog.couchbase.com/how-many-nodes-part-1-introduction-sizing-couchbase-server-20-cluster"
        format="html" scope="external">Couchbase Blog</xref>. </p>
    <note>The sizing recommendations and calculations discussed in this guide are based on an
      analysis of performance data and common use-cases.</note>
    
    <section><title>General Considerations</title> 
    
    <p>The sizing of your Couchbase Server cluster is critical to its overall stability and
        performance. While there are obviously many variables, the main point is this: You need to
        evaluate the overall performance and capacity requirements for your workload and dataset,
        and then divide that into the hardware and resources you have available. Your application
        wants the majority of reads to come out of the cache, and to have the I/O capacity to handle
        the writes. There needs to be enough capacity in all areas to support everything the system
        is doing while maintaining the required level of performance. </p>
    </section>
    
    <section><title>Sizing Couchbase Server Resources </title>
    <p>This guide discusses five determining factors that you should be aware of when sizing a
        Couchbase Server cluster node: <ul>
          <li>CPU</li>
          <li>RAM</li>
          <li>Disk space</li>
          <li>Network</li>
        </ul></p>
    
      
        
      
      <dl>
        <dlentry>
          <dt>CPU</dt>
          <dd>CPU refers to the number of cores and clock speed that are required to run your
            workload. </dd>
        </dlentry>
      </dl>
      <dl>
        <dlentry>
          <dt>RAM</dt>
          <dd>RAM is frequently one of the most crucial areas to size correctly. Cached documents
            allow the reads to be served at very low latency and consistently high throughput.</dd>
          <dd>This resource represents the main memory that you allocate to Couchbase Server and
            should be determined by the following factors: <ul>
              <li>How much free RAM is available beyond OS and other applications</li>
              <li>How much data do you want to store in main memory</li>
              <li>How much latency you expect from KV/indexing/query performance</li>
            </ul><p>Some components that require RAM are:</p><ul>
              <li>Memory-optimized Global Indexes, which enable in-memory index processing and index
                scans with the lowest latency.</li>
              <li>Full Text Search (FTS)</li>
            </ul></dd>
          <dd>
            <p>
              <table frame="all" rowsep="1" colsep="1" id="table_j34_3cq_y5">
                <title>Minimum RAM Quota for Couchbase Server Components</title>
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                  <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                  <thead>
                    <row>
                      <entry>Component</entry>
                      <entry>Minimum RAM</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>Data Service</entry>
                      <entry>1 GB</entry>
                    </row>
                    <row>
                      <entry>Indexing Service powered by ForestDB</entry>
                      <entry>256 MB</entry>
                    </row>
                    <row>
                      <entry>Indexing Service powered by Memory-Optimized Index</entry>
                      <entry>256 MB minimum, 1 GB recommended.</entry>
                    </row>
                    <row>
                      <entry>Full text search</entry>
                      <entry>512 MB; 2048+ MB recommended</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </p>
          </dd>
        </dlentry>
      </dl><dl>
        <dlentry>
          <dt>Disk space</dt>
          <dd>Requirements for your disk subsystem are: <ul>
              <li><term>Disk size</term> — which refers to the amount of the disk storage space that
                is needed to hold your entire data set.</li>
              <li><term>Disk I/O</term> — which is a combination of your sustained read/write rate,
                the need for compacting the database files, and anything else that requires disk
                access. </li>
            </ul><p>To better support Couchbase Server, keep in mind the following: <ul>
                <li>Disk space continues to grow if fragmentation ratio keeps climbing. To mitigate
                  this, add enough buffer in your disk space to store all of the data. Also, keep an
                  eye on the fragmentation ratio in the Couchbase user interfaces and trigger
                  compaction processes when needed. </li>
                <li>Solid State Drives (SSDs) are desired, but not required. An SSD will give much
                  better performance than a Hard Disk Drive (HDD) when it comes to disk throughput
                  and latency.</li>
              </ul></p></dd>
        </dlentry>
      </dl>
      <table frame="all" rowsep="1" colsep="1" id="table_qzz_b1q_y5">
        <title>Minimum and Recommended Hardware Resources</title>
        <tgroup cols="3">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="1*"/>
          <colspec colname="c3" colnum="3" colwidth="3*"/>
          <thead>
            <row>
              <entry>Resource</entry>
              <entry>Minimum</entry>
              <entry>Recommended</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>CPU </entry>
              <entry>4 cores</entry>
              <entry>8 cores and above<p>Both the number and performance of CPUs are
                important.</p></entry>
            </row>
            <row>
              <entry>RAM</entry>
              <entry>4 GB</entry>
              <entry>16 GB and above<p>More RAM means that more data can be served from memory,
                  leading to better performance.</p></entry>
            </row>
            <row>
              <entry>Disk</entry>
              <entry>8 GB</entry>
              <entry>16 GB and above</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    
    <dl>
      <dlentry>
        <dt>Network</dt>
        <dd>Enough network bandwidth is vital to the performance of Couchbase Server. A reliable
            high-speed network for intra-cluster and inter-cluster communications has a huge effect
            on overall performance and scalability of Couchbase Server. Most deployments can achieve
            optimal performance with 1 Gbps interconnects, but some may need 10 Gbps.</dd>
      </dlentry>
    </dl>
    </section>
   
    
    
    
    
    
    <section><title>Sizing Data Service Nodes</title><p>Data Service nodes handle data service
        operations, such as create/read/update/delete (CRUD).</p>It's important to keep use-cases
      and application workloads in mind since different application workloads have different
      resource requirements. For example, if your working set needs to be fully in memory, you might
      need large RAM size. On the other hand, if your application requires only 5% of data in
      memory, you will need disks with enough space to store all of the data, and that are fast
      enough for your read/write operations.<p>You can start sizing the Data Service nodes by
        answering the following questions: </p><ol>
        <li>Is the application primarily (or even exclusively) using individual document access? </li>
        <li>Do you plan to use Views? </li>
        <li>Do you plan to use XDCR? </li>
        <li>What’s your working set size and what are your data operation throughput and latency
          requirements? </li>
      </ol><p>Answers to the above questions can help you better understand the capacity requirement
        of your cluster and provide a better estimation for sizing. </p><b>The following is an
        example use-case for sizing RAM:</b><table>
        <title>Input Variables for Sizing RAM</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="1*"/>
          <thead>
            <row>
              <entry>Input Variable</entry>
              <entry>Value</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><varname>documents_num</varname></entry>
              <entry>1,000,000</entry>
            </row>
            <row>
              <entry><varname>ID_size</varname></entry>
              <entry>100</entry>
            </row>
            <row>
              <entry><varname>value_size</varname></entry>
              <entry>10,000</entry>
            </row>
            <row>
              <entry><varname>number_of_replicas</varname></entry>
              <entry>1</entry>
            </row>
            <row>
              <entry><varname>working_set_percentage</varname></entry>
              <entry>20% </entry>
            </row>
          </tbody>
        </tgroup>
      </table><table>
        <title>Constants for Sizing RAM</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="1*"/>
          <thead>
            <row>
              <entry>Constants</entry>
              <entry>Value</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Type of Storage</entry>
              <entry>SSD</entry>
            </row>
            <row>
              <entry><codeph>overhead_percentage</codeph></entry>
              <entry>25%</entry>
            </row>
            <row>
              <entry><codeph>metadata_per_document</codeph></entry>
              <entry>56 for 2.1 and higher, 64 for 2.0.x</entry>
            </row>
            <row>
              <entry><codeph>high_water_mark</codeph></entry>
              <entry>85% </entry>
            </row>
          </tbody>
        </tgroup>
      </table><p>Based on the provided data, a rough sizing guideline formula would be:</p><table>
        <title>Guideline Formula for Sizing a Cluster</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="2*"/>
          <thead>
            <row>
              <entry>Variable</entry>
              <entry>Calculation</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><varname>no_of_copies</varname></entry>
              <entry><codeph>1 + number_of_replicas</codeph></entry>
            </row>
            <row>
              <entry><varname>total_metadata</varname></entry>
              <entry><codeph>(documents_num) * (metadata_per_document + ID_size) *
                  (no_of_copies)</codeph></entry>
            </row>
            <row>
              <entry><varname>total_dataset</varname></entry>
              <entry><codeph>(documents_num) * (value_size) * (no_of_copies)</codeph></entry>
            </row>
            <row>
              <entry><varname>working_set</varname></entry>
              <entry><codeph>total_dataset * (working_set_percentage)</codeph></entry>
            </row>
            <row>
              <entry>Cluster RAM quota required</entry>
              <entry><codeph>(total_metadata + working_set) * (1 + headroom) /
                  (high_water_mark)</codeph></entry>
            </row>
            <row>
              <entry>number of nodes</entry>
              <entry><codeph>Cluster RAM quota required / per_node_ram_quota</codeph>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table><p>Based on the above formula, these are the suggested sizing guidelines:</p><table>
        <title>Suggested Sizing Guideline</title>
        <tgroup cols="2">
          <colspec colname="col1" colwidth="1*"/>
          <colspec colname="col2" colwidth="2*"/>
          <thead>
            <row>
              <entry>Variable</entry>
              <entry>Calculation</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><varname>no_of_copies</varname></entry>
              <entry>= 1 for original and 1 for replica</entry>
            </row>
            <row>
              <entry><varname>total_metadata</varname></entry>
              <entry>= 1,000,000 * (100 + 56) * (2) = 312,000,000</entry>
            </row>
            <row>
              <entry><varname>total_dataset</varname></entry>
              <entry>= 1,000,000 * (10,000) * (2) = 20,000,000,000</entry>
            </row>
            <row>
              <entry><varname>working_set</varname></entry>
              <entry>= 20,000,000,000 * (0.2) = 4,000,000,000</entry>
            </row>
            <row>
              <entry>Cluster RAM quota required</entry>
              <entry>= (312,000,000 + 4,000,000,000) * (1+0.25)/(0.85) = 6,341,176,470</entry>
            </row>
          </tbody>
        </tgroup>
      </table><p>This tells you that the RAM requirement for the whole cluster is 7 GB. Note that
        this amount is in addition to the RAM requirements for the operating system and any other
        software that runs on the cluster nodes.</p></section> 
    
    
    
    
    
    
    
    
    
    <section><title>Sizing Index Service Nodes </title><p>A node running the Index Service must be
        sized properly to create and maintain secondary indexes and to perform index scan for N1QL
        queries.</p><p>Similarly to the nodes that run the Data Service, there is a set of questions
        you need to answer to take care of your application needs: </p><ol>
        <li>What is the length of the document key?</li>
        <li>Which fields need to be indexed? </li>
        <li>Will you be using simple or compound indexes?</li>
        <li>What is the minimum, maximum, or average value size of the index field?</li>
        <li>How many indexes do you need? </li>
        <li>How many documents need to be indexed? </li>
        <li>How often do you want compaction to run? </li>
      </ol><p>Answers to these questions can help you better understand the capacity requirement of
        your cluster, and provide a better estimation for sizing.</p><b>The following is an example
        use-case for sizing disk:</b><table frame="all" rowsep="1" colsep="1" id="table_pwg_l2c_3t">
        <title>Disk Sizes</title>
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Input variable</entry>
              <entry>Value</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>docID</entry>
              <entry>20 bytes</entry>
            </row>
            <row>
              <entry>Number of index fields</entry>
              <entry>1</entry>
            </row>
            <row>
              <entry>Secondary index</entry>
              <entry>24 bytes</entry>
            </row>
            <row>
              <entry>Number of documents to be indexed</entry>
              <entry>20M</entry>
            </row>
          </tbody>
        </tgroup>
      </table><p>When you calculate disk usage for the above test cases, there are a few factors you
        need to keep in mind: </p><ol>
        <li>Compaction is disabled. This case illustrates the worst-case scenario for disk
          usage.</li>
        <li>Couchbase Server uses an append-only storage format. Therefore, actual disk usage will
          be larger than data size.</li>
        <li>Fragmentation will affect the disk usage. The larger the fragmentation, the more disk
          you will need. </li>
      </ol><p>The above index consumes 6 GB of disk space. </p></section> 
    <section><title>Sizing Query Service Nodes</title><p>A node that runs the Query Service executes
        queries for your application needs.</p><p>Since the Query Service doesn’t need to persist
        data to disk, there are very minimal resource requirements for disk space and disk I/O. You
        only need to consider CPU and memory.</p><p>There are a few questions that will help size
        the cluster: </p><ol>
        <li>What types of queries do you need to run? </li>
        <li>Do you need to run <codeph>stale=ok</codeph> or <codeph>stale=false</codeph>
          queries?</li>
        <li>Are the queries simple or complex (requiring JOINs, for example)?</li>
        <li>What are the throughput and latency requirements for your queries? </li>
      </ol><p>Different queries have different resource requirements. A simple query might return
        results within milliseconds while a complex query may require several seconds.</p><b>The
        following is an example use-case for sizing CPU:</b><p>Assume that you have a user profile
        store, which stores a user’s name and email address. You would like to query based on a
        user’s email address, and you create a secondary index on email. Now you would like to run a
        query that looks like this:
        </p><codeblock>Select * from bucket where email = "foo@gmail.com" </codeblock><p>By default,
        N1QL uses <codeph>stale=ok</codeph> for a consistency model. </p><p>It was observed that
        this query utilized 24 cores completely to achieve an 80% latency of 5ms against a bucket of
        20M documents.</p></section> 
    
    
    
  </body>
</topic>
