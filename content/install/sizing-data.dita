<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_dhn_jlg_xs">
  <title>Data Service Nodes</title> <shortdesc>Data Service nodes handle data service operation, such as create/read/update/delete
    (CRUD) operations. </shortdesc>
  <body>
    <p>It is important to keep use cases and application workloads in mind since different
      application workloads have different resource requirements. For example, if your working set
      needs to be fully in memory, you might need large RAM size. On the other hand, if your
      application requires only 5% of data in memory, you will need enough disk space to store all
      the data and fast enough disks for your read/write operations.</p>
    <p>You can start sizing the Data Service nodes by asking the following questions: </p>
    <ol>
      <li>Is your application primarily (or even exclusively) using individual document access? </li>
      <li>Do you plan to use Views? </li>
      <li>Do you plan to use XDCR? Please refer to Couchbase Server 4.0 documentation about different deployment topology. </li>
      <li>Whatâ€™s your working set size and what are your data operation throughput or latency
        requirements? </li>
    </ol>
    
    <p>Answers to the above questions can help you better understand the capacity requirement of
      your cluster and provide a better estimation for sizing. </p>
    <p>Inside Couchbase Server, we looked at performance data and customer use cases to provide
      sizing calculations for each of the areas: CPU, Memory, Disk, and Network.</p>
    <p>A use case for RAM sizing was used as an example with the following data: </p>
    <table><title>Input variables for sizing RAM</title>
      <tgroup cols="2">
        <colspec colname="col1" colwidth="1*"/>
        <colspec colname="col2" colwidth="1*"/>
        <thead>
          <row>
            <entry>Input Variable</entry>
            <entry>value</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry><varname>documents_num</varname></entry>
            <entry>1,000,000</entry>
          </row>
          <row>
            <entry><varname>ID_size</varname></entry>
            <entry>100</entry>
          </row>
          <row>
            <entry><varname>value_size</varname></entry>
            <entry>10,000</entry>
          </row>
          <row>
            <entry><varname>number_of_replicas</varname></entry>
            <entry>1</entry>
          </row>
          <row>
            <entry><varname>working_set_percentage</varname></entry>
            <entry>20% </entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    
    
    <table><title>Constants for sizing RAM</title>
      <tgroup cols="2">
        <colspec colname="col1" colwidth="1*"/>
        <colspec colname="col2" colwidth="1*"/>
        <thead>
          <row>
            <entry>Constants</entry>
            <entry>value</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>Type of Storage</entry>
            <entry>SSD</entry>
          </row>
          <row>
            <entry><codeph>overhead_percentage</codeph></entry>
            <entry>25%</entry>
          </row>
          <row>
            <entry><codeph>metadata_per_document</codeph></entry>
            <entry>56 for 2.1 and higher, 64 for 2.0.x</entry>
          </row>
          <row>
            <entry><codeph>high_water_mark</codeph></entry>
            <entry>85% </entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    <p>Based on the provided data, a rough sizing guideline formula would be:</p>
    
    <table><title>Guideline Formula for Sizing a Cluster</title>
      <tgroup cols="2">
        <colspec colname="col1" colwidth="1*"/>
        <colspec colname="col2" colwidth="2*"/>
        <thead>
          <row>
            <entry>Variable</entry>
            <entry>Calculation</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry><varname>no_of_copies</varname></entry>
            <entry><codeph>1 + number_of_replicas</codeph></entry>
          </row>
          <row>
            <entry><varname>total_metadata</varname></entry>
            <entry><codeph>(documents_num) * (metadata_per_document + ID_size) *
                (no_of_copies)</codeph></entry>
          </row>
          <row>
            <entry><varname>total_dataset</varname></entry>
            <entry><codeph>(documents_num) * (value_size) * (no_of_copies)</codeph></entry>
          </row>
          <row>
            <entry><varname>working_set</varname></entry>
            <entry><codeph>total_dataset * (working_set_percentage)</codeph></entry>
          </row>
          <row>
            <entry>Cluster RAM quota required</entry>
            <entry><codeph>(total_metadata + working_set) * (1 + headroom) /
                (high_water_mark)</codeph></entry>
          </row>
          <row>
            <entry>number of nodes</entry>
            <entry><codeph>Cluster RAM quota required / per_node_ram_quota</codeph>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    <p>Based on the above formula, these are the suggested sizing guidelines:</p>  
    
    
    <table><title>Suggested Sizing Guideline</title>
      <tgroup cols="2">
        <colspec colname="col1" colwidth="1*"/>
        <colspec colname="col2" colwidth="2*"/>
        <thead>
          <row>
            <entry>Variable</entry>
            <entry>Calculation</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry><varname>no_of_copies</varname></entry>
            <entry>= 1 for original and 1 for replica</entry>
          </row>
          <row>
            <entry><varname>total_metadata</varname></entry>
            <entry>= 1,000,000 * (100 + 56) * (2) = 312,000,000</entry>
          </row>
          <row>
            <entry><varname>total_dataset</varname></entry>
            <entry>= 1,000,000 * (10,000) * (2) = 20,000,000,000</entry>
          </row>
          <row>
            <entry><varname>working_set</varname></entry>
            <entry>= 20,000,000,000 * (0.2) = 4,000,000,000</entry>
          </row>
          <row>
            <entry>Cluster RAM quota required</entry>
            <entry>= (312,000,000 + 4,000,000,000) * (1+0.25)/(0.85) = 6,341,176,470</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    
    <p>This tells you that the RAM requirement for whole cluster is <codeph>7GB</codeph>. 
      Note that this amount is in addition to the RAM requirements for the operating system and any 
      other software that runs on the cluster nodes.</p>
    
   
 
  </body>
</topic>
